[
  {
    "path": "posts/2021-09-16-control-structures/",
    "title": "Control Structures",
    "description": "Introduction to control the flow of execution of a series of R expressions.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-16",
    "categories": [
      "module 2",
      "week 3",
      "R",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nControl Structures\nif-else\nfor Loops\nNested for loops\nwhile Loops\nrepeat Loops\nnext, break\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rafalab.github.io/dsbook/programming-basics.html\nhttps://r4ds.had.co.nz/iteration.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-control-structures.html\nhttps://r4ds.had.co.nz/iteration.html\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to use commonly used control structures including if, while, repeat, and for\nBe able to skip an iteration of a loop using next\nBe able to exit a loop immediately using break\n\nControl Structures\nControl structures in R allow you to control the flow of execution of a series of R expressions. Basically, control structures allow you to put some “logic” into your R code, rather than just always executing the same R code every time. Control structures allow you to respond to inputs or to features of the data and execute different R expressions accordingly.\nCommonly used control structures are\nif and else: testing a condition and acting on it\nfor: execute a loop a fixed number of times\nwhile: execute a loop while a condition is true\nrepeat: execute an infinite loop (must break out of it to stop)\nbreak: break the execution of a loop\nnext: skip an interation of a loop\nMost control structures are not used in interactive sessions, but rather when writing functions or longer expresisons. However, these constructs do not have to be used in functions and it’s a good idea to become familiar with them before we delve into functions.\nif-else\nThe if-else combination is probably the most commonly used control structure in R (or perhaps any language). This structure allows you to test a condition and act on it depending on whether it’s true or false.\nFor starters, you can just use the if statement.\nif(<condition>) {\n        ## do something\n} \n## Continue with rest of code\nThe above code does nothing if the condition is false. If you have an action you want to execute when the condition is false, then you need an else clause.\nif(<condition>) {\n        ## do something\n} \nelse {\n        ## do something else\n}\nYou can have a series of tests by following the initial if with any number of else ifs.\nif(<condition1>) {\n        ## do something\n} else if(<condition2>)  {\n        ## do something different\n} else {\n        ## do something different\n}\nHere is an example of a valid if/else structure.\n\n\n## Generate a uniform random number\nx <- runif(1, 0, 10)  \nif(x > 3) {\n        y <- 10\n} else {\n        y <- 0\n}\n\n\n\nThe value of y is set depending on whether x > 3 or not. This expression can also be written a different, but equivalent, way in R.\n\n\ny <- if(x > 3) {\n        10\n} else { \n        0\n}\n\n\n\nNeither way of writing this expression is more correct than the other. Which one you use will depend on your preference and perhaps those of the team you may be working with.\nOf course, the else clause is not necessary. You could have a series of if clauses that always get executed if their respective conditions are true.\nif(<condition1>) {\n\n}\n\nif(<condition2>) {\n\n}\nfor Loops\nFor loops are pretty much the only looping construct that you will need in R. While you may occasionally find a need for other types of loops, in my experience doing data analysis, I’ve found very few situations where a for loop wasn’t sufficient.\nIn R, for loops take an interator variable and assign it successive values from a sequence or vector. For loops are most commonly used for iterating over the elements of an object (list, vector, etc.)\n\n\nfor(i in 1:10) {\n        print(i)\n}\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n\nThis loop takes the i variable and in each iteration of the loop gives it values 1, 2, 3, …, 10, executes the code within the curly braces, and then the loop exits.\nThe following three loops all have the same behavior.\n\n\nx <- c(\"a\", \"b\", \"c\", \"d\")\n\nfor(i in 1:4) {\n        ## Print out each element of 'x'\n        print(x[i])  \n}\n\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\nThe seq_along() function is commonly used in conjunction with for loops in order to generate an integer sequence based on the length of an object (in this case, the object x).\n\n\n## Generate a sequence based on length of 'x'\nfor(i in seq_along(x)) {   \n        print(x[i])\n}\n\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\nIt is not necessary to use an index-type variable.\n\n\nfor(letter in x) {\n        print(letter)\n}\n\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\nFor one line loops, the curly braces are not strictly necessary.\n\n\nfor(i in 1:4) print(x[i])\n\n\n[1] \"a\"\n[1] \"b\"\n[1] \"c\"\n[1] \"d\"\n\nHowever, I like to use curly braces even for one-line loops, because that way if you decide to expand the loop to multiple lines, you won’t be burned because you forgot to add curly braces (and you will be burned by this).\nNested for loops\nfor loops can be nested inside of each other.\nx <- matrix(1:6, 2, 3)\n\nfor(i in seq_len(nrow(x))) {\n        for(j in seq_len(ncol(x))) {\n                print(x[i, j])\n        }   \n}\nNested loops are commonly needed for multidimensional or hierarchical data structures (e.g. matrices, lists). Be careful with nesting though. Nesting beyond 2 to 3 levels often makes it difficult to read/understand the code. If you find yourself in need of a large number of nested loops, you may want to break up the loops by using functions (discussed later).\nwhile Loops\nWhile loops begin by testing a condition. If it is true, then they execute the loop body. Once the loop body is executed, the condition is tested again, and so forth, until the condition is false, after which the loop exits.\n\n\ncount <- 0\nwhile(count < 10) {\n        print(count)\n        count <- count + 1\n}\n\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n\nWhile loops can potentially result in infinite loops if not written properly. Use with care!\nSometimes there will be more than one condition in the test.\n\n\nz <- 5\nset.seed(1)\n\nwhile(z >= 3 && z <= 10) {\n        coin <- rbinom(1, 1, 0.5)\n        \n        if(coin == 1) {  ## random walk\n                z <- z + 1\n        } else {\n                z <- z - 1\n        } \n}\nprint(z)\n\n\n[1] 2\n\nConditions are always evaluated from left to right. For example, in the above code, if z were less than 3, the second test would not have been evaluated.\nrepeat Loops\nrepeat initiates an infinite loop right from the start. These are not commonly used in statistical or data analysis applications but they do have their uses. The only way to exit a repeat loop is to call break.\nOne possible paradigm might be in an iterative algorithm where you may be searching for a solution and you don’t want to stop until you’re close enough to the solution. In this kind of situation, you often don’t know in advance how many iterations it’s going to take to get “close enough” to the solution.\n\n\nx0 <- 1\ntol <- 1e-8\n\nrepeat {\n        x1 <- computeEstimate()\n        \n        if(abs(x1 - x0) < tol) {  ## Close enough?\n                break\n        } else {\n                x0 <- x1\n        } \n}\n\n\n\nNote that the above code will not run if the computeEstimate() function is not defined (I just made it up for the purposes of this demonstration).\nThe loop above is a bit dangerous because there’s no guarantee it will stop. You could get in a situation where the values of x0 and x1 oscillate back and forth and never converge. Better to set a hard limit on the number of iterations by using a for loop and then report whether convergence was achieved or not.\nnext, break\nnext is used to skip an iteration of a loop.\n\n\nfor(i in 1:100) {\n        if(i <= 20) {\n                ## Skip the first 20 iterations\n                next                 \n        }\n        ## Do something here\n}\n\n\n\nbreak is used to exit a loop immediately, regardless of what iteration the loop may be on.\n\n\nfor(i in 1:100) {\n      print(i)\n\n      if(i > 20) {\n              ## Stop loop after 20 iterations\n              break  \n      }    \n}\n\n\n\nSummary\nControl structures like if, while, and for allow you to control the flow of an R program\nInfinite loops should generally be avoided, even if (you believe) they are theoretically correct.\nControl structures mentioned here are primarily useful for writing programs; for command-line interactive work, the “apply” functions are more useful.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWrite for loops to compute the mean of every column in mtcars.\nImagine you have a directory full of CSV files that you want to read in. You have their paths in a vector, files <- dir(\"data/\", pattern = \"\\\\.csv$\", full.names = TRUE), and now want to read each one with read_csv(). Write the for loop that will load them into a single data frame.\nWhat happens if you use for (nm in names(x)) and x has no names? What if only some of the elements are named? What if the names are not unique?\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-control-structures.html\nhttps://rafalab.github.io/dsbook/programming-basics.html\nhttps://r4ds.had.co.nz/iteration.html\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-16T13:04:56-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-16-r-nuts-and-bolts/",
    "title": "R Nuts and Bolts",
    "description": "Introduction to data types and objects in R.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-16",
    "categories": [
      "module 2",
      "week 3",
      "R",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nR Nuts and Bolts\nEntering Input\nEvaluation\nR Objects\nNumbers\nAttributes\nCreating Vectors\nMixing Objects\nExplicit Coercion\nMatrices\nLists\nFactors\nMissing Values\nData Frames\nNames\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rafalab.github.io/dsbook/r-basics.html\nhttps://r4ds.had.co.nz/vectors.html?q=typeof#vectors\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-r-nuts-and-bolts.html\nLearning objectives\n\nAt the end of this lesson you will:\nKnow the 5 basic built-in data types (or classes) of objects in R\nKnow the types of attributes (or metadata) R objects can have\nBe able to create a vector, matrix, data frame, factor, and list in R\nRecognize missing values in R\n\nR Nuts and Bolts\nEntering Input\nAt the R prompt we type expressions. The <- symbol is the assignment operator.\n\n\nx <- 1\nprint(x)\n\n\n[1] 1\n\nx\n\n\n[1] 1\n\nmsg <- \"hello\"\n\n\n\nThe grammar of the language determines whether an expression is complete or not.\nx <-  ## Incomplete expression\nThe # character indicates a comment. Anything to the right of the # (including the # itself) is ignored. This is the only comment character in R. Unlike some other languages, R does not support multi-line comments or comment blocks.\nEvaluation\nWhen a complete expression is entered at the prompt, it is evaluated and the result of the evaluated expression is returned. The result may be auto-printed.\n\n\nx <- 5  ## nothing printed\nx       ## auto-printing occurs\n\n\n[1] 5\n\nprint(x)  ## explicit printing\n\n\n[1] 5\n\nThe [1] shown in the output indicates that x is a vector and 5 is its first element.\nTypically with interactive work, we do not explicitly print objects with the print function; it is much easier to just auto-print them by typing the name of the object and hitting return/enter. However, when writing scripts, functions, or longer programs, there is sometimes a need to explicitly print objects because auto-printing does not work in those settings.\nWhen an R vector is printed you will notice that an index for the vector is printed in square brackets [] on the side. For example, see this integer sequence of length 20.\n\n\n\n\n\nx <- 11:30\nx\n\n\n [1] 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n\n\n\n\nThe numbers in the square brackets are not part of the vector itself, they are merely part of the printed output.\nWith R, it’s important that one understand that there is a difference between the actual R object and the manner in which that R object is printed to the console. Often, the printed output may have additional bells and whistles to make the output more friendly to the users. However, these bells and whistles are not inherently part of the object.\nNote that the : operator is used to create integer sequences.\nR Objects\nR has five basic or “atomic” classes of vector objects:\nlogical: FALSE, TRUE, and NA\nnumeric: real numbers\ninteger (and doubles): these are known collectively as numeric vectors\ncomplex: complex numbers\ncharacter: the most complex type of atomic vector, because each element of a character vector is a string, and a string can contain an arbitrary amount of data.\n\n\n\nFigure 1: The hierarchy of R’s vector types\n\n\n\n[Source: R 4 Data Science]\n\nInteger and double vectors are known collectively as numeric vectors. In R, numbers are doubles by default. To make an integer, place an L after the number:\n\n\ntypeof(1)\n\n\n[1] \"double\"\n\ntypeof(1L)\n\n\n[1] \"integer\"\n\n1.5L\n\n\n[1] 1.5\n\n\n\nThe distinction between integers and doubles is not usually important, but there are two important differences that you should be aware of:\nDoubles are approximations. Doubles represent floating point numbers that can not always be precisely represented with a fixed amount of memory. This means that you should consider all doubles to be approximations.\nFor example, what is square of the square root of two?\n\n\nx <- sqrt(2) ^ 2\nx\n\n\n[1] 2\n\nx - 2\n\n\n[1] 4.440892e-16\n\n\nThe most basic type of R object is a vector. Empty vectors can be created with the vector() function. There is really only one rule about vectors in R, which is that A vector can only contain objects of the same class.\nFor example, if you run this, what happens?\n\n\nvector(\"a\", 1)\n\n\n\nBut of course, like any good rule, there is an exception, which is a list, which we will get to a bit later. A list is represented as a vector but can contain objects of different classes. Indeed, that’s usually why we use them.\nThere is also a class for “raw” objects, but they are not commonly used directly in data analysis and I won’t cover them here.\nNumbers\nNumbers in R are generally treated as numeric objects (i.e. double precision real numbers). This means that even if you see a number like “1” or “2” in R, which you might think of as integers, they are likely represented behind the scenes as numeric objects (so something like “1.00” or “2.00”). This isn’t important most of the time…except when it is.\nIf you explicitly want an integer, you need to specify the L suffix. So entering 1 in R gives you a numeric object; entering 1L explicitly gives you an integer object.\nThere is also a special number Inf which represents infinity. This allows us to represent entities like 1 / 0. This way, Inf can be used in ordinary calculations; e.g. 1 / Inf is 0.\nThe value NaN represents an undefined value (“not a number”); e.g. 0 / 0; NaN can also be thought of as a missing value (more on that later)\nAttributes\nR objects can have attributes, which are like metadata for the object. These metadata can be very useful in that they help to describe the object. For example, column names on a data frame help to tell us what data are contained in each of the columns. Some examples of R object attributes are\nnames, dimnames\ndimensions (e.g. matrices, arrays)\nclass (e.g. integer, numeric)\nlength\nother user-defined attributes/metadata\nAttributes of an object (if any) can be accessed using the attributes() function. Not all R objects contain attributes, in which case the attributes() function returns NULL.\nHowever, every vector has two key properties:\nIts type, which you can determine with typeof().\n\n\ntypeof(letters)\n\n\n[1] \"character\"\n\ntypeof(1:10)\n\n\n[1] \"integer\"\n\nIts length, which you can determine with length().\n\n\nx <- list(\"a\", \"b\", 1:10)\nlength(x)\n\n\n[1] 3\n\ntypeof(x)\n\n\n[1] \"list\"\n\nCreating Vectors\nThe c() function can be used to create vectors of objects by concatenating things together.\n\n\nx <- c(0.5, 0.6)       ## numeric\nx <- c(TRUE, FALSE)    ## logical\nx <- c(T, F)           ## logical\nx <- c(\"a\", \"b\", \"c\")  ## character\nx <- 9:29              ## integer\nx <- c(1+0i, 2+4i)     ## complex\n\n\n\nNote that in the above example, T and F are short-hand ways to specify TRUE and FALSE. However, in general one should try to use the explicit TRUE and FALSE values when indicating logical values. The T and F values are primarily there for when you’re feeling lazy.\nYou can also use the vector() function to initialize vectors.\n\n\nx <- vector(\"numeric\", length = 10) \nx\n\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\nMixing Objects\nThere are occasions when different classes of R objects get mixed together. Sometimes this happens by accident but it can also happen on purpose. So what happens with the following code?\n\n\ny <- c(1.7, \"a\")   ## character\ny <- c(TRUE, 2)    ## numeric\ny <- c(\"a\", TRUE)  ## character\n\n\n\nIn each case above, we are mixing objects of two different classes in a vector. But remember that the only rule about vectors says this is not allowed. When different objects are mixed in a vector, coercion occurs so that every element in the vector is of the same class.\nIn the example above, we see the effect of implicit coercion. What R tries to do is find a way to represent all of the objects in the vector in a reasonable fashion. Sometimes this does exactly what you want and…sometimes not. For example, combining a numeric object with a character object will create a character vector, because numbers can usually be easily represented as strings.\nExplicit Coercion\nObjects can be explicitly coerced from one class to another using the as.* functions, if available.\n\n\nx <- 0:6\nclass(x)\n\n\n[1] \"integer\"\n\nas.numeric(x)\n\n\n[1] 0 1 2 3 4 5 6\n\nas.logical(x)\n\n\n[1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\nas.character(x)\n\n\n[1] \"0\" \"1\" \"2\" \"3\" \"4\" \"5\" \"6\"\n\nSometimes, R can’t figure out how to coerce an object and this can result in NAs being produced.\n\n\nx <- c(\"a\", \"b\", \"c\")\nas.numeric(x)\n\n\n[1] NA NA NA\n\nas.logical(x)\n\n\n[1] NA NA NA\n\nas.complex(x)\n\n\n[1] NA NA NA\n\nWhen nonsensical coercion takes place, you will usually get a warning from R.\nMatrices\nMatrices are vectors with a dimension attribute. The dimension attribute is itself an integer vector of length 2 (number of rows, number of columns)\n\n\nm <- matrix(nrow = 2, ncol = 3) \nm\n\n\n     [,1] [,2] [,3]\n[1,]   NA   NA   NA\n[2,]   NA   NA   NA\n\ndim(m)\n\n\n[1] 2 3\n\nattributes(m)\n\n\n$dim\n[1] 2 3\n\nMatrices are constructed column-wise, so entries can be thought of starting in the “upper left” corner and running down the columns.\n\n\nm <- matrix(1:6, nrow = 2, ncol = 3) \nm\n\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\nMatrices can also be created directly from vectors by adding a dimension attribute.\n\n\nm <- 1:10 \nm\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\ndim(m) <- c(2, 5)\nm\n\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    3    5    7    9\n[2,]    2    4    6    8   10\n\nMatrices can be created by column-binding or row-binding with the cbind() and rbind() functions.\n\n\nx <- 1:3\ny <- 10:12\ncbind(x, y)\n\n\n     x  y\n[1,] 1 10\n[2,] 2 11\n[3,] 3 12\n\nrbind(x, y) \n\n\n  [,1] [,2] [,3]\nx    1    2    3\ny   10   11   12\n\nLists\nLists are a special type of vector that can contain elements of different classes. Lists are a very important data type in R and you should get to know them well. Lists, in combination with the various “apply” functions discussed later, make for a powerful combination.\nLists can be explicitly created using the list() function, which takes an arbitrary number of arguments.\n\n\nx <- list(1, \"a\", TRUE, 1 + 4i) \nx\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1] TRUE\n\n[[4]]\n[1] 1+4i\n\nWe can also create an empty list of a prespecified length with the vector() function\n\n\nx <- vector(\"list\", length = 5)\nx\n\n\n[[1]]\nNULL\n\n[[2]]\nNULL\n\n[[3]]\nNULL\n\n[[4]]\nNULL\n\n[[5]]\nNULL\n\nFactors\nFactors are used to represent categorical data and can be unordered or ordered. One can think of a factor as an integer vector where each integer has a label. Factors are important in statistical modeling and are treated specially by modelling functions like lm() and glm().\nUsing factors with labels is better than using integers because factors are self-describing. Having a variable that has values “Yes” and “No” or “Smoker” and “Non-Smoker” is better than a variable that has values 1 and 2.\nFactor objects can be created with the factor() function.\n\n\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\")) \nx\n\n\n[1] yes yes no  yes no \nLevels: no yes\n\ntable(x) \n\n\nx\n no yes \n  2   3 \n\n## See the underlying representation of factor\nunclass(x)  \n\n\n[1] 2 2 1 2 1\nattr(,\"levels\")\n[1] \"no\"  \"yes\"\n\nOften factors will be automatically created for you when you read a dataset in using a function like read.table(). Those functions often default to creating factors when they encounter data that look like characters or strings.\nThe order of the levels of a factor can be set using the levels argument to factor(). This can be important in linear modelling because the first level is used as the baseline level.\n\n\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"))\nx  ## Levels are put in alphabetical order\n\n\n[1] yes yes no  yes no \nLevels: no yes\n\nx <- factor(c(\"yes\", \"yes\", \"no\", \"yes\", \"no\"),\n            levels = c(\"yes\", \"no\"))\nx\n\n\n[1] yes yes no  yes no \nLevels: yes no\n\nMissing Values\nMissing values are denoted by NA or NaN for q undefined mathematical operations.\nis.na() is used to test objects if they are NA\nis.nan() is used to test for NaN\nNA values have a class also, so there are integer NA, character NA, etc.\nA NaN value is also NA but the converse is not true\n\n\n## Create a vector with NAs in it\nx <- c(1, 2, NA, 10, 3)  \n## Return a logical vector indicating which elements are NA\nis.na(x)    \n\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\n## Return a logical vector indicating which elements are NaN\nis.nan(x)   \n\n\n[1] FALSE FALSE FALSE FALSE FALSE\n\n\n\n## Now create a vector with both NA and NaN values\nx <- c(1, 2, NaN, NA, 4)\nis.na(x)\n\n\n[1] FALSE FALSE  TRUE  TRUE FALSE\n\nis.nan(x)\n\n\n[1] FALSE FALSE  TRUE FALSE FALSE\n\nData Frames\nData frames are used to store tabular data in R. They are an important type of object in R and are used in a variety of statistical modeling applications. Hadley Wickham’s package dplyr has an optimized set of functions designed to work efficiently with data frames.\nData frames are represented as a special type of list where every element of the list has to have the same length. Each element of the list can be thought of as a column and the length of each element of the list is the number of rows.\nUnlike matrices, data frames can store different classes of objects in each column. Matrices must have every element be the same class (e.g. all integers or all numeric).\nIn addition to column names, indicating the names of the variables or predictors, data frames have a special attribute called row.names which indicate information about each row of the data frame.\nData frames are usually created by reading in a dataset using the read.table() or read.csv(). However, data frames can also be created explicitly with the data.frame() function or they can be coerced from other types of objects like lists.\nData frames can be converted to a matrix by calling data.matrix(). While it might seem that the as.matrix() function should be used to coerce a data frame to a matrix, almost always, what you want is the result of data.matrix().\n\n\nx <- data.frame(foo = 1:4, bar = c(T, T, F, F)) \nx\n\n\n  foo   bar\n1   1  TRUE\n2   2  TRUE\n3   3 FALSE\n4   4 FALSE\n\nnrow(x)\n\n\n[1] 4\n\nncol(x)\n\n\n[1] 2\n\nNames\nR objects can have names, which is very useful for writing readable code and self-describing objects. Here is an example of assigning names to an integer vector.\n\n\nx <- 1:3\nnames(x)\n\n\nNULL\n\nnames(x) <- c(\"New York\", \"Seattle\", \"Los Angeles\") \nx\n\n\n   New York     Seattle Los Angeles \n          1           2           3 \n\nnames(x)\n\n\n[1] \"New York\"    \"Seattle\"     \"Los Angeles\"\n\nLists can also have names, which is often very useful.\n\n\nx <- list(\"Los Angeles\" = 1, Boston = 2, London = 3) \nx\n\n\n$`Los Angeles`\n[1] 1\n\n$Boston\n[1] 2\n\n$London\n[1] 3\n\nnames(x)\n\n\n[1] \"Los Angeles\" \"Boston\"      \"London\"     \n\nMatrices can have both column and row names.\n\n\nm <- matrix(1:4, nrow = 2, ncol = 2)\ndimnames(m) <- list(c(\"a\", \"b\"), c(\"c\", \"d\")) \nm\n\n\n  c d\na 1 3\nb 2 4\n\nColumn names and row names can be set separately using the colnames() and rownames() functions.\n\n\ncolnames(m) <- c(\"h\", \"f\")\nrownames(m) <- c(\"x\", \"z\")\nm\n\n\n  h f\nx 1 3\nz 2 4\n\nNote that for data frames, there is a separate function for setting the row names, the row.names() function. Also, data frames do not have column names, they just have names (like lists). So to set the column names of a data frame just use the names() function. Yes, I know its confusing. Here’s a quick summary:\nObject\nSet column names\nSet row names\ndata frame\nnames()\nrow.names()\nmatrix\ncolnames()\nrownames()\nSummary\nThere are a variety of different builtin-data types in R. In this chapter we have reviewed the following\natomic classes: numeric, logical, character, integer, complex\nvectors, lists\nfactors\nmissing values\ndata frames and matrices\nAll R objects can have attributes that help to describe what is in the object. Perhaps the most useful attribute is names, such as column and row names in a data frame, or simply names in a vector or list. Attributes like dimensions are also important as they can modify the behavior of objects, like turning a vector into a matrix.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nDescribe the difference between is.finite(x) and !is.infinite(x).\nA logical vector can take 3 possible values. How many possible values can an integer vector take? How many possible values can a double take? Use google to do some research.\nWhat functions from the readr package allow you to turn a string into logical, integer, and double vector?\nTry and make a tibble that has columns with different lengths. What happens?\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-r-nuts-and-bolts.html\nhttps://rafalab.github.io/dsbook/r-basics.html\nhttps://r4ds.had.co.nz/vectors.html?q=typeof#vectors\n\n\n\n\n",
    "preview": "https://d33wubrfki0l68.cloudfront.net/1d1b4e1cf0dc5f6e80f621b0225354b0addb9578/6ee1c/diagrams/data-structures-overview.png",
    "last_modified": "2021-09-16T12:48:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-14-ggplot2-plotting-system-part-1/",
    "title": "The ggplot2 plotting system: qplot()",
    "description": "An overview of the ggplot2 plotting system in R with qplot().",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-14",
    "categories": [
      "module 1",
      "week 3",
      "R",
      "programming",
      "ggplot2",
      "data viz"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nThe ggplot2 Plotting System\nThe Basics: qplot()\nBefore you start: label your data\nggplot2 “Hello, world!”\nModifying aesthetics\nAdding a geom\nHistograms and boxplots\nFacets\nSummary\n\nPost-lecture materials\nCase Study: MAACS Cohort\nFinal Questions\nAdditional Resources\n\n\n\n“The greatest value of a picture is when it forces us to notice what we never expected to see.” —John Tukey\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/data-visualisation.html\nhttp://vita.had.co.nz/papers/layered-grammar.pdf\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-the-ggplot2-plotting-system-part-1.html\nLearning objectives\n\nAt the end of this lesson you will:\nRecognize the difference between aesthetics and geoms\nBecome familiar with different types of plots (e.g. scatterplots, boxplots, and histograms)\nBe able to facet plots into a grid\n\nThe ggplot2 Plotting System\nThe ggplot2 package in R is an implementation of The Grammar of Graphics as described by Leland Wilkinson in his book. The package was originally written by Hadley Wickham while he was a graduate student at Iowa State University (he still actively maintains the packgae). The package implements what might be considered a third graphics system for R (along with base graphics and lattice). The package is available from CRAN via install.packages(); the latest version of the source can be found on the package’s GitHub Repository. Documentation of the package can be found at the tidyverse web site.\nThe grammar of graphics represents an abstraction of graphics ideas and objects. You can think of this as developing the verbs, nouns, and adjectives for data graphics. Developing such a grammar allows for a “theory” of graphics on which to build new graphics and graphics objects. To quote from Hadley Wickham’s book on ggplot2, we want to “shorten the distance from mind to page”. In summary,\n\n“…the grammar tells us that a statistical graphic is a mapping from data to aesthetic attributes (colour, shape, size) of geometric objects (points, lines, bars). The plot may also contain statistical transformations of the data and is drawn on a specific coordinate system” – from ggplot2 book\n\nYou might ask yourself “Why do we need a grammar of graphics?” Well, for much the same reasons that having a grammar is useful for spoken languages. The grammar allows for a more compact summary of the base components of a language, and it allows us to extend the language and to handle situations that we have not before seen.\nIf you think about making a plot with the base graphics system, the plot is constructed by calling a series of functions that either create or annotate a plot. There’s no convenient agreed-upon way to describe the plot, except to just recite the series of R functions that were called to create the thing in the first place. In a previous lesson, we described the base plotting system as a kind of “artist’s palette” model, where you start with blank “canvas” and build up from there.\nFor example, consider the following plot made using base graphics.\n\n\nwith(airquality, { \n        plot(Temp, Ozone)\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nFigure 1: Scatterplot of Temperature and Ozone in New York (base graphics)\n\n\n\nHow would one describe the creation of this plot? Well, we could say that we called the plot() function and then added a loess smoother by calling the lines() function on the output of loess.smooth().\nThe base plotting system is convenient and it often mirrors how we think of building plots and analyzing data. But a key drawback is that you can’t go back once plot has started (e.g. to adjust margins), so there is in fact a need to plan in advance. Furthermore, it is difficult to “translate” a plot to others because there’s no formal graphical language; each plot is just a series of R commands.\nHere is the same plot made using ggplot2.\n\n\nlibrary(tidyverse)\nairquality %>%\n        ggplot(aes(Temp, Ozone)) + \n        geom_point() + \n        geom_smooth(method = \"loess\", \n                    se = FALSE) + \n        theme_minimal()\n\n\n\n\nFigure 2: Scatterplot of Temperature and Ozone in New York (ggplot2)\n\n\n\nNote: the output is roughly equivalent, and the amount of code is similar, but ggplot2 allows for a more elegant way of expressing the components of the plot. In this case, the plot is a dataset (airquality) with aesthetic mappings (visual properties of the objects in your plot) derived from the Temp and Ozone variables, a set of points, and a smoother. In a sense, the ggplot2 system takes many of the cues from the base plotting system and formalizes them a bit.\nThe ggplot2 system also takes some cues from lattice. With the lattice system, plots are created with a single function call (xyplot, bwplot, etc.). Things like margins and spacing are set automatically because the entire plot is specified at once. The lattice system is most useful for conditioning types of plots and is good for putting many many plots on a screen. That said, it is sometimes awkward to specify an entire plot in a single function call because many different options have to be specified at once. Furthermore, annotation in plots is not intuitive and the use of panel functions and subscripts is difficult to wield and requires intense preparation.\nThe ggplot2 system essentially takes the good parts of both the base graphics and lattice graphics system. It automatically handles things like margins and spacing, and also has the concept of “themes” which provide a default set of plotting symbols and colors. While ggplot2 bears a superficial similarity to lattice, ggplot2 is generally easier and more intuitive to use. The default themes makes many choices for you, but you can customize the presentation if you want.\nThe Basics: qplot()\nThe qplot() function in ggplot2 is meant to get you going quickly. It works much like the plot() function in base graphics system. It looks for variables to plot within a data frame, similar to lattice, or in the parent environment. In general, it is good to get used to putting your data in a data frame and then passing it to qplot().\n\nPro tip: The qplot() function is somewhat discouraged in ggplot2 now and new users are encouraged to use the more general ggplot() function (more details in the next lesson).\nHowever, the qplot() function is still useful and may be easier to use if transitioning from the base plotting system or a different statistical package.\n\nPlots are made up of aesthetics (e.g. size, shape, color) and geoms (e.g. points, lines). Factors play an important role for indicating subsets of the data (if they are to have different properties) so they should be labeled properly. The qplot() hides much of what goes on underneath, which is okay for most operations, ggplot() is the core function and is very flexible for doing things qplot() cannot do.\nBefore you start: label your data\nOne thing that is always true, but is particularly useful when using ggplot2, is that you should always use informative and descriptive labels on your data. More generally, your data should have appropriate metadata so that you can quickly look at a dataset and know\nwhat the variables are\nwhat the values of each variable mean\nThis means that each column of a data frame should have a meaningful (but concise) variable name that accurately reflects the data stored in that column. Also, non-numeric or categorical variables should be coded as factor variables and have meaningful labels for each level of the factor. For example, it is common to code a binary variable as a “0” or a “1”, but the problem is that from quickly looking at the data, it’s impossible to know whether which level of that variable is represented by a “0” or a “1”. Much better to simply label each observation as what they are. If a variable represents temperature categories, it might be better to use “cold”, “mild”, and “hot” rather than “1”, “2”, and “3”.\nWhile it is sometimes a pain to make sure all of your data are properly labeled, this investment in time can pay dividends down the road when you’re trying to figure out what you were plotting. In other words, including the proper metadata can make your exploratory plots essentially self-documenting.\nggplot2 “Hello, world!”\nThis example dataset comes with the ggplot2 package and contains data on the fuel economy of 38 popular car models from 1999 to 2008.\n\n\nlibrary(tidyverse) # this loads the ggplot2 R package\n# library(ggplot2) # an alternative way to just load the ggplot2 R package\nglimpse(mpg)\n\n\nRows: 234\nColumns: 11\n$ manufacturer <chr> \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\",…\n$ model        <chr> \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 q…\n$ displ        <dbl> 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.…\n$ year         <int> 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999,…\n$ cyl          <int> 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6,…\n$ trans        <chr> \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(a…\n$ drv          <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4…\n$ cty          <int> 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15,…\n$ hwy          <int> 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25,…\n$ fl           <chr> \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        <chr> \"compact\", \"compact\", \"compact\", \"compact\", \"co…\n\nYou can see from the glimpse() (part of the dplyr package) output that all of the categorical variables (like “manufacturer” or “class”) are appropriately coded with meaningful labels. This will come in handy when qplot() has to label different aspects of a plot. Also note that all of the columns/variables have meaningful (if sometimes abbreviated) names, rather than names like “X1”, and “X2”, etc.\nWe can make a quick scatterplot of the engine displacement (displ) and the highway miles per gallon (hwy).\n\n\nqplot(x = displ, y = hwy, data = mpg)\n\n\n\n\nFigure 3: Plot of engine displacement and highway mileage using the mtcars dataset\n\n\n\nIt has a very similar feeling to plot() in base R.\n\nNote: In the call to qplot() you must specify the data argument so that qplot() knows where to look up the variables.\nYou must also specify x and y, but hopefully that part is obvious.\n\nModifying aesthetics\nWe can introduce a third variable into the plot by modifying the color of the points based on the value of that third variable.\nColor (or colour) is one type of aesthetic and using the ggplot2 language:\n\n“the color of each point can be mapped to a variable”.\n\nThis sounds technical, but let’s give an example.\n\nNote: the x-coordinates and y-coordinates are aesthetics too, and they got mapped to the displ and hwy variables, respectively\n\nHere, we will map the color to the drv variable, which indicates whether a car is front wheel drive, rear wheel drive, or 4-wheel drive.\n\n\nqplot(displ, hwy, data = mpg, color = drv)\n\n\n\n\nFigure 4: Engine displacement and highway mileage by drive class\n\n\n\nNow we can see that the front wheel drive cars tend to have lower displacement relative to the 4-wheel or rear wheel drive cars. Also, it’s clear that the 4-wheel drive cars have the lowest highway gas mileage.\n\nQuestion: In the above plot, I did not specify the x and y variable. What happens when you run these two code chunks. What’s the difference?\n\n\nqplot(displ, hwy, data = mpg, color = drv)\n\n\n\n\n\nqplot(x = displ, y = hwy, data = mpg, color = drv)\n\n\n\n\n\nqplot(hwy, displ, data = mpg, color = drv)\n\n\n\n\n\nqplot(y = hwy, x = displ, data = mpg, color = drv)\n\n\n\n\n\nExample: Let’s try mapping colors in another dataset, namely the palmerpenguins dataset. These data contain observations for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica.\n\n\nlibrary(palmerpenguins)\nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species           <fct> Adelie, Adelie, Adelie, Adelie, Adelie, Ad…\n$ island            <fct> Torgersen, Torgersen, Torgersen, Torgersen…\n$ bill_length_mm    <dbl> 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39…\n$ bill_depth_mm     <dbl> 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19…\n$ flipper_length_mm <int> 181, 186, 195, NA, 193, 190, 181, 195, 193…\n$ body_mass_g       <int> 3750, 3800, 3250, NA, 3450, 3650, 3625, 46…\n$ sex               <fct> male, female, female, NA, female, male, fe…\n$ year              <int> 2007, 2007, 2007, 2007, 2007, 2007, 2007, …\n\nIf we wanted to count the number of penguins for each of the three species, we can use the count() function in dplyr:\n\n\npenguins %>% \n  count(species)\n\n\n# A tibble: 3 × 2\n  species       n\n  <fct>     <int>\n1 Adelie      152\n2 Chinstrap    68\n3 Gentoo      124\n\n\n\n\nFigure 5: Palmer penguins\n\n\n\n[Source: Artwork by Allison Horst]\nQuestion: If we wanted to use qplot() to map flipper_length_mm and bill_length_mm to the x and y coordinates, what would we do?\n\n\n# try it yourself\n\n\n\nNow try mapping color to the species variable on top of the code you just wrote:\n\n\n# try it yourself\n\n\n\n\nAdding a geom\nSometimes it is nice to add a smoother to a scatterplot to highlight any trends. Trends can be difficult to see if the data are very noisy or there are many data points obscuring the view. A smooth is a “geom” that you can add along with your data points.\n\n\nqplot(displ, hwy, data = mpg, geom = c(\"point\", \"smooth\"))\n\n\n\n\nFigure 6: Engine displacement and highway mileage w/smoother\n\n\n\nNote that previously, we did not have to specify geom = \"point\" because that was done automatically. But if you want the smoother overlaid with the points, then you need to specify both explicitly.\nHere it seems that engine displacement and highway mileage have a nonlinear U-shaped relationship, but from the previous plot we know that this is largely due to confounding by the drive class of the car.\nLook at what happens if we do not include the point geom.\n\n\nqplot(displ, hwy, data = mpg, geom = c(\"smooth\"))\n\n\n\n\nFigure 7: Engine displacement and highway mileage w/smoother\n\n\n\nSometimes that is the plot you want to show, but in this case it might make more sense to show the data along with the smoother.\n\nExample: Let’s add a smoother to our palmerpenguins dataset example. Using the code we previously wrote mapping variables to points and color, add a “point” and “smooth” geom:\n\n\n# try it yourself\n\n\n\n\nHistograms and boxplots\nThe qplot() function can be used to be used to plot 1-dimensional data too. By specifying a single variable, qplot() will by default make a histogram.\nHere, we make a histogram if the highway mileage data and stratify on the drive class. So technically this is three histograms overlayed on top of each other.\n\n\nqplot(hwy, data = mpg, fill = drv, binwidth = 2)\n\n\n\n\nFigure 8: Histogram of highway mileage by drive class\n\n\n\n\nQuestion: Notice, I used fill here to map color to the drv variable. Why is this? What happens when you use color instead?\n\n\n# try it yourself\n\n\n\n\nHaving the different colors for each drive class is nice, but the three histograms can be a bit difficult to separate out. Side-by-side boxplots is one solution to this problem.\n\n\nqplot(drv, hwy, data = mpg, geom = \"boxplot\")\n\n\n\n\nFigure 9: Boxplots of highway mileage by drive class\n\n\n\nAnother solution is to plot the histograms in separate panels using facets.\nFacets\nFacets are a way to create multiple panels of plots based on the levels of categorical variable. Here, we want to see a histogram of the highway mileages and the categorical variable is the drive class variable. We can do that using the facets argument to qplot().\nThe facets argument expects a formula type of input, with a ~ separating the left hand side variable and the right hand side variable. The left hand side variable indicates how the rows of the panels should be divided and the right hand side variable indicates how the columns of the panels should be divided. Here, we just want three rows of histograms (and just one column), one for each drive class, so we specify drv on the left hand side and . on the right hand side indicating that there’s no variable there (it’s empty).\n\n\nqplot(hwy, data = mpg, facets = drv ~ ., binwidth = 2)\n\n\n\n\nFigure 10: Histogram of highway mileage by drive class\n\n\n\nWe could also look at more data using facets, so instead of histograms we could look at scatterplots of engine displacement and highway mileage by drive class. Here, we put the drv variable on the right hand side to indicate that we want a column for each drive class (as opposed to splitting by rows like we did above).\n\n\nqplot(displ, hwy, data = mpg, facets = . ~ drv)\n\n\n\n\nFigure 11: Engine displacement and highway mileage by drive class\n\n\n\nWhat if you wanted to add a smoother to each one of those panels? Simple, you literally just add the smoother as another geom.\n\n\nqplot(displ, hwy, data = mpg, facets = . ~ drv) + geom_smooth(method = \"lm\")\n\n\n\n\nFigure 12: Engine displacement and highway mileage by drive class w/smoother\n\n\n\n\nNote: we used a different type of smoother. Here, we add a linear regression line (a type of smoother) to each group to see if there’s any difference.\n\n\nExample: Let’s facet our palmerpenguins dataset example and explore different types of plots.\nBuilding off the code we previously wrote, perform the following tasks:\nFacet the plot based on species with the the three species along rows.\nAdd a linear regression line to each the types of species\n\n\n# try it yourself\n\n\n\nNext, make a histogram of the body_mass_g for each of the species colored by the three species.\n\n\n# try it yourself\n\n\n\n\nSummary\nThe qplot() function in ggplot2 is the analog of plot() in base graphics but with many built-in features that the traditionaly plot() does not provide. The syntax is somewhere in between the base and lattice graphics system. The qplot() function is useful for quickly putting data on the page/screen, but for ultimate customization, it may make more sense to use some of the lower level functions that we discuss later in the next lesson.\nPost-lecture materials\nCase Study: MAACS Cohort\n\nClick here for case study practicing the qplot() function.\nThis case study will use data based on the Mouse Allergen and Asthma Cohort Study (MAACS). This study was aimed at characterizing the indoor (home) environment and its relationship with asthma morbidity amonst children aged 5–17 living in Baltimore, MD. The children all had persistent asthma, defined as having had an exacerbation in the past year. A representative publication of results from this study can be found in this paper by Lu, et al.\n\nBecause the individual-level data for this study are protected by various U.S. privacy laws, we cannot make those data available. For the purposes of this lesson, we have simulated data that share many of the same features of the original data, but do not contain any of the actual measurements or values contained in the original dataset.\n\nHere is a snapshot of what the data look like.\n\n\nlibrary(here)\nmaacs <- read_csv(here(\"data\", \"maacs_sim.csv\"), col_types = \"icnn\")\nmaacs\n\n\n# A tibble: 750 × 4\n      id mopos  pm25    eno\n   <int> <chr> <dbl>  <dbl>\n 1     1 yes    6.01  28.8 \n 2     2 no    25.2   17.7 \n 3     3 yes   21.8   43.6 \n 4     4 no    13.4  288.  \n 5     5 no    49.4    7.60\n 6     6 no    43.4   12.0 \n 7     7 yes   33.0   79.2 \n 8     8 yes   32.7   34.2 \n 9     9 yes   52.2   12.1 \n10    10 yes   51.9   65.0 \n# … with 740 more rows\n\nThe key variables are:\nmopos: an indicator of whether the subject is allergic to mouse allergen (yes/no)\npm25: average level of PM2.5 over the course of 7 days (micrograms per cubic meter)\neno: exhaled nitric oxide\nThe outcome of interest for this analysis will be exhaled nitric oxide (eNO), which is a measure of pulmonary inflamation. We can get a sense of how eNO is distributed in this population by making a quick histogram of the variable. Here, we take the log of eNO because some right-skew in the data.\n\n\nqplot(log(eno), data = maacs)\n\n\n\n\nFigure 13: Histogram of log eNO\n\n\n\nA quick glance suggests that the histogram is a bit “fat”, suggesting that there might be multiple groups of people being lumped together. We can stratify the histogram by whether they are allergic to mouse.\n\n\nqplot(log(eno), data = maacs, fill = mopos)\n\n\n\n\nFigure 14: Histogram of log eNO by mouse allergic status\n\n\n\nWe can see from this plot that the non-allergic subjects are shifted slightly to the left, indicating a lower eNO and less pulmonary inflammation. That said, there is significant overlap between the two groups.\nAn alternative to histograms is a density smoother, which sometimes can be easier to visualize when there are multiple groups. Here is a density smooth of the entire study population.\n\n\nqplot(log(eno), data = maacs, geom = \"density\")\n\n\n\n\nFigure 15: Density smooth of log eNO\n\n\n\nAnd here are the densities straitified by allergic status. We can map the color aesthetic to the mopos variable.\n\n\nqplot(log(eno), data = maacs, geom = \"density\", color = mopos)\n\n\n\n\nFigure 16: Density smooth of log eNO by mouse allergic status\n\n\n\nThese tell the same story as the stratified histograms, which should come as no surprise.\nNow we can examine the indoor environment and its relationship to eNO. Here, we use the level of indoor PM2.5 as a measure of indoor environment air quality. We can make a simple scatterplot of PM2.5 and eNO.\n\n\nqplot(log(pm25), log(eno), data = maacs, geom = c(\"point\", \"smooth\"))\n\n\n\n\nFigure 17: eNO and PM2.5\n\n\n\nThe relationship appears modest at best, as there is substantial noise in the data. However, one question that we might be interested in is whether allergic individuals are perhaps more sensitive to PM2.5 inhalation than non-allergic individuals. To examine that question we can stratify the data into two groups.\nThis first plot uses different plot symbols for the two groups and overlays them on a single canvas. We can do this by mapping the mopos variable to the shape aesthetic.\n\n\nqplot(log(pm25), log(eno), data = maacs, shape = mopos)\n\n\n\n\nFigure 18: eNO and PM2.5 by mouse allergic status\n\n\n\nBecause there is substantial overlap in the data it is a bit challenging to discern the circles from the triangles. Part of the reason might be that all of the symbols are the same color (black).\nWe can plot each group a different color to see if that helps.\n\n\nqplot(log(pm25), log(eno), data = maacs, color = mopos)\n\n\n\n\nFigure 19: eNO and PM2.5 by mouse allergic status\n\n\n\nThis is slightly better but the substantial overlap makes it difficult to discern any trends in the data. For this we need to add a smoother of some sort. Here we add a linear regression line (a type of smoother) to each group to see if there’s any difference.\n\n\nqplot(log(pm25), log(eno), data = maacs, color = mopos) + \n        geom_smooth(method = \"lm\")\n\n\n\n\nHere we see quite clearly that the red group and the green group exhibit rather different relationships between PM2.5 and eNO. For the non-allergic individuals, there appears to be a slightly negative relationship between PM2.5 and eNO and for the allergic individuals, there is a positive relationship. This suggests a strong interaction between PM2.5 and allergic status, an hypothesis perhaps worth following up on in greater detail than this brief exploratory analysis.\nAnother, and perhaps more clear, way to visualize this interaction is to use separate panels for the non-allergic and allergic individuals using the facets argument to qplot().\n\n\nqplot(log(pm25), log(eno), data = maacs, facets = . ~ mopos) + \n        geom_smooth(method = \"lm\")\n\n\n\n\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is gone wrong with this code? Why are the points not blue?\n\n\nqplot(x = displ, y = hwy, data = mpg, color = \"blue\")\n\n\n\n\nWhich variables in mpg are categorical? Which variables are continuous? (Hint: type ?mpg to read the documentation for the dataset). How can you see this information when you run mpg?\nMap a continuous variable to color, size, and shape aesthetics. How do these aesthetics behave differently for categorical vs. continuous variables?\n\nAdditional Resources\n\nhttps://r4ds.had.co.nz/data-visualisation.html\nhttps://rdpeng.github.io/Biostat776/lecture-the-ggplot2-plotting-system-part-1.html\n\n\n\n\n",
    "preview": "posts/2021-09-14-ggplot2-plotting-system-part-1/ggplot2-plotting-system-part-1_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-09-13T21:15:21-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 960
  },
  {
    "path": "posts/2021-09-14-ggplot2-plotting-system-part-2/",
    "title": "The ggplot2 plotting system: ggplot()",
    "description": "An overview of the ggplot2 plotting system in R with ggplot().",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-14",
    "categories": [
      "module 1",
      "week 3",
      "R",
      "programming",
      "ggplot2",
      "data viz"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nThe ggplot2 Plotting System\nBasic components of a ggplot2 plot\nExample: BMI, PM2.5, Asthma\nBuilding up in layers\nFirst plot with point layer\nAdding more layers: smooth\nAdding more layers: facets\nModifying geom properties\nModifying labels\nCustomizing the smooth\nChanging the theme\nMore complex example\nA quick aside about axis limits\nResources\n\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/data-visualisation.html\nhttp://vita.had.co.nz/papers/layered-grammar.pdf\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-the-ggplot2-plotting-system-part-2.html\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to build up layers of graphics using ggplot()\nBe able to modify properties of a ggplot() including layers and labels\n\nThe ggplot2 Plotting System\nIn this lesson, we will get into a little more of the nitty gritty of how ggplot2 builds plots and how you can customize various aspects of any plot.\nIn the previous lesson, we used the qplot() function to quickly put points on a page. The qplot() function’s syntax is very similar to that of the plot() function in base graphics so for those switching over, it makes for an easy transition. But it is worth knowing the underlying details of how ggplot2 works so that you can really exploit its power.\nBasic components of a ggplot2 plot\nA ggplot2 plot consists of a number of key components. Here are a few of the more commonly used ones.\nA data frame: stores all of the data that will be displayed on the plot\naesthetic mappings: describe how data are mapped to color, size, shape, location\ngeoms: geometric objects like points, lines, shapes.\nfacets: describes how conditional/panel plots should be constructed\nstats: statistical transformations like binning, quantiles, smoothing.\nscales: what scale an aesthetic map uses (example: left-handed = red, right-handed = blue).\ncoordinate system: describes the system in which the locations of the geoms will be drawn\nIt is essential that you properly organize your data into a data frame before you start with ggplot2. In particular, it is important that you provide all of the appropriate metadata so that your data frame is self-describing and your plots will be self-documenting.\nWhen building plots in ggplot2 (rather than using qplot()), the “artist’s palette” model may be the closest analogy. Essentially, you start with some raw data, and then you gradually add bits and pieces to it to create a plot. Plots are built up in layers, with the typically ordering being\nPlot the data\nOverlay a summary\nAdd metadata and annotation\nFor quick exploratory plots you may not get past step 1.\nExample: BMI, PM2.5, Asthma\nTo demonstrate the various pieces of ggplot2 we will use a running example from the Mouse Allergen and Asthma Cohort Study (MAACS), which was used as a case study in the previous lesson. Here, the question we are interested in is\n\n“Are overweight individuals, as measured by body mass index (BMI), more susceptible than normal weight individuals to the harmful effects of PM2.5 on asthma symptoms?”\n\nThere is a suggestion that overweight individuals may be more susceptible to the negative effects of inhaling PM2.5. This would suggest that increases in PM2.5 exposure in the home of an overweight child would be more deleterious to his/her asthma symptoms than they would be in the home of a normal weight child. We want to see if we can see that difference in the data from MAACS.\n\nBecause the individual-level data for this study are protected by various U.S. privacy laws, we cannot make those data available. For the purposes of this lesson, we have simulated data that share many of the same features of the original data, but do not contain any of the actual measurements or values contained in the original dataset.\n\nWe can look at the data quickly by reading it in as a tibble with read_csv() in the tidyverse package.\n\n\nlibrary(tidyverse)\nlibrary(here)\nmaacs <- read_csv(here(\"data\", \"bmi_pm25_no2_sim.csv\"),\n                  col_types = \"nnci\")\nmaacs\n\n\n# A tibble: 517 × 4\n   logpm25 logno2_new bmicat        NocturnalSympt\n     <dbl>      <dbl> <chr>                  <int>\n 1   1.25       1.18  normal weight              1\n 2   1.12       1.55  overweight                 0\n 3   1.93       1.43  normal weight              0\n 4   1.37       1.77  overweight                 2\n 5   0.775      0.765 normal weight              0\n 6   1.49       1.11  normal weight              0\n 7   2.16       1.43  normal weight              0\n 8   1.65       1.40  normal weight              0\n 9   1.55       1.81  normal weight              0\n10   2.04       1.35  overweight                 3\n# … with 507 more rows\n\nThe outcome we will look at here, NocturnalSymp, is the number of days in the past 2 weeks where the child experienced asthma symptoms (e.g. coughing, wheezing) while sleeping.\nThe other key variables are:\nlogpm25: average level of PM2.5 over the course of 7 days (micrograms per cubic meter) on the log scale\nlogno2_new: exhaled nitric oxide on the log scale\nbmicat: categorical variable with BMI status\nBuilding up in layers\nFirst, we can create a ggplot object that stores the dataset and the basic aesthetics for mapping the x- and y-coordinates for the plot. Here, we will eventually be plotting the log of PM2.5 and NocturnalSymp variable.\n\n\ng <- ggplot(maacs, aes(x = logpm25, y = NocturnalSympt))\nsummary(g)\n\n\ndata: logpm25, logno2_new, bmicat, NocturnalSympt [517x4]\nmapping:  x = ~logpm25, y = ~NocturnalSympt\nfaceting: <ggproto object: Class FacetNull, Facet, gg>\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  <ggproto object: Class FacetNull, Facet, gg>\n\nclass(g)\n\n\n[1] \"gg\"     \"ggplot\"\n\nYou can see above that the object g contains the dataset maacs and the mappings.\nNow, normally if you were to print() a ggplot object a plot would appear on the plot device, however, our object g actually does not contain enough information to make a plot yet.\n\n\ng <- maacs %>%\n        ggplot(aes(logpm25, NocturnalSympt))\nprint(g)\n\n\n\n\nFigure 1: Nothing to see here!\n\n\n\nFirst plot with point layer\nTo make a scatterplot we need add at least one geom, such as points. Here, we add the geom_point() function to create a traditional scatterplot.\n\n\ng <- maacs %>%\n        ggplot(aes(logpm25, NocturnalSympt))\ng + geom_point()\n\n\n\n\nHow does ggplot know what points to plot? In this case, it can grab them from the data frame maacs that served as the input into the ggplot() function.\nAdding more layers: smooth\nBecause the data appear rather noisy, it might be better if we added a smoother on top of the points to see if there is a trend in the data with PM2.5.\n\n\ng + geom_point() + geom_smooth()\n\n\n\n\nFigure 2: Scatterplot with smoother\n\n\n\nThe default smoother is a loess smoother, which is flexible and nonparametric but might be too flexible for our purposes. Perhaps we’d prefer a simple linear regression line to highlight any first order trends. We can do this by specifying method = \"lm\" to geom_smooth().\n\n\ng + geom_point() + geom_smooth(method = \"lm\")\n\n\n\n\nFigure 3: Scatterplot with linear regression line\n\n\n\nHere, we can see there appears to be a slight increasing trend, suggesting that higher levels of PM2.5 are associated with increased days with nocturnal symptoms.\n\nExample: Let’s use the ggplot() function with our palmerpenguins dataset example and make a scatter plot with flipper_length_mm on the x-axis, bill_length_mm on the y-axis, colored by species, and a smoother by adding a linear regression.\n\n\n# try it yourself\n\nlibrary(palmerpenguins)\npenguins \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <fct>   <fct>              <dbl>         <dbl>             <int>\n 1 Adelie  Torgersen           39.1          18.7               181\n 2 Adelie  Torgersen           39.5          17.4               186\n 3 Adelie  Torgersen           40.3          18                 195\n 4 Adelie  Torgersen           NA            NA                  NA\n 5 Adelie  Torgersen           36.7          19.3               193\n 6 Adelie  Torgersen           39.3          20.6               190\n 7 Adelie  Torgersen           38.9          17.8               181\n 8 Adelie  Torgersen           39.2          19.6               195\n 9 Adelie  Torgersen           34.1          18.1               193\n10 Adelie  Torgersen           42            20.2               190\n# … with 334 more rows, and 3 more variables: body_mass_g <int>,\n#   sex <fct>, year <int>\n\n\nAdding more layers: facets\nBecause our primary question involves comparing overweight individuals to normal weight individuals, we can stratify the scatter plot of PM2.5 and nocturnal symptoms by the BMI category (bmicat) variable, which indicates whether an individual is overweight or now. To visualize this we can add a facet_grid(), which takes a formula argument. Here we want one row and two columns, one column for each weight category. So we specify bmicat on the right hand side of the forumla passed to facet_grid().\n\n\ng + geom_point() + \n        geom_smooth(method = \"lm\") +\n        facet_grid(. ~ bmicat) \n\n\n\n\nFigure 4: Scatterplot of PM2.5 and nocturnal symptoms by BMI category\n\n\n\nNow it seems clear that the relationship between PM2.5 and nocturnal symptoms is relatively flat among normal weight individuals, while the relationship is increasing among overweight individuals. This plot suggests that overweight individuals may be more susceptible to the effects of PM2.5.\nThere are a variety of annotations you can add to a plot, including different kinds of labels. You can use xlab() for x-axis labels, ylab() for y-axis labels, and ggtitle() for specifying plot titles. The labs() function is generic and can be used to modify multiple types of labels at once.\nFor things that only make sense globally, use theme(), i.e. theme(legend.position = \"none\"). Two standard appearance themes are included\ntheme_gray(): The default theme (gray background)\ntheme_bw(): More stark/plain\nModifying geom properties\nYou can modify properties of geoms by specifying options to their respective geom_* functions. For example, here we modify the points in the scatterplot to make the color “steelblue”, the size larger , and the alpha transparency greater.\n\n\ng + geom_point(color = \"steelblue\", size = 4, alpha = 1/2)\n\n\n\n\nFigure 5: Modifying point color with a constant\n\n\n\nIn addition to setting specific geom attributes to constants, we can map aesthetics to variables. So, here, we map the color aesthetic color to the variable bmicat, so the points will be colored according to the levels of bmicat. We use the aes() function to indicate this difference from the plot above.\n\n\ng + geom_point(aes(color = bmicat), size = 4, alpha = 1/2)\n\n\n\n\nFigure 6: Mapping color to a variable\n\n\n\nModifying labels\nHere is an example of modifying the title and the x and y labels to make the plot a bit more informative.\n\n\ng + geom_point(aes(color = bmicat)) + \n        labs(title = \"MAACS Cohort\") + \n        labs(x = expression(\"log \" * PM[2.5]), y = \"Nocturnal Symptoms\")\n\n\n\n\nFigure 7: Modifying plot labels\n\n\n\nCustomizing the smooth\nWe can also customize aspects of the smoother that we overlay on the points with geom_smooth(). Here we change the line type and increase the size from the default. We also remove the shaded standard error from the line.\n\n\ng + geom_point(aes(color = bmicat), \n               size = 2, alpha = 1/2) + \n        geom_smooth(size = 4, linetype = 3, \n                    method = \"lm\", se = FALSE)\n\n\n\n\nFigure 8: Customizing a smoother\n\n\n\nChanging the theme\nThe default theme for ggplot2 uses the gray background with white grid lines. If you don’t find this suitable, you can use the black and white theme by using the theme_bw() function. The theme_bw() function also allows you to set the typeface for the plot, in case you don’t want the default Helvetica. Here we change the typeface to Times.\n\n\ng + geom_point(aes(color = bmicat)) + \n        theme_bw(base_family = \"Times\")\n\n\n\n\nFigure 9: Modifying the theme for a plot\n\n\n\n\nExample: Let’s take our palmerpenguins scatterplot from above and change out the theme to use theme_dark().\n\n\n# try it yourself\n\nlibrary(palmerpenguins)\npenguins \n\n\n# A tibble: 344 × 8\n   species island    bill_length_mm bill_depth_mm flipper_length_mm\n   <fct>   <fct>              <dbl>         <dbl>             <int>\n 1 Adelie  Torgersen           39.1          18.7               181\n 2 Adelie  Torgersen           39.5          17.4               186\n 3 Adelie  Torgersen           40.3          18                 195\n 4 Adelie  Torgersen           NA            NA                  NA\n 5 Adelie  Torgersen           36.7          19.3               193\n 6 Adelie  Torgersen           39.3          20.6               190\n 7 Adelie  Torgersen           38.9          17.8               181\n 8 Adelie  Torgersen           39.2          19.6               195\n 9 Adelie  Torgersen           34.1          18.1               193\n10 Adelie  Torgersen           42            20.2               190\n# … with 334 more rows, and 3 more variables: body_mass_g <int>,\n#   sex <fct>, year <int>\n\n\nMore complex example\nNow you get the sense that plots in the ggplot2 system are constructed by successively adding components to the plot, starting with the base dataset and maybe a scatterplot. In this section bleow, you can see a slightly more complicated example with an additional variable.\n\nClick here for a slightly more complicated example with ggplot().\nNow, we will ask the question\n\nHow does the relationship between PM2.5 and nocturnal symptoms vary by BMI category and nitrogen dioxide (NO2)?\n\nUnlike our previous BMI variable, NO2 is continuous, and so we need to make NO2 categorical so we can condition on it in the plotting. We can use the cut() function for this purpose. We will divide the NO2 variable into tertiles.\nFirst we need to calculate the tertiles with the quantile() function.\n\n\ncutpoints <- quantile(maacs$logno2_new, seq(0, 1, length = 4), na.rm = TRUE)\n\n\n\nThen we need to divide the original logno2_new variable into the ranges defined by the cut points computed above.\n\n\nmaacs$no2tert <- cut(maacs$logno2_new, cutpoints)\n\n\n\nThe not2tert variable is now a categorical factor variable containing 3 levels, indicating the ranges of NO2 (on the log scale).\n\n\n## See the levels of the newly created factor variable\nlevels(maacs$no2tert)\n\n\n[1] \"(0.342,1.23]\" \"(1.23,1.47]\"  \"(1.47,2.17]\" \n\nThe final plot shows the relationship between PM2.5 and nocturnal symptoms by BMI category and NO2 tertile.\n\n\n## Setup ggplot with data frame\ng <- maacs %>%\n        ggplot(aes(logpm25, NocturnalSympt))\n\n## Add layers\ng + geom_point(alpha = 1/3) + \n        facet_grid(bmicat ~ no2tert) + \n        geom_smooth(method=\"lm\", se=FALSE, col=\"steelblue\") + \n        theme_bw(base_family = \"Avenir\", base_size = 10) + \n        labs(x = expression(\"log \" * PM[2.5])) + \n        labs(y = \"Nocturnal Symptoms\") + \n        labs(title = \"MAACS Cohort\")\n\n\n\n\nFigure 10: PM2.5 and nocturnal symptoms by BMI category and NO2 tertile\n\n\n\nA quick aside about axis limits\nOne quick quirk about ggplot2 that caught me up when I first started using the package can be displayed in the following example. I make a lot of time series plots and I often want to restrict the range of the y-axis while still plotting all the data. In the base graphics system you can do that as follows.\n\n\ntestdat <- data.frame(x = 1:100, y = rnorm(100))\ntestdat[50,2] <- 100  ## Outlier!\nplot(testdat$x, testdat$y, type = \"l\", ylim = c(-3,3))\n\n\n\n\nFigure 11: Time series plot with base graphics\n\n\n\nHere I’ve restricted the y-axis range to be between -3 and 3, even though there is a clear outlier in the data.\nWith ggplot2 the default settings will give you this.\n\n\ng <- ggplot(testdat, aes(x = x, y = y))\ng + geom_line()\n\n\n\n\nFigure 12: Time series plot with default settings\n\n\n\nModifying the ylim() attribute would seem to give you the same thing as the base plot, but it doesn’t.\n\n\ng + geom_line() + ylim(-3, 3)\n\n\n\n\nFigure 13: Time series plot with modified ylim\n\n\n\nEffectively, what this does is subset the data so that only observations between -3 and 3 are included, then plot the data.\nTo plot the data without subsetting it first and still get the restricted range, you have to do the following.\n\n\ng + geom_line() + coord_cartesian(ylim = c(-3, 3))\n\n\n\n\nFigure 14: Time series plot with restricted y-axis range\n\n\n\nAnd now you know!\nResources\nThe ggplot2 book by Hadley Wickham\nThe R Graphics Cookbook by Winston Chang (examples in base plots and in ggplot2)\ntidyverse web site\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat happens if you facet on a continuous variable?\nRead ?facet_wrap. What does nrow do? What does ncol do? What other options control the layout of the individual panels? Why doesn’t facet_grid() have nrow and ncol arguments?\nWhat geom would you use to draw a line chart? A boxplot? A histogram? An area chart?\nWhat does geom_col() do? How is it different to geom_bar()?\n\nAdditional Resources\n\nhttps://r4ds.had.co.nz/data-visualisation.html\nhttps://rdpeng.github.io/Biostat776/lecture-the-ggplot2-plotting-system-part-2.html\n\n\n\n\n",
    "preview": "posts/2021-09-14-ggplot2-plotting-system-part-2/ggplot2-plotting-system-part-2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-09-13T21:15:36-04:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2021-09-07-joining-data-in-r/",
    "title": "Joining data in R",
    "description": "Introduction to relational data and join functions in the dplyr R package.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "dplyr",
      "here",
      "tidyverse"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nRelational data\nKeys\n\nMutating joins\nThe first table\nA second table\nLeft Join\nLeft Join with Incomplete Data\nInner Join\nRight Join\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/relational-data.html\nhttps://rafalab.github.io/dsbook/joining-tables.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-joining-data-in-r-basics.html\nhttps://r4ds.had.co.nz/relational-data.html\nhttps://rafalab.github.io/dsbook/joining-tables.html\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to define relational data and keys\nBe able to define the three types of join functions for relational data\nBe able to implement mutational join functions\n\nRelational data\nData analyses rarely involve only a single table of data. Typically you have many tables of data, and you must combine them to answer the questions that you are interested in. Collectively, multiple tables of data are called relational data because it is the relations, not just the individual datasets, that are important.\nRelations are always defined between a pair of tables. All other relations are built up from this simple idea: the relations of three or more tables are always a property of the relations between each pair. Sometimes both elements of a pair can be the same table! This is needed if, for example, you have a table of people, and each person has a reference to their parents.\nTo work with relational data you need verbs that work with pairs of tables. There are three families of verbs designed to work with relational data:\nMutating joins: A mutating join allows you to combine variables from two tables. It first matches observations by their keys, then copies across variables from one table to the other on the right side of the table (similar to mutate()). We will discuss a few of these below.\nFiltering joins: filtering joins match observations in the same way as mutating joins, but affect the observations, not the variables (i.e. filter observations from one data frame based on whether or not they match an observation in the other). Two types: semi_join(x, y) and anti_join(x, y).\nSet operations: treat observations as if they were set elements. Typically used less frequently, but occasionally useful when you want to break a single complex filter into simpler pieces. All these operations work with a complete row, comparing the values of every variable. These expect the x and y inputs to have the same variables, and treat the observations like sets: e.g. intersect(x, y), union(x, y), and setdiff(x, y).\nKeys\nThe variables used to connect each pair of tables are called keys. A key is a variable (or set of variables) that uniquely identifies an observation. In simple cases, a single variable is sufficient to identify an observation.\nThere are two types of keys:\nA primary key uniquely identifies an observation in its own table.\nA foreign key uniquely identifies an observation in another table.\nMutating joins\nThe dplyr package provides a set of functions for joining two data frames into a single data frame based on a set of key columns. There are several functions in the *_join() family. These functions all merge together two data frames; they differ in how they handle observations that exist in one but not both data frames. Here, are the four functions from this family that you will likely use the most often:\n\nFunction\nWhat it includes in merged data frame\nleft_join()\nIncludes all observations in the left data frame, whether or not there is a match in the right data frame\nright_join()\nIncludes all observations in the right data frame, whether or not there is a match in the left data frame\ninner_join()\nIncludes only observations that are in both data frames\nfull_join()\nIncludes all observations from both data frames\n\nThe first table\nImagine you are conduct a study and collecting data on subjects and a health outcome. Often, subjects will make multiple visits (a so-called longitudinal study) and so we will record the outcome for each visit. Similarly, we may record other information about them, such as the kind of housing they live in.\nThis code creates a simple table with some made up data about some hypothetical subjects’ outcomes.\n\n\nlibrary(tidyverse)\n\noutcomes <- tibble(\n        id = rep(c(\"a\", \"b\", \"c\"), each = 3),\n        visit = rep(0:2, 3),\n        outcome = rnorm(3 * 3, 3)\n)\n\nprint(outcomes)\n\n\n# A tibble: 9 × 3\n  id    visit outcome\n  <chr> <int>   <dbl>\n1 a         0    3.45\n2 a         1    4.04\n3 a         2    2.68\n4 b         0    2.53\n5 b         1    4.38\n6 b         2    3.36\n7 c         0    3.22\n8 c         1    2.05\n9 c         2    3.63\n\nNote that subjects are labeled by their id in the id column.\nA second table\nHere is some code to create a second table (we will be joining the first and second tables shortly). This table contains some data about the hypothetical subjects’ housing situation by recording the type of house they live in.\n\n\nsubjects <- tibble(\n        id = c(\"a\", \"b\", \"c\"),\n        house = c(\"detached\", \"rowhouse\", \"rowhouse\")\n)\n\nprint(subjects)\n\n\n# A tibble: 3 × 2\n  id    house   \n  <chr> <chr>   \n1 a     detached\n2 b     rowhouse\n3 c     rowhouse\n\nLeft Join\nNow suppose we want to create a table that combines the information about houses with the information about the outcomes. We can use the left_join() function to merge the outcomes and subjects tables and produce the output above.\n\n\nleft_join(x = outcomes, y = subjects, by = \"id\")\n\n\n# A tibble: 9 × 4\n  id    visit outcome house   \n  <chr> <int>   <dbl> <chr>   \n1 a         0    3.45 detached\n2 a         1    4.04 detached\n3 a         2    2.68 detached\n4 b         0    2.53 rowhouse\n5 b         1    4.38 rowhouse\n6 b         2    3.36 rowhouse\n7 c         0    3.22 rowhouse\n8 c         1    2.05 rowhouse\n9 c         2    3.63 rowhouse\n\nThe by argument indicates the column (or columns) that the two tables have in common.\nLeft Join with Incomplete Data\nIn the previous examples, the subjects table didn’t have a visit column. But suppose it did? Maybe people move around during the study. We could image a table like this one.\n\n\nsubjects <- tibble(\n        id = c(\"a\", \"b\", \"c\"),\n        visit = c(0, 1, 0),\n        house = c(\"detached\", \"rowhouse\", \"rowhouse\"),\n)\n\nprint(subjects)\n\n\n# A tibble: 3 × 3\n  id    visit house   \n  <chr> <dbl> <chr>   \n1 a         0 detached\n2 b         1 rowhouse\n3 c         0 rowhouse\n\nWhen we left joint the tables now we get:\n\n\nleft_join(outcomes, subjects, by = c(\"id\", \"visit\"))\n\n\n# A tibble: 9 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 a         0    3.45 detached\n2 a         1    4.04 <NA>    \n3 a         2    2.68 <NA>    \n4 b         0    2.53 <NA>    \n5 b         1    4.38 rowhouse\n6 b         2    3.36 <NA>    \n7 c         0    3.22 rowhouse\n8 c         1    2.05 <NA>    \n9 c         2    3.63 <NA>    \n\nNotice how now if we do not have information about a subject’s housing in a given visit, the left_join() function automatically inserts an NA value to indicate that it is missing.\nAlso, in the above example, we joined on the id and the visit columns.\nWe may even have a situation where we are missing housing data for a subject completely. The following table has no information about subject a.\n\n\nsubjects <- tibble(\n        id = c(\"b\", \"c\"),\n        visit = c(1, 0),\n        house = c(\"rowhouse\", \"rowhouse\"),\n)\n\nsubjects\n\n\n# A tibble: 2 × 3\n  id    visit house   \n  <chr> <dbl> <chr>   \n1 b         1 rowhouse\n2 c         0 rowhouse\n\nBut we can still join the tables together and the house values for subject a will all be NA.\n\n\nleft_join(x = outcomes, y = subjects, by = c(\"id\", \"visit\"))\n\n\n# A tibble: 9 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 a         0    3.45 <NA>    \n2 a         1    4.04 <NA>    \n3 a         2    2.68 <NA>    \n4 b         0    2.53 <NA>    \n5 b         1    4.38 rowhouse\n6 b         2    3.36 <NA>    \n7 c         0    3.22 rowhouse\n8 c         1    2.05 <NA>    \n9 c         2    3.63 <NA>    \n\nThe bottom line for left_join() is that it always retains the values in the “left” argument (in this case the outcomes table). If there are no corresponding values in the “right” argument, NA values will be filled in.\nInner Join\nThe inner_join() function only retains the rows of both tables that have corresponding values. Here we can see the difference.\n\n\ninner_join(x = outcomes, y = subjects, by = c(\"id\", \"visit\"))\n\n\n# A tibble: 2 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 b         1    4.38 rowhouse\n2 c         0    3.22 rowhouse\n\nRight Join\nThe right_join() function is like the left_join() function except that it gives priority to the “right” hand argument.\n\n\nright_join(x = outcomes, y = subjects, by = c(\"id\", \"visit\"))\n\n\n# A tibble: 2 × 4\n  id    visit outcome house   \n  <chr> <dbl>   <dbl> <chr>   \n1 b         1    4.38 rowhouse\n2 c         0    3.22 rowhouse\n\nSummary\nleft_join() is useful for merging a “large” data frame with a “smaller” one while retaining all the rows of the “large” data frame\ninner_join() gives you the intersection of the rows between two data frames\nright_join() is like left_join() with the arguments reversed (likely only useful at the end of a pipeline)\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nIf you had three data frames to combine with a shared key, how would you join them using the verbs you now know?\nUsing df1 and df2 below, what is the difference between inner_join(df1, df2), semi_join(df1, df2) and anti_join(df1, df2)?\n\n\n# Create first example data frame\ndf1 <- data.frame(ID = 1:3,\n                  X1 = c(\"a1\", \"a2\", \"a3\"))\n# Create second example data frame\ndf2 <- data.frame(ID = 2:4, \n                  X2 = c(\"b1\", \"b2\", \"b3\"))\n\n\n\nTry changing the order from the above e.g. inner_join(df2, df1), semi_join(df2, df1) and anti_join(df2, df1). What changed? What did not change?\n\nAdditional Resources\n\nhttps://rdpeng.github.io/Biostat776/lecture-joining-data-in-r-basics.html\nhttps://r4ds.had.co.nz/relational-data.html\nhttps://rafalab.github.io/dsbook/joining-tables.html\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-08T23:26:49-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-07-tidy-data-and-the-tidyverse/",
    "title": "Tidy data and the Tidyverse",
    "description": "Introduction to tidy data and how to convert between wide and long data with the tidyr R package.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "tidyr",
      "here",
      "tidyverse"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nTidy data\nThe “Tidyverse”\npivot_longer() and pivot_wider()\nseparate() and unite()\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\n“Happy families are all alike; every unhappy family is unhappy in its own way.” –– Leo Tolstoy\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nTidy Data paper published in the Journal of Statistical Software\nhttps://r4ds.had.co.nz/tidy-data.html\ntidyr cheat sheet from RStudio\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-tidy-data-and-the-tidyverse.html\nhttps://r4ds.had.co.nz/tidy-data.html\nLearning objectives\n\nAt the end of this lesson you will:\nDefine tidy data\nBe able to transform non-tidy data into tidy data\nBe able to transform wide data into long data\nBe able to separate character columns into multiple columns\nBe able to unite multiple character columns into one column\n\nTidy data\nAs we learned in the last lesson, one unifying concept of the tidyverse is the notion of tidy data. As defined by Hadley Wickham in his 2014 paper published in the Journal of Statistical Software, a tidy dataset has the following properties:\nEach variable forms a column.\nEach observation forms a row.\nEach type of observational unit forms a table.\n\n\n\nFigure 1: Artwork by Allison Horst on tidy data\n\n\n\n[Source: Artwork by Allison Horst]\nThe purpose of defining tidy data is to highlight the fact that most data do not start out life as tidy. In fact, much of the work of data analysis may involve simply making the data tidy (at least this has been our experience). Once a dataset is tidy, it can be used as input into a variety of other functions that may transform, model, or visualize the data.\nAs a quick example, consider the following data illustrating religion and income survey data with the number of respondees with income range in column name. This is in a classic table format:\n\n\nlibrary(tidyr)\nrelig_income\n\n\n# A tibble: 18 × 11\n   religion  `<$10k` `$10-20k` `$20-30k` `$30-40k` `$40-50k` `$50-75k`\n   <chr>       <dbl>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1 Agnostic       27        34        60        81        76       137\n 2 Atheist        12        27        37        52        35        70\n 3 Buddhist       27        21        30        34        33        58\n 4 Catholic      418       617       732       670       638      1116\n 5 Don’t kn…      15        14        15        11        10        35\n 6 Evangeli…     575       869      1064       982       881      1486\n 7 Hindu           1         9         7         9        11        34\n 8 Historic…     228       244       236       238       197       223\n 9 Jehovah'…      20        27        24        24        21        30\n10 Jewish         19        19        25        25        30        95\n11 Mainline…     289       495       619       655       651      1107\n12 Mormon         29        40        48        51        56       112\n13 Muslim          6         7         9        10         9        23\n14 Orthodox       13        17        23        32        32        47\n15 Other Ch…       9         7        11        13        13        14\n16 Other Fa…      20        33        40        46        49        63\n17 Other Wo…       5         2         3         4         2         7\n18 Unaffili…     217       299       374       365       341       528\n# … with 4 more variables: $75-100k <dbl>, $100-150k <dbl>,\n#   >150k <dbl>, Don't know/refused <dbl>\n\nWhile this format is canonical and is useful for quickly observing the relationship between multiple variables, it is not tidy. This format violates the tidy form because there are variables in the columns. In this case the variables are religion, income bracket, and the number of respondents, which is the third variable, is presented inside the table.\nConverting this data to tidy format would give us\n\n\nlibrary(tidyverse)\n\nrelig_income %>%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") %>%\n  mutate(religion = factor(religion), income = factor(income))\n\n\n# A tibble: 180 × 3\n   religion income             respondents\n   <fct>    <fct>                    <dbl>\n 1 Agnostic <$10k                       27\n 2 Agnostic $10-20k                     34\n 3 Agnostic $20-30k                     60\n 4 Agnostic $30-40k                     81\n 5 Agnostic $40-50k                     76\n 6 Agnostic $50-75k                    137\n 7 Agnostic $75-100k                   122\n 8 Agnostic $100-150k                  109\n 9 Agnostic >150k                       84\n10 Agnostic Don't know/refused          96\n# … with 170 more rows\n\nSome of these functions you have seen before, others might be new to you. Let’s talk about each one in the context of the Tidyverse package.\nThe “Tidyverse”\nThere are a number of R packages that take advantage of the tidy data form and can be used to do interesting things with data. Many (but not all) of these packages are written by Hadley Wickham and the collection of packages is sometimes referred to as the “tidyverse” because of their dependence on and presumption of tidy data. “Tidyverse” packages include:\nggplot2: a plotting system based on the grammar of graphics\nmagrittr: defines the %>% operator for chaining functions together in a series of operations on data\ndplyr: a suite of (fast) functions for working with data frames\ntidyr: easily tidy data with pivot_wider() and pivot_longer() functions (also separate() and unite())\nWe will be using these packages quite a bit this week.\nThe “tidyverse” package can be used to install all of the packages in the tidyverse at once. For example, instead of starting an R script with this:\n\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(readr)\nlibrary(ggplot2)\n\n\n\nYou can start with this:\n\n\nlibrary(tidyverse)\n\n\n\nIn the code above, let’s talk about what we did using the pivot_longer() function. We will also talk about pivot_wider().\npivot_longer() and pivot_wider()\nThe tidyr package includes functions to transfer a data frame between long and wide.\nWide format data tends to have different attributes or variables describing an observation placed in separate columns.\nLong format data tends to have different attributes encoded as levels of a single variable, followed by another column that contains tha values of the observation at those different levels.\nIn the section above, we showed an example that used pivot_longer() to convert data into a tidy format.\nThe key problem with the tidyness of the data is that the income variables are not in their own columns, but rather are embedded in the structure of the columns.\nTo fix this, you can use the pivot_longer() function to gather values spread across several columns into a single column, with the column names gathered into an income column. When gathering, exclude any columns that you do not want “gathered” (religion in this case) by including the column names with a the minus sign in the pivot_longer() function. For example:\n\n\n# Gather everything EXCEPT religion to tidy data\nrelig_income %>%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\")\n\n\n# A tibble: 180 × 3\n   religion income             respondents\n   <chr>    <chr>                    <dbl>\n 1 Agnostic <$10k                       27\n 2 Agnostic $10-20k                     34\n 3 Agnostic $20-30k                     60\n 4 Agnostic $30-40k                     81\n 5 Agnostic $40-50k                     76\n 6 Agnostic $50-75k                    137\n 7 Agnostic $75-100k                   122\n 8 Agnostic $100-150k                  109\n 9 Agnostic >150k                       84\n10 Agnostic Don't know/refused          96\n# … with 170 more rows\n\nEven if your data is in a tidy format, pivot_longer() is occasionally useful for pulling data together to take advantage of faceting, or plotting separate plots based on a grouping variable. We will talk more about that in a future lecture.\nThe pivot_wider() function is less commonly needed to tidy data. It can, however, be useful for creating summary tables. For example, you use the summarize() function in dplyr to summarize the total number of respondents per income category.\n\n\nrelig_income %>%\n  pivot_longer(-religion, names_to = \"income\", values_to = \"respondents\") %>%\n  mutate(religion = factor(religion), income = factor(income)) %>% \n  group_by(income) %>% \n  summarize(total_respondents = sum(respondents)) %>%\n  pivot_wider(names_from = \"income\", \n              values_from = \"total_respondents\") %>%\n  knitr::kable()\n\n\n<$10k\n>150k\n$10-20k\n$100-150k\n$20-30k\n$30-40k\n$40-50k\n$50-75k\n$75-100k\nDon’t know/refused\n1930\n2608\n2781\n3197\n3357\n3302\n3085\n5185\n3990\n6121\n\nNotice in this example how pivot_wider() has been used at the very end of the code sequence to convert the summarized data into a shape that offers a better tabular presentation for a report. In the pivot_wider() call, you first specify the name of the column to use for the new column names (income in this example) and then specify the column to use for the cell values (total_respondents here).\n\nExample: Let’s try another dataset. This data contain an excerpt of the Gapminder data on life expectancy, GDP per capita, and population by country.\n\n\nlibrary(gapminder)\ngapminder\n\n\n# A tibble: 1,704 × 6\n   country     continent  year lifeExp      pop gdpPercap\n   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia       1952    28.8  8425333      779.\n 2 Afghanistan Asia       1957    30.3  9240934      821.\n 3 Afghanistan Asia       1962    32.0 10267083      853.\n 4 Afghanistan Asia       1967    34.0 11537966      836.\n 5 Afghanistan Asia       1972    36.1 13079460      740.\n 6 Afghanistan Asia       1977    38.4 14880372      786.\n 7 Afghanistan Asia       1982    39.9 12881816      978.\n 8 Afghanistan Asia       1987    40.8 13867957      852.\n 9 Afghanistan Asia       1992    41.7 16317921      649.\n10 Afghanistan Asia       1997    41.8 22227415      635.\n# … with 1,694 more rows\n\nIf we wanted to make lifeExp, pop and gdpPercap (all measurements that we observe) go from a wide table into a long table, what would we do?\n\n\n# try it yourself\n\n\n\n\n\nOne more! Try using pivot_longer() to convert the the following data that contains made-up revenues for three companies by quarter for years 2006 to 2009.\nAfterward, use group_by() and summarize() to calculate the average revenue for each company across all years and all quarters.\nBonus: Calculate a mean revenue for each company AND each year (averaged across all 4 quarters).\n\n\ndf <- tibble(\n  \"company\" = rep(1:3, each=4), \n  \"year\"  = rep(2006:2009, 3),\n  \"Q1\"    = sample(x = 0:100, size = 12),\n  \"Q2\"    = sample(x = 0:100, size = 12),\n  \"Q3\"    = sample(x = 0:100, size = 12),\n  \"Q4\"    = sample(x = 0:100, size = 12),\n)\ndf\n\n\n# A tibble: 12 × 6\n   company  year    Q1    Q2    Q3    Q4\n     <int> <int> <int> <int> <int> <int>\n 1       1  2006   100    90    24    86\n 2       1  2007    23    29    30    67\n 3       1  2008    42    39    53    77\n 4       1  2009    98    98    60    87\n 5       2  2006     8    97    17    73\n 6       2  2007    15    83    18     8\n 7       2  2008     7    12    38    72\n 8       2  2009    22    49    99    82\n 9       3  2006    65    28    39    22\n10       3  2007    20    33    14    56\n11       3  2008    56    78    72    44\n12       3  2009    36    67    91    42\n\n\n\n# try it yourself \n\n\n\n\nseparate() and unite()\nThe same tidyr package also contains two useful functions:\nunite(): combine contents of two or more columns into a single column\nseparate(): separate contents of a column into two or more columns\nFirst, we combine the first three columns into one new column using unite().\n\n\ngapminder %>% \n  unite(col=\"country_continent_year\", country:year, sep=\"_\")\n\n\n# A tibble: 1,704 × 4\n   country_continent_year lifeExp      pop gdpPercap\n   <chr>                    <dbl>    <int>     <dbl>\n 1 Afghanistan_Asia_1952     28.8  8425333      779.\n 2 Afghanistan_Asia_1957     30.3  9240934      821.\n 3 Afghanistan_Asia_1962     32.0 10267083      853.\n 4 Afghanistan_Asia_1967     34.0 11537966      836.\n 5 Afghanistan_Asia_1972     36.1 13079460      740.\n 6 Afghanistan_Asia_1977     38.4 14880372      786.\n 7 Afghanistan_Asia_1982     39.9 12881816      978.\n 8 Afghanistan_Asia_1987     40.8 13867957      852.\n 9 Afghanistan_Asia_1992     41.7 16317921      649.\n10 Afghanistan_Asia_1997     41.8 22227415      635.\n# … with 1,694 more rows\n\nNext, we show how to separate the columns into three separate columns using separate() using the col, into and sep arguments.\n\n\ngapminder %>% \n  unite(col=\"country_continent_year\", country:year, sep=\"_\") %>% \n  separate(col=\"country_continent_year\", into=c(\"country\", \"continent\", \"year\"), sep=\"_\")\n\n\n# A tibble: 1,704 × 6\n   country     continent year  lifeExp      pop gdpPercap\n   <chr>       <chr>     <chr>   <dbl>    <int>     <dbl>\n 1 Afghanistan Asia      1952     28.8  8425333      779.\n 2 Afghanistan Asia      1957     30.3  9240934      821.\n 3 Afghanistan Asia      1962     32.0 10267083      853.\n 4 Afghanistan Asia      1967     34.0 11537966      836.\n 5 Afghanistan Asia      1972     36.1 13079460      740.\n 6 Afghanistan Asia      1977     38.4 14880372      786.\n 7 Afghanistan Asia      1982     39.9 12881816      978.\n 8 Afghanistan Asia      1987     40.8 13867957      852.\n 9 Afghanistan Asia      1992     41.7 16317921      649.\n10 Afghanistan Asia      1997     41.8 22227415      635.\n# … with 1,694 more rows\n\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nUsing prose, describe how the variables and observations are organised in a tidy dataset versus an non-tidy dataset.\nWhat do the extra and fill arguments do in separate()? Experiment with the various options for the following two toy datasets.\n\n\ntibble(x = c(\"a,b,c\", \"d,e,f,g\", \"h,i,j\")) %>% \n  separate(x, c(\"one\", \"two\", \"three\"))\n\ntibble(x = c(\"a,b,c\", \"d,e\", \"f,g,i\")) %>% \n  separate(x, c(\"one\", \"two\", \"three\"))\n\n\n\nBoth unite() and separate() have a remove argument. What does it do? Why would you set it to FALSE?\nCompare and contrast separate() and extract(). Why are there three variations of separation (by position, by separator, and with groups), but only one unite()?\n\nAdditional Resources\n\nTidy Data paper published in the Journal of Statistical Software\nhttps://r4ds.had.co.nz/tidy-data.html\ntidyr cheat sheet from RStudio\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/tidydata_1.jpg",
    "last_modified": "2021-09-08T23:26:14-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-09-plotting-systems/",
    "title": "Plotting Systems",
    "description": "Overview of three plotting systems in R",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "ggplot2",
      "data viz"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nPlotting Systems\nThe Base Plotting System\nThe Lattice System\nThe ggplot2 System\nReferences\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nThe data may not contain the answer. And, if you torture the data long enough, it will tell you anything. —John W. Tukey\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/data-visualisation.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-plotting-systems.html\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to identify and describe the three plotting systems in R\n\nPlotting Systems\nThere are three different plotting systems in R and they each have different characteristics and modes of operation. They three systems are the base plotting system, the lattice system, and the ggplot2 system. This course will focus primarily on the ggplot2 plotting system. The other two systems are presented for context.\nThe Base Plotting System\nThe base plotting system is the original plotting system for R. The basic model is sometimes referred to as the “artist’s palette” model. The idea is you start with blank canvas and build up from there.\nIn more R-specific terms, you typically start with plot function (or similar plot creating function) to initiate a plot and then annotate the plot with various annotation functions (text, lines, points, axis)\nThe base plotting system is often the most convenient plotting system to use because it mirrors how we sometimes think of building plots and analyzing data. If we do not have a completely well-formed idea of how we want to look at some data, often we will start by “throwing some data on the page” and then slowly add more information to it as our thought process evolves.\nFor example, we might look at a simple scatterplot and then decide to add a linear regression line or a smoother to it to highlight the trends.\n\n\ndata(airquality)\nwith(airquality, {\n        plot(Temp, Ozone)\n        lines(loess.smooth(Temp, Ozone))\n})\n\n\n\n\nFigure 1: Scatterplot with loess curve\n\n\n\nIn the code above, the plot function creates the initial plot and draws the points (circles) on the canvas. The lines function is used to annotate or add to the plot; in this case it adds a loess smoother to the scatterplot.\nHere, we use the plot() function to draw the points on the scatterplot and then use the title function to add a main title to the plot.\nOne downside with constructing base plots is that you can not go backwards once the plot has started. So it is possible that you could start down the road of constructing a plot and realize later (when it is too late) that you do not have enough room to add a y-axis label or something like that.\nIf you have specific plot in mind, there is then a need to plan in advance to make sure, for example, that you have set your margins to be the right size to fit all of the annotations that you may want to include. While the base plotting system is nice in that it gives you the flexibility to specify these kinds of details to painstaking accuracy, sometimes it would be nice if the system could just figure it out for you.\nAnother downside of the base plotting system is that it is difficult to describe or translate a plot to others because there is no clear graphical language or grammar that can be used to communicate what you have done. The only real way to describe what you have done in a base plot is to just list the series of commands/functions that you have executed, which is not a particularly compact way of communicating things. This is one problem that the ggplot2 package attempts to address.\nAnother typical base plot is constructed with the following code.\n\n\ndata(cars)\n\n## Create the plot / draw canvas\nwith(cars, plot(speed, dist))\n\n## Add annotation\ntitle(\"Speed vs. Stopping distance\")\n\n\n\n\nFigure 2: Base plot with title\n\n\n\nWe will go into more detail on what these functions do in later chapters.\nThe Lattice System\nThe lattice plotting system is implemented in the lattice package which comes with every installation of R (although it is not loaded by default). To use the lattice plotting functions you must first load the lattice package with the library function.\n\n\nlibrary(lattice)\n\n\n\nWith the lattice system, plots are created with a single function call, such as xyplot() or bwplot(). There is no real distinction between functions that create or initiate plots and functions that annotate plots because it all happens at once.\nLattice plots tend to be most useful for conditioning types of plots, i.e. looking at how y changes with x across levels of z. These types of plots are useful for looking at multi-dimensional data and often allow you to squeeze a lot of information into a single window or page.\nAnother aspect of lattice that makes it different from base plotting is that things like margins and spacing are set automatically. This is possible because entire plot is specified at once via a single function call, so all of the available information needed to figure out the spacing and margins is already there.\nHere is an example of a lattice plot that looks at the relationship between life expectancy and income and how that relationship varies by region in the United States.\n\n\nstate <- data.frame(state.x77, region = state.region)\nxyplot(Life.Exp ~ Income | region, data = state, layout = c(4, 1))\n\n\n\n\nFigure 3: Lattice plot\n\n\n\nYou can see that the entire plot was generated by the call to xyplot and all of the data for the plot were stored in the state data frame. The plot itself contains four panels—one for each region—and within each panel is a scatterplot of life expectancy and income. The notion of panels comes up a lot with lattice plots because you typically have many panels in a lattice plot (each panel typically represents a condition, like “region”).\nOne downside with the lattice system is that it can sometimes be very awkward to specify an entire plot in a single function call (you end up with functions with many many arguments). Also, annotation in panels in plots is not especially intuitive and can be difficult to explain. In particular, the use of custom panel functions and subscripts can be difficult to wield and requires intense preparation. Finally, once a plot is created, you cannot “add” to the plot (but of course you can just make it again with modifications).\nThe ggplot2 System\nThe ggplot2 plotting system attempts to split the difference between base and lattice in a number of ways. Taking cues from lattice, the ggplot2 system automatically deals with spacings, text, titles but also allows you to annotate by “adding” to a plot.\nThe ggplot2 system is implemented in the ggplot2 package (part of the tidyverse package), which is available from CRAN (it does not come with R). You can install it from CRAN via\n\n\ninstall.packages(\"ggplot2\")\n\n\n\nand then load it into R via the library() function.\n\n\nlibrary(ggplot2)\n\n\n\nSuperficially, the ggplot2 functions are similar to lattice, but the system isgenerally easier and more intuitive to use. The defaults used in ggplot2 make many choices for you, but you can still customize plots to your heart’s desire.\nA typical plot with the ggplot package looks as follows.\n\n\nlibrary(tidyverse)\ndata(mpg)\nmpg %>%\n  ggplot(aes(displ, hwy)) + \n  geom_point()\n\n\n\n\nFigure 4: ggplot2 plot\n\n\n\nThere are additional functions in ggplot2 that allow you to make arbitrarily sophisticated plots.\nReferences\nPaul Murrell (2011). R Graphics, CRC Press.\nHadley Wickham (2009). ggplot2, Springer.\nDeepayan Sarkar (2008). Lattice: Multivariate Data Visualization with R, Springer.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\n\n\n\n\nAdditional Resources\n\n\n\n\n\n\n\n\n",
    "preview": "posts/2021-09-09-plotting-systems/plotting-systems_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-09-08T22:38:56-04:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 960
  },
  {
    "path": "posts/2021-09-07-managing-data-frames-with-tidyverse/",
    "title": "Managing data frames with the Tidyverse",
    "description": "An introduction to data frames in R and the managing them with the dplyr R package.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-07",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "dplyr",
      "here",
      "tibble",
      "tidyverse"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nData Frames\nTibbles\n\nThe dplyr Package\ndplyr grammar\ndplyr functions\ndplyr installation\nselect()\nfilter()\narrange()\nrename()\nmutate()\ngroup_by()\n%>%\nslice_*()\n\nSummary\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://r4ds.had.co.nz/tibbles.html\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-managing-data-frames-with-the-tidyverse.html\nhttps://jhudatascience.org/tidyversecourse/get-data.html#tibbles\nLearning objectives\n\nAt the end of this lesson you will:\nUnderstand the advantages of a tibble and data.frame data objects in R\nLearn about the dplyr R package to manage data frames\nRecognize the key verbs to manage data frames in dplyr\nUse the “pipe” operator to combine verbs together\n\nData Frames\nThe data frame (or data.frame) is a key data structure in statistics and in R. The basic structure of a data frame is that there is one observation per row and each column represents a variable, a measure, feature, or characteristic of that observation. R has an internal implementation of data frames that is likely the one you will use most often. However, there are packages on CRAN that implement data frames via things like relational databases that allow you to operate on very very large data frames (but we will not discuss them here).\nGiven the importance of managing data frames, it is important that we have good tools for dealing with them. For example, operations like filtering rows, re-ordering rows, and selecting columns, can often be tedious operations in R whose syntax is not very intuitive. The dplyr package is designed to mitigate a lot of these problems and to provide a highly optimized set of routines specifically for dealing with data frames.\nTibbles\nAnother type of data structure that we need to discuss is called the tibble! It’s best to think of tibbles as an updated and stylish version of the data.frame. And, tibbles are what tidyverse packages work with most seamlessly. Now, that does not mean tidyverse packages require tibbles. In fact, they still work with data.frames, but the more you work with tidyverse and tidyverse-adjacent packages, the more you will see the advantages of using tibbles.\nBefore we go any further, tibbles are data frames, but they have some new bells and whistles to make your life easier.\nHow tibbles differ from data.frame\nThere are a number of differences between tibbles and data.frames. To see a full vignette about tibbles and how they differ from data.frame, you will want to execute vignette(\"tibble\") and read through that vignette. However, we will summarize some of the most important points here:\nInput type remains unchanged - data.frame is notorious for treating strings as factors; this will not happen with tibbles\nVariable names remain unchanged - In base R, creating data.frames will remove spaces from names, converting them to periods or add “x” before numeric column names. Creating tibbles will not change variable (column) names.\nThere are no row.names() for a tibble - Tidy data requires that variables be stored in a consistent way, removing the need for row names.\nTibbles print first ten rows and columns that fit on one screen - Printing a tibble to screen will never print the entire huge data frame out. By default, it just shows what fits to your screen.\nCreating a tibble\nThe tibble package is part of the tidyverse and can thus be loaded in (once installed) using:\n\n\nlibrary(tidyverse)\n\n\n\nas_tibble()\nSince many packages use the historical data.frame from base R, you will often find yourself in the situation that you have a data.frame and want to convert that data.frame to a tibble. To do so, the as_tibble() function is exactly what you are looking for.\nFor the example, in this lesson we will be using a dataset containing air pollution and temperature data for the city of Chicago in the U.S. The dataset is available in this repository.\nYou can load the data into R using the readRDS() function.\n\n\nlibrary(here)\nchicago <- readRDS(here(\"data\", \"chicago.rds\"))\n\n\n\nYou can see some basic characteristics of the dataset with the dim() and str() functions.\n\n\ndim(chicago)\n\n\n[1] 6940    8\n\nstr(chicago)\n\n\n'data.frame':   6940 obs. of  8 variables:\n $ city      : chr  \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num  31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num  31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date, format: \"1987-01-01\" ...\n $ pm25tmean2: num  NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num  34 NA 34.2 47 NA ...\n $ o3tmean2  : num  4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num  20 23.2 23.8 30.4 30.3 ...\n\nWe see this data structure is a data.frame with 6940 observations and 8 variables. To convert this data.frame to a tibble you would use the following:\n\n\nstr(as_tibble(chicago))\n\n\ntibble [6,940 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:6940] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:6940] 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date[1:6940], format: \"1987-01-01\" ...\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\nNote in the above example and as mentioned earlier, that tibbles, by default, only print the first ten rows to screen. If you were to print chicago to screen, all 6940 rows would be displayed. When working with large data.frames, this default behavior can be incredibly frustrating. Using tibbles removes this frustration because of the default settings for tibble printing.\nAdditionally, you will note that the type of the variable is printed for each variable in the tibble. This helpful feature is another added bonus of tibbles relative to data.frame.\nIf you do want to see more rows from the tibble, there are a few options! First, the View() function in RStudio is incredibly helpful. The input to this function is the data.frame or tibble you would like to see. Specifically, View(chicago) would provide you, the viewer, with a scrollable view (in a new tab) of the complete dataset.\nA second option is the fact that print() enables you to specify how many rows and columns you would like to display. Here, we again display the chicago data.frame as a tibble but specify that we’d only like to see 5 rows. The width = Inf argument specifies that we would like to see all the possible columns. Here, there are only 8, but for larger datasets, this can be helpful to specify.\n\n\nas_tibble(chicago) %>% \n  print(n = 5, width = Inf)\n\n\n# A tibble: 6,940 × 8\n  city   tmpd  dptp date       pm25tmean2 pm10tmean2 o3tmean2\n  <chr> <dbl> <dbl> <date>          <dbl>      <dbl>    <dbl>\n1 chic   31.5  31.5 1987-01-01         NA       34       4.25\n2 chic   33    29.9 1987-01-02         NA       NA       3.30\n3 chic   33    27.4 1987-01-03         NA       34.2     3.33\n4 chic   29    28.6 1987-01-04         NA       47       4.38\n5 chic   32    28.9 1987-01-05         NA       NA       4.75\n  no2tmean2\n      <dbl>\n1      20.0\n2      23.2\n3      23.8\n4      30.4\n5      30.3\n# … with 6,935 more rows\n\ntibble()\nAlternatively, you can create a tibble on the fly by using tibble() and specifying the information you’d like stored in each column. Note that if you provide a single value, this value will be repeated across all rows of the tibble. This is referred to as “recycling inputs of length 1.”\nIn the example here, we see that the column c will contain the value ‘1’ across all rows.\n\n\ntibble(\n  a = 1:5,\n  b = 6:10,\n  c = 1,\n  z = (a + b)^2 + c\n)\n\n\n# A tibble: 5 × 4\n      a     b     c     z\n  <int> <int> <dbl> <dbl>\n1     1     6     1    50\n2     2     7     1    82\n3     3     8     1   122\n4     4     9     1   170\n5     5    10     1   226\n\nThe tibble() function allows you to quickly generate tibbles and even allows you to reference columns within the tibble you’re creating, as seen in column z of the example above.\nWe also noted previously that tibbles can have column names that are not allowed in data.frame. In this example, we see that to utilize a nontraditional variable name, you surround the column name with backticks. Note that to refer to such columns in other tidyverse packages, you’ll continue to use backticks surrounding the variable name.\n\n\ntibble(\n  `two words` = 1:5,\n  `12` = \"numeric\",\n  `:)` = \"smile\",\n)\n\n\n# A tibble: 5 × 3\n  `two words` `12`    `:)` \n        <int> <chr>   <chr>\n1           1 numeric smile\n2           2 numeric smile\n3           3 numeric smile\n4           4 numeric smile\n5           5 numeric smile\n\nSubsetting\nSubsetting tibbles also differs slightly from how subsetting occurs with data.frame. When it comes to tibbles, [[ can subset by name or position; $ only subsets by name. For example:\n\n\ndf <- tibble(\n  a = 1:5,\n  b = 6:10,\n  c = 1,\n  z = (a + b)^2 + c\n)\n\n# Extract by name using $ or [[]]\ndf$z\n\n\n[1]  50  82 122 170 226\n\ndf[[\"z\"]]\n\n\n[1]  50  82 122 170 226\n\n\n# Extract by position requires [[]]\ndf[[4]]\n\n\n[1]  50  82 122 170 226\n\nHaving now discussed tibbles, which are the type of object most tidyverse and tidyverse-adjacent packages work best with, we now know the goal. In many cases, tibbles are ultimately what we want to work with in R. However, data are stored in many different formats outside of R. We will spend the rest of this lesson discussing wrangling functions that work either a data.frame or tibble.\nThe dplyr Package\nThe dplyr package was developed by RStudio and is an optimized and distilled version of the older plyr package for data manipulation. The dplyr package does not provide any “new” functionality to R per se, in the sense that everything dplyr does could already be done with base R, but it greatly simplifies existing functionality in R.\nOne important contribution of the dplyr package is that it provides a “grammar” (in particular, verbs) for data manipulation and for operating on data frames. With this grammar, you can sensibly communicate what it is that you are doing to a data frame that other people can understand (assuming they also know the grammar). This is useful because it provides an abstraction for data manipulation that previously did not exist. Another useful contribution is that the dplyr functions are very fast, as many key operations are coded in C++.\n\n\n\nFigure 1: Artwork by Allison Horst on the dplyr package\n\n\n\n[Source: Artwork by Allison Horst]\ndplyr grammar\nSome of the key “verbs” provided by the dplyr package are\nselect(): return a subset of the columns of a data frame, using a flexible notation\nfilter(): extract a subset of rows from a data frame based on logical conditions\narrange(): reorder rows of a data frame\nrename(): rename variables in a data frame\nmutate(): add new variables/columns or transform existing variables\nsummarise() / summarize(): generate summary statistics of different variables in the data frame, possibly within strata\n%>%: the “pipe” operator is used to connect multiple verb actions together into a pipeline\nThe dplyr package as a number of its own data types that it takes advantage of. For example, there is a handy print method that prevents you from printing a lot of data to the console. Most of the time, these additional data types are transparent to the user and do not need to be worried about.\ndplyr functions\nAll of the functions that we will discuss in this Chapter will have a few common characteristics. In particular,\nThe first argument is a data frame.\nThe subsequent arguments describe what to do with the data frame specified in the first argument, and you can refer to columns in the data frame directly (without using the $ operator, just use the column names).\nThe return result of a function is a new data frame.\nData frames must be properly formatted and annotated for this to all be useful. In particular, the data must be tidy. In short, there should be one observation per row, and each column should represent a feature or characteristic of that observation.\n\n\n\nFigure 2: Artwork by Allison Horst on tidy data\n\n\n\n[Source: Artwork by Allison Horst]\ndplyr installation\nThe dplyr package can be installed from CRAN or from GitHub using the devtools package and the install_github() function. The GitHub repository will usually contain the latest updates to the package and the development version.\nTo install from CRAN, just run\n\n\ninstall.packages(\"dplyr\")\n\n\n\nThe dplyr package is also installed when you install the tidyverse meta-package.\nAfter installing the package it is important that you load it into your R session with the library() function.\n\n\nlibrary(dplyr)\n\n\n\nYou may get some warnings when the package is loaded because there are functions in the dplyr package that have the same name as functions in other packages. For now you can ignore the warnings.\nselect()\nWe will continue to use the chicago dataset containing air pollution and temperature data.\n\n\nchicago <- as_tibble(chicago)\nstr(chicago)\n\n\ntibble [6,940 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:6940] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:6940] 31.5 33 33 29 32 40 34.5 29 26.5 32.5 ...\n $ dptp      : num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date      : Date[1:6940], format: \"1987-01-01\" ...\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\nThe select() function can be used to select columns of a data frame that you want to focus on. Often you will have a large data frame containing “all” of the data, but any given analysis might only use a subset of variables or observations. The select() function allows you to get the few columns you might need.\nSuppose we wanted to take the first 3 columns only. There are a few ways to do this. We could for example use numerical indices. But we can also use the names directly.\n\n\nnames(chicago)[1:3]\n\n\n[1] \"city\" \"tmpd\" \"dptp\"\n\nsubset <- select(chicago, city:dptp)\nhead(subset)\n\n\n# A tibble: 6 × 3\n  city   tmpd  dptp\n  <chr> <dbl> <dbl>\n1 chic   31.5  31.5\n2 chic   33    29.9\n3 chic   33    27.4\n4 chic   29    28.6\n5 chic   32    28.9\n6 chic   40    35.1\n\nNote that the : normally cannot be used with names or strings, but inside the select() function you can use it to specify a range of variable names.\nYou can also omit variables using the select() function by using the negative sign. With select() you can do\n\n\nselect(chicago, -(city:dptp))\n\n\n\nwhich indicates that we should include every variable except the variables city through dptp. The equivalent code in base R would be\n\n\ni <- match(\"city\", names(chicago))\nj <- match(\"dptp\", names(chicago))\nhead(chicago[, -(i:j)])\n\n\n\nNot super intuitive, right?\nThe select() function also allows a special syntax that allows you to specify variable names based on patterns. So, for example, if you wanted to keep every variable that ends with a “2”, we could do\n\n\nsubset <- select(chicago, ends_with(\"2\"))\nstr(subset)\n\n\ntibble [6,940 × 4] (S3: tbl_df/tbl/data.frame)\n $ pm25tmean2: num [1:6940] NA NA NA NA NA NA NA NA NA NA ...\n $ pm10tmean2: num [1:6940] 34 NA 34.2 47 NA ...\n $ o3tmean2  : num [1:6940] 4.25 3.3 3.33 4.38 4.75 ...\n $ no2tmean2 : num [1:6940] 20 23.2 23.8 30.4 30.3 ...\n\nOr if we wanted to keep every variable that starts with a “d”, we could do\n\n\nsubset <- select(chicago, starts_with(\"d\"))\nstr(subset)\n\n\ntibble [6,940 × 2] (S3: tbl_df/tbl/data.frame)\n $ dptp: num [1:6940] 31.5 29.9 27.4 28.6 28.9 ...\n $ date: Date[1:6940], format: \"1987-01-01\" ...\n\nYou can also use more general regular expressions if necessary. See the help page (?select) for more details.\nfilter()\nThe filter() function is used to extract subsets of rows from a data frame. This function is similar to the existing subset() function in R but is quite a bit faster in my experience.\n\n\n\nFigure 3: Artwork by Allison Horst on the filter function\n\n\n\n[Source: Artwork by Allison Horst]\nSuppose we wanted to extract the rows of the chicago data frame where the levels of PM2.5 are greater than 30 (which is a reasonably high level), we could do\n\n\nchic.f <- filter(chicago, pm25tmean2 > 30)\nstr(chic.f)\n\n\ntibble [194 × 8] (S3: tbl_df/tbl/data.frame)\n $ city      : chr [1:194] \"chic\" \"chic\" \"chic\" \"chic\" ...\n $ tmpd      : num [1:194] 23 28 55 59 57 57 75 61 73 78 ...\n $ dptp      : num [1:194] 21.9 25.8 51.3 53.7 52 56 65.8 59 60.3 67.1 ...\n $ date      : Date[1:194], format: \"1998-01-17\" ...\n $ pm25tmean2: num [1:194] 38.1 34 39.4 35.4 33.3 ...\n $ pm10tmean2: num [1:194] 32.5 38.7 34 28.5 35 ...\n $ o3tmean2  : num [1:194] 3.18 1.75 10.79 14.3 20.66 ...\n $ no2tmean2 : num [1:194] 25.3 29.4 25.3 31.4 26.8 ...\n\nYou can see that there are now only 194 rows in the data frame and the distribution of the pm25tmean2 values is.\n\n\nsummary(chic.f$pm25tmean2)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  30.05   32.12   35.04   36.63   39.53   61.50 \n\nWe can place an arbitrarily complex logical sequence inside of filter(), so we could for example extract the rows where PM2.5 is greater than 30 and temperature is greater than 80 degrees Fahrenheit.\n\n\nchic.f <- filter(chicago, pm25tmean2 > 30 & tmpd > 80)\nselect(chic.f, date, tmpd, pm25tmean2)\n\n\n# A tibble: 17 × 3\n   date        tmpd pm25tmean2\n   <date>     <dbl>      <dbl>\n 1 1998-08-23    81       39.6\n 2 1998-09-06    81       31.5\n 3 2001-07-20    82       32.3\n 4 2001-08-01    84       43.7\n 5 2001-08-08    85       38.8\n 6 2001-08-09    84       38.2\n 7 2002-06-20    82       33  \n 8 2002-06-23    82       42.5\n 9 2002-07-08    81       33.1\n10 2002-07-18    82       38.8\n11 2003-06-25    82       33.9\n12 2003-07-04    84       32.9\n13 2005-06-24    86       31.9\n14 2005-06-27    82       51.5\n15 2005-06-28    85       31.2\n16 2005-07-17    84       32.7\n17 2005-08-03    84       37.9\n\nNow there are only 17 observations where both of those conditions are met.\nOther logical operators you should be aware of include:\nOperator\nMeaning\nExample\n==\nEquals\ncity == chic\n!=\nDoes not equal\ncity != chic\n>\nGreater than\ntmpd > 32.0\n>=\nGreater than or equal to\ntmpd >- 32.0\n<\nLess than\ntmpd < 32.0\n<=\nLess than or equal to\ntmpd <= 32.0\n%in%\nIncluded in\ncity %in% c(\"chic\", \"bmore\")\nis.na()\nIs a missing value\nis.na(pm10tmean2)\nIf you are ever unsure of how to write a logical statement, but know how to write its opposite, you can use the ! operator to negate the whole statement. A common use of this is to identify observations with non-missing data (e.g., !(is.na(pm10tmean2))).\narrange()\nThe arrange() function is used to reorder rows of a data frame according to one of the variables/columns. Reordering rows of a data frame (while preserving corresponding order of other columns) is normally a pain to do in R. The arrange() function simplifies the process quite a bit.\nHere we can order the rows of the data frame by date, so that the first row is the earliest (oldest) observation and the last row is the latest (most recent) observation.\n\n\nchicago <- arrange(chicago, date)\n\n\n\nWe can now check the first few rows\n\n\nhead(select(chicago, date, pm25tmean2), 3)\n\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 1987-01-01         NA\n2 1987-01-02         NA\n3 1987-01-03         NA\n\nand the last few rows.\n\n\ntail(select(chicago, date, pm25tmean2), 3)\n\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 2005-12-29       7.45\n2 2005-12-30      15.1 \n3 2005-12-31      15   \n\nColumns can be arranged in descending order too by useing the special desc() operator.\n\n\nchicago <- arrange(chicago, desc(date))\n\n\n\nLooking at the first three and last three rows shows the dates in descending order.\n\n\nhead(select(chicago, date, pm25tmean2), 3)\n\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 2005-12-31      15   \n2 2005-12-30      15.1 \n3 2005-12-29       7.45\n\ntail(select(chicago, date, pm25tmean2), 3)\n\n\n# A tibble: 3 × 2\n  date       pm25tmean2\n  <date>          <dbl>\n1 1987-01-03         NA\n2 1987-01-02         NA\n3 1987-01-01         NA\n\nrename()\nRenaming a variable in a data frame in R is surprisingly hard to do! The rename() function is designed to make this process easier.\nHere you can see the names of the first five variables in the chicago data frame.\n\n\nhead(chicago[, 1:5], 3)\n\n\n# A tibble: 3 × 5\n  city   tmpd  dptp date       pm25tmean2\n  <chr> <dbl> <dbl> <date>          <dbl>\n1 chic     35  30.1 2005-12-31      15   \n2 chic     36  31   2005-12-30      15.1 \n3 chic     35  29.4 2005-12-29       7.45\n\nThe dptp column is supposed to represent the dew point temperature adn the pm25tmean2 column provides the PM2.5 data. However, these names are pretty obscure or awkward and probably be renamed to something more sensible.\n\n\nchicago <- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)\nhead(chicago[, 1:5], 3)\n\n\n# A tibble: 3 × 5\n  city   tmpd dewpoint date        pm25\n  <chr> <dbl>    <dbl> <date>     <dbl>\n1 chic     35     30.1 2005-12-31 15   \n2 chic     36     31   2005-12-30 15.1 \n3 chic     35     29.4 2005-12-29  7.45\n\nThe syntax inside the rename() function is to have the new name on the left-hand side of the = sign and the old name on the right-hand side.\n\nQuestion: How would you do the equivalent in base R without dplyr?\n\nmutate()\nThe mutate() function exists to compute transformations of variables in a data frame. Often, you want to create new variables that are derived from existing variables and mutate() provides a clean interface for doing that.\n\n\n\nFigure 4: Artwork by Allison Horst on the mutate function\n\n\n\n[Source: Artwork by Allison Horst]\nFor example, with air pollution data, we often want to detrend the data by subtracting the mean from the data. That way we can look at whether a given day’s air pollution level is higher than or less than average (as opposed to looking at its absolute level).\nHere we create a pm25detrend variable that subtracts the mean from the pm25 variable.\n\n\nchicago <- mutate(chicago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE))\nhead(chicago)\n\n\n# A tibble: 6 × 9\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n  <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>    <dbl>     <dbl>\n1 chic     35     30.1 2005-12-31 15          23.5     2.53      13.2\n2 chic     36     31   2005-12-30 15.1        19.2     3.03      22.8\n3 chic     35     29.4 2005-12-29  7.45       23.5     6.79      20.0\n4 chic     37     34.5 2005-12-28 17.8        27.5     3.26      19.3\n5 chic     40     33.6 2005-12-27 23.6        27       4.47      23.5\n6 chic     35     29.6 2005-12-26  8.4         8.5    14.0       16.8\n# … with 1 more variable: pm25detrend <dbl>\n\nThere is also the related transmute() function, which does the same thing as mutate() but then drops all non-transformed variables.\nHere, we de-trend the PM10 and ozone (O3) variables.\n\n\nhead(transmute(chicago, \n               pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE),\n               o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE)))\n\n\n# A tibble: 6 × 2\n  pm10detrend o3detrend\n        <dbl>     <dbl>\n1      -10.4     -16.9 \n2      -14.7     -16.4 \n3      -10.4     -12.6 \n4       -6.40    -16.2 \n5       -6.90    -15.0 \n6      -25.4      -5.39\n\nNote that there are only two columns in the transmuted data frame.\ngroup_by()\nThe group_by() function is used to generate summary statistics from the data frame within strata defined by a variable. For example, in this air pollution dataset, you might want to know what the average annual level of PM2.5 is. So the stratum is the year, and that is something we can derive from the date variable.\nIn conjunction with the group_by() function we often use the summarize() function (or summarise() for some parts of the world).\nThe general operation here is a combination of splitting a data frame into separate pieces defined by a variable or group of variables (group_by()), and then applying a summary function across those subsets (summarize()).\nFirst, we can create a year variable using as.POSIXlt().\n\n\nchicago <- mutate(chicago, year = as.POSIXlt(date)$year + 1900)\n\n\n\nNow we can create a separate data frame that splits the original data frame by year.\n\n\nyears <- group_by(chicago, year)\n\n\n\nFinally, we compute summary statistics for each year in the data frame with the summarize() function.\n\n\nsummarize(years, pm25 = mean(pm25, na.rm = TRUE), \n          o3 = max(o3tmean2, na.rm = TRUE), \n          no2 = median(no2tmean2, na.rm = TRUE))\n\n\n# A tibble: 19 × 4\n    year  pm25    o3   no2\n   <dbl> <dbl> <dbl> <dbl>\n 1  1987 NaN    63.0  23.5\n 2  1988 NaN    61.7  24.5\n 3  1989 NaN    59.7  26.1\n 4  1990 NaN    52.2  22.6\n 5  1991 NaN    63.1  21.4\n 6  1992 NaN    50.8  24.8\n 7  1993 NaN    44.3  25.8\n 8  1994 NaN    52.2  28.5\n 9  1995 NaN    66.6  27.3\n10  1996 NaN    58.4  26.4\n11  1997 NaN    56.5  25.5\n12  1998  18.3  50.7  24.6\n13  1999  18.5  57.5  24.7\n14  2000  16.9  55.8  23.5\n15  2001  16.9  51.8  25.1\n16  2002  15.3  54.9  22.7\n17  2003  15.2  56.2  24.6\n18  2004  14.6  44.5  23.4\n19  2005  16.2  58.8  22.6\n\nsummarize() returns a data frame with year as the first column, and then the annual averages of pm25, o3, and no2.\nIn a slightly more complicated example, we might want to know what are the average levels of ozone (o3) and nitrogen dioxide (no2) within quintiles of pm25. A slicker way to do this would be through a regression model, but we can actually do this quickly with group_by() and summarize().\nFirst, we can create a categorical variable of pm25 divided into quantiles\n\n\nqq <- quantile(chicago$pm25, seq(0, 1, 0.2), na.rm = TRUE)\nchicago <- mutate(chicago, pm25.quint = cut(pm25, qq))\n\n\n\nNow we can group the data frame by the pm25.quint variable.\n\n\nquint <- group_by(chicago, pm25.quint)\n\n\n\nFinally, we can compute the mean of o3 and no2 within quintiles of pm25.\n\n\nsummarize(quint, o3 = mean(o3tmean2, na.rm = TRUE), \n          no2 = mean(no2tmean2, na.rm = TRUE))\n\n\n# A tibble: 6 × 3\n  pm25.quint     o3   no2\n  <fct>       <dbl> <dbl>\n1 (1.7,8.7]    21.7  18.0\n2 (8.7,12.4]   20.4  22.1\n3 (12.4,16.7]  20.7  24.4\n4 (16.7,22.6]  19.9  27.3\n5 (22.6,61.5]  20.3  29.6\n6 <NA>         18.8  25.8\n\nFrom the table, it seems there is not a strong relationship between pm25 and o3, but there appears to be a positive correlation between pm25 and no2. More sophisticated statistical modeling can help to provide precise answers to these questions, but a simple application of dplyr functions can often get you most of the way there.\n%>%\nThe pipeline operator %>% is very handy for stringing together multiple dplyr functions in a sequence of operations. Notice above that every time we wanted to apply more than one function, the sequence gets buried in a sequence of nested function calls that is difficult to read, i.e.\n\n\nthird(second(first(x)))\n\n\n\nThis nesting is not a natural way to think about a sequence of operations. The %>% operator allows you to string operations in a left-to-right fashion, i.e.\n\n\nfirst(x) %>% second %>% third\n\n\n\nTake the example that we just did in the last section where we computed the mean of o3 and no2 within quintiles of pm25. There we had to\ncreate a new variable pm25.quint\nsplit the data frame by that new variable\ncompute the mean of o3 and no2 in the sub-groups defined by pm25.quint\nThat can be done with the following sequence in a single R expression.\n\n\nmutate(chicago, pm25.quint = cut(pm25, qq)) %>%    \n        group_by(pm25.quint) %>% \n        summarize(o3 = mean(o3tmean2, na.rm = TRUE), \n                  no2 = mean(no2tmean2, na.rm = TRUE))\n\n\n# A tibble: 6 × 3\n  pm25.quint     o3   no2\n  <fct>       <dbl> <dbl>\n1 (1.7,8.7]    21.7  18.0\n2 (8.7,12.4]   20.4  22.1\n3 (12.4,16.7]  20.7  24.4\n4 (16.7,22.6]  19.9  27.3\n5 (22.6,61.5]  20.3  29.6\n6 <NA>         18.8  25.8\n\nThis way we do not have to create a set of temporary variables along the way or create a massive nested sequence of function calls.\nNotice in the above code that I pass the chicago data frame to the first call to mutate(), but then afterwards I do not have to pass the first argument to group_by() or summarize(). Once you travel down the pipeline with %>%, the first argument is taken to be the output of the previous element in the pipeline.\nAnother example might be computing the average pollutant level by month. This could be useful to see if there are any seasonal trends in the data.\n\n\nmutate(chicago, month = as.POSIXlt(date)$mon + 1) %>% \n        group_by(month) %>% \n        summarize(pm25 = mean(pm25, na.rm = TRUE), \n                  o3 = max(o3tmean2, na.rm = TRUE), \n                  no2 = median(no2tmean2, na.rm = TRUE))\n\n\n# A tibble: 12 × 4\n   month  pm25    o3   no2\n   <dbl> <dbl> <dbl> <dbl>\n 1     1  17.8  28.2  25.4\n 2     2  20.4  37.4  26.8\n 3     3  17.4  39.0  26.8\n 4     4  13.9  47.9  25.0\n 5     5  14.1  52.8  24.2\n 6     6  15.9  66.6  25.0\n 7     7  16.6  59.5  22.4\n 8     8  16.9  54.0  23.0\n 9     9  15.9  57.5  24.5\n10    10  14.2  47.1  24.2\n11    11  15.2  29.5  23.6\n12    12  17.5  27.7  24.5\n\nHere, we can see that o3 tends to be low in the winter months and high in the summer while no2 is higher in the winter and lower in the summer.\nslice_*()\nThe slice_sample() function of the dplyr package will allow you to see a sample of random rows in random order. The number of rows to show is specified by the n argument. This can be useful if you don’t want to print the entire tibble, but you want to get a greater sense of the values. This is a good option for data analysis reports, where printing the entire tibble would not be appropriate if the tibble is quite large.\n\n\nslice_sample(chicago, n = 10)\n\n\n# A tibble: 10 × 11\n   city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n   <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>    <dbl>     <dbl>\n 1 chic   76       65   1993-07-19  NA         43      28.8       21.8\n 2 chic   24       14.3 2004-01-21  20.2       26      10.2       26.0\n 3 chic   58       33.6 1988-05-13  NA         30      30.0       14.6\n 4 chic   50.5     32   1987-11-15  NA         51      19.2       36.9\n 5 chic   71       48   1994-09-21  NA         82      30.5       48.5\n 6 chic   67       51.3 2004-06-27  12         26.5    26.1       23.2\n 7 chic   79       51.1 1988-06-07  NA        139      54.2       34.7\n 8 chic   35       29.7 2001-02-13  37.3       34       7.17      29.5\n 9 chic   65.5     56.9 1989-09-20  NA         61      27.4       44.0\n10 chic   53       40.2 1992-04-16  NA         26      17.2       28.2\n# … with 3 more variables: pm25detrend <dbl>, year <dbl>,\n#   pm25.quint <fct>\n\nYou can also use slice_head() or slice_tail() to take a look at the top rows or bottom rows of your tibble. Again the number of rows can be specified with the n argument.\nThis will show the first 5 rows.\n\n\nslice_head(chicago, n = 5)\n\n\n# A tibble: 5 × 11\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n  <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>    <dbl>     <dbl>\n1 chic     35     30.1 2005-12-31 15          23.5     2.53      13.2\n2 chic     36     31   2005-12-30 15.1        19.2     3.03      22.8\n3 chic     35     29.4 2005-12-29  7.45       23.5     6.79      20.0\n4 chic     37     34.5 2005-12-28 17.8        27.5     3.26      19.3\n5 chic     40     33.6 2005-12-27 23.6        27       4.47      23.5\n# … with 3 more variables: pm25detrend <dbl>, year <dbl>,\n#   pm25.quint <fct>\n\nThis will show the last 5 rows.\n\n\nslice_tail(chicago, n = 5)\n\n\n# A tibble: 5 × 11\n  city   tmpd dewpoint date        pm25 pm10tmean2 o3tmean2 no2tmean2\n  <chr> <dbl>    <dbl> <date>     <dbl>      <dbl>    <dbl>     <dbl>\n1 chic   32       28.9 1987-01-05    NA       NA       4.75      30.3\n2 chic   29       28.6 1987-01-04    NA       47       4.38      30.4\n3 chic   33       27.4 1987-01-03    NA       34.2     3.33      23.8\n4 chic   33       29.9 1987-01-02    NA       NA       3.30      23.2\n5 chic   31.5     31.5 1987-01-01    NA       34       4.25      20.0\n# … with 3 more variables: pm25detrend <dbl>, year <dbl>,\n#   pm25.quint <fct>\n\nSummary\nThe dplyr pacfkage provides a concise set of operations for managing data frames. With these functions we can do a number of complex operations in just a few lines of code. In particular, we can often conduct the beginnings of an exploratory analysis with the powerful combination of group_by() and summarize().\nOnce you learn the dplyr grammar there are a few additional benefits\ndplyr can work with other data frame “back ends” such as SQL databases. There is an SQL interface for relational databases via the DBI package\ndplyr can be integrated with the data.table package for large fast tables\nThe dplyr package is handy way to both simplify and speed up your data frame management code. It is rare that you get such a combination at the same time!\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nHow can you tell if an object is a tibble?\nWhat option controls how many additional column names are printed at the footer of a tibble?\nUsing the trees dataset in base R (this dataset stores the girth, height, and volume for Black Cherry Trees) and using the pipe operator: (i) convert the data.frame to a tibble, (ii) filter for rows with a tree height of greater than 70, and (iii) order rows by Volume (smallest to largest).\n\n\nhead(trees)\n\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\nAdditional Resources\n\nhttps://r4ds.had.co.nz/tibbles.html\nhttps://jhudatascience.org/tidyversecourse/wrangle-data.html#data-wrangling\ndplyr cheat sheet from RStudio\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/dplyr_wrangling.png",
    "last_modified": "2021-09-05T22:33:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-07-reading-and-writing-data/",
    "title": "Reading and Writing data",
    "description": "How to get data in and out of R using relative paths",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-07",
    "categories": [
      "module 1",
      "week 2",
      "R",
      "Programming",
      "readr",
      "here",
      "tidyverse"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nCat meme\nIntroduction\nRelative versus absolute paths\nThe here package\nFinding and creating files locally\n\nReading data in base R\ntxt or csv\nR code\nR objects\n\nReading data files with read.table()\nReading in larger datasets with read.table()\nCalculating Memory Requirements for R Objects\nUsing the readr package\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\n\n“When writing code, you’re always collaborating with future-you; and past-you doesn’t respond to emails”. —Hadley Wickham\n\n[Source]\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rdpeng.github.io/Biostat776/lecture-getting-and-cleaning-data.html\nhttps://jhudatascience.org/tidyversecourse/get-data.html\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-getting-and-cleaning-data.html\nhttps://r4ds.had.co.nz/data-import.html\nLearning objectives\n\nAt the end of this lesson you will:\nKnow difference between relative vs absolute paths\nBe able to read and write text / csv files in R\nBe able to read and write R data objects in R\nBe able to calculate memory requirements for R objects\nUse modern R packages for reading and writing data\n\nCat meme\n\n\n\nFigure 1: Meme: that variable should be cat-egorical\n\n\n\n[Source]\nIntroduction\nThis lesson introduces ways to read and write data (e.g. .txt and .csv files) using base R functions as well as more modern R packages, such as readr, which is typically 10x faster than base R.\nWe will also briefly describe different ways for reading and writing other data types such as, Excel files, google spreadsheets, or SQL databases.\nRelative versus absolute paths\nWhen you are starting a data analysis, you have already learned about the use of .Rproj files. When you open up a .Rproj file, RStudio changes the path (location on your computer) to the .Rproj location.\nAfter opening up a .Rproj file, you can test this by\n\n\ngetwd()\n\n\n\nWhen you open up someone else’s R code or analysis, you might also see the\n\n\nsetwd()\n\n\n\nfunction being used which explicitly tells R to change the absolute path or absolute location of which directory to move into.\nFor example, say I want to clone a GitHub repo from Roger, which has 100 R script files, and in every one of those files at the top is:\n\nsetwd(\"C:\\Users\\Roger\\path\\only\\that\\Roger\\has\")\n\nThe problem is, if I want to use his code, I will need to go and hand-edit every single one of those paths (C:\\Users\\Roger\\path\\only\\that\\Roger\\has) to the path that I want to use on my computer or wherever I saved the folder on my computer (e.g.  /Users/Stephanie/Documents/path/only/I/have).\nThis is an unsustainable practice.\nI can go in and manually edit the path, but this assumes I know how to set a working directory. Not everyone does.\nSo instead of absolute paths:\n\n\nsetwd(\"/Users/jtleek/data\")\nsetwd(\"~/Desktop/files/data\")\nsetwd(\"C:\\\\Users\\\\Michelle\\\\Downloads\")\n\n\n\nA better idea is to use relative paths:\n\n\nsetwd(\"../data\")\nsetwd(\"../files\")\nsetwd(\"..\\tmp\")\n\n\n\nWithin R, an even better idea is to use the here R package will recognize the top-level directory of a Git repo and supports building all paths relative to that. For more on project-oriented workflow suggestions, read this post from Jenny Bryan.\n\n\n\nFigure 2: Artwork by Allison Horst on on the setwd function\n\n\n\n[Source: Artwork by Allison Horst]\nThe here package\nIn her post, Jenny Bryan writes\n\n“I suggest organizing each data analysis into a project: a folder on your computer that holds all the files relevant to that particular piece of work.”\n\nInstead of using setwd() at the top your .R or .Rmd file, she suggests:\nOrganize each logical project into a folder on your computer.\nMake sure the top-level folder advertises itself as such. This can be as simple as having an empty file named .here. Or, if you use RStudio and/or Git, those both leave characteristic files behind that will get the job done.\nUse the here() function from the here package to build the path when you read or write a file. Create paths relative to the top-level directory.\nWhenever you work on this project, launch the R process from the project’s top-level directory. If you launch R from the shell, cd to the correct folder first.\nLet’s test this out. We can use getwd() to see our current working directory path and the files available using list.files()\n\n\ngetwd()\n\n\n[1] \"/Users/shicks/Documents/github/teaching/jhustatcomputing2021/_posts/2021-09-07-reading-and-writing-data\"\n\nlist.files()\n\n\n[1] \"reading-and-writing-data_files\" \"reading-and-writing-data.html\" \n[3] \"reading-and-writing-data.Rmd\"  \n\nOK so our current location is in the reading and writing lectures sub-folder of the jhustatcomputing2021 course repository. Let’s try using the here package.\n\n\nlibrary(here)\n\nlist.files(here::here())\n\n\n [1] \"_custom.html\"               \"_exercises\"                \n [3] \"_post_template.Rmd\"         \"_posts\"                    \n [5] \"_projects\"                  \"_projects_custom.html\"     \n [7] \"_site.yml\"                  \"courses.css\"               \n [9] \"data\"                       \"docs\"                      \n[11] \"downloads\"                  \"images\"                    \n[13] \"index.Rmd\"                  \"jhustatcomputing2021.Rproj\"\n[15] \"lectures.Rmd\"               \"projects.Rmd\"              \n[17] \"README.md\"                  \"resources.Rmd\"             \n[19] \"schedule.Rmd\"               \"syllabus.Rmd\"              \n[21] \"videos\"                    \n\nlist.files(here(\"data\"))\n\n\n[1] \"2016-07-19.csv.bz2\" \"chicago.rds\"        \"team_standings.csv\"\n\nNow we see that using the here::here() function is a relative path (relative to the .Rproj file in our jhustatcomputing2021 repository. We also see there is are two .csv files in the data folder. We will learn how to read those files into R in the next section.\n\n\n\nFigure 3: Artwork by Allison Horst on the here package\n\n\n\n[Source: Artwork by Allison Horst]\nFinding and creating files locally\nOne last thing. If you want to download a file, one way to use the file.exists(), dir.create() and list.files() functions.\nfile.exists(here(\"my\", \"relative\", \"path\")): logical test if the file exists\ndir.create(here(\"my\", \"relative\", \"path\")): create a folder\nlist.files(here(\"my\", \"relative\", \"path\")): list contents of folder\nfile.create(here(\"my\", \"relative\", \"path\")): create a file\nfile.remove(here(\"my\", \"relative\", \"path\")): delete a file\nFor example, I can put all this together by\nChecking to see if a file exists in my path. If not, then\nCreate a directory in that path.\nList the files in the path.\n\n\nif(!file.exists(here(\"my\", \"relative\", \"path\"))){\n  dir.create(here(\"my\", \"relative\", \"path\"))\n}\nlist.files(here(\"my\", \"relative\", \"path\"))\n\n\n\nLet’s put relative paths to use while reading and writing data.\nReading data in base R\nIn this section, we’re going to demonstrate the essential functions you need to know to read and write (or save) data in R.\ntxt or csv\nThere are a few primary functions reading data from base R.\nread.table(), read.csv(): for reading tabular data\nreadLines(): for reading lines of a text file\nThere are analogous functions for writing data to files\nwrite.table(): for writing tabular data to text files (i.e. CSV) or connections\nwriteLines(): for writing character data line-by-line to a file or connection\nLet’s try reading some data into R with the read.csv() function.\n\n\ndf <- read.csv(here(\"data\", \"team_standings.csv\"))\ndf\n\n\n   Standing         Team\n1         1        Spain\n2         2  Netherlands\n3         3      Germany\n4         4      Uruguay\n5         5    Argentina\n6         6       Brazil\n7         7        Ghana\n8         8     Paraguay\n9         9        Japan\n10       10        Chile\n11       11     Portugal\n12       12          USA\n13       13      England\n14       14       Mexico\n15       15  South Korea\n16       16     Slovakia\n17       17  Ivory Coast\n18       18     Slovenia\n19       19  Switzerland\n20       20 South Africa\n21       21    Australia\n22       22  New Zealand\n23       23       Serbia\n24       24      Denmark\n25       25       Greece\n26       26        Italy\n27       27      Nigeria\n28       28      Algeria\n29       29       France\n30       30     Honduras\n31       31     Cameroon\n32       32  North Korea\n\nWe can use the $ symbol to pick out a specific column:\n\n\ndf$Team\n\n\n [1] \"Spain\"        \"Netherlands\"  \"Germany\"      \"Uruguay\"     \n [5] \"Argentina\"    \"Brazil\"       \"Ghana\"        \"Paraguay\"    \n [9] \"Japan\"        \"Chile\"        \"Portugal\"     \"USA\"         \n[13] \"England\"      \"Mexico\"       \"South Korea\"  \"Slovakia\"    \n[17] \"Ivory Coast\"  \"Slovenia\"     \"Switzerland\"  \"South Africa\"\n[21] \"Australia\"    \"New Zealand\"  \"Serbia\"       \"Denmark\"     \n[25] \"Greece\"       \"Italy\"        \"Nigeria\"      \"Algeria\"     \n[29] \"France\"       \"Honduras\"     \"Cameroon\"     \"North Korea\" \n\nWe can also ask for the full paths for specific files\n\n\nhere(\"data\", \"team_standings.csv\")\n\n\n[1] \"/Users/shicks/Documents/github/teaching/jhustatcomputing2021/data/team_standings.csv\"\n\n\nQuestions:\nWhat happens when you use readLines() function with the team_standings.csv data?\nHow would you only read in the first 5 lines?\n\nR code\nSometimes, someone will give you a file that ends in a .R. This is what’s called an R script file. It may contain code someone has written (maybe even you!), for example, a function that you can use with your data. In this case, you want the function available for you to use. To use the function, you have to first, read in the function from R script file into R.\nYou can check to see if the function already is loaded in R by looking at the Environment tab.\nThe function you want to use is\nsource(): for reading in R code files\nFor example, it might be something like this:\n\n\nsource(here::here('functions.R'))\n\n\n\nR objects\nAlternatively, you might be interested in reading and writing R objects.\nWriting data in e.g. .txt, .csv or Excel file formats is good if you want to open these files with other analysis software, such as Excel. However, these formats do not preserve data structures, such as column data types (numeric, character or factor). In order to do that, the data should be written out in a R data format.\nThere are several types R data file formats to be aware of:\n.RData: Stores multiple R objects\n.Rda: This is short for .RData and is equivalent.\n.Rds: Stores a single R object\n\nQuestion: why is saving data in as a R object useful?\nSaving data into R data formats can typically reduce considerably the size of large files by compression.\n\nNext, we will learn how to save\nA single R object\nMultiple R objects\nYour entire work space in a specified file\nReading in data from files\nload(): for reading in single or multiple R objects (opposite of save()) with a .Rda or .RData file format (objects must be same name)\nreadRDS(): for reading in a single object with a .Rds file format (can rename objects)\nunserialize(): for reading single R objects in binary form\nWriting data to files\nsave(): for saving an arbitrary number of R objects in binary format (possibly compressed) to a file.\nsaveRDS(): for saving a single object\nserialize(): for converting an R object into a binary format for outputting to a connection (or file).\nsave.image(): short for ’save my current workspace; while this sounds nice, it’s not terribly useful for reproducibility (hence not suggested); it’s also what happens when you try to quit R and it asks if you want to save your work space.\n\n\n\nFigure 4: Save data into R data file formats: RDS and RDATA\n\n\n\n[Source]\nLet’s try an example. Let’s save a vector of length 5 into the two file formats.\n\n\nx <- 1:5\nsave(x, file=here(\"data\", \"x.Rda\"))\nsaveRDS(x, file=here(\"data\", \"x.Rds\"))\nlist.files(path=here(\"data\"))\n\n\n[1] \"2016-07-19.csv.bz2\" \"chicago.rds\"        \"team_standings.csv\"\n[4] \"x.Rda\"              \"x.Rds\"             \n\nHere we assign the imported data to an object using readRDS()\n\n\nnew_x1 <- readRDS(here(\"data\", \"x.Rds\"))\nnew_x1\n\n\n[1] 1 2 3 4 5\n\nHere we assign the imported data to an object using load()\n\n\nnew_x2 <- load(here(\"data\", \"x.Rda\"))\nnew_x2\n\n\n[1] \"x\"\n\nNOTE: load() simply returns the name of the objects loaded. Not the values.\nLet’s clean up our space.\n\n\nfile.remove(here(\"data\", \"x.Rda\"))\n\n\n[1] TRUE\n\nfile.remove(here(\"data\", \"x.Rds\"))\n\n\n[1] TRUE\n\nrm(x)\n\n\n\n\nWhat do you think this code will do?\nHint: change eval=TRUE to see result\n\n\nx <- 1:5\ny <- x^2\nsave(x,y, file=here(\"data\", \"x.Rda\"))\nnew_x2 <- load(here(\"data\", \"x.Rda\"))\n\n\n\nWhen you are done:\n\n\nfile.remove(here(\"data\", \"x.Rda\"))\n\n\n\n\nNow, there are of course, many R packages that have been developed to read in all kinds of other datasets, and you may need to resort to one of these packages if you are working in a specific area.\nFor example, check out\nDBI for relational databases\nhaven for SPSS, Stata, and SAS data\nhttr for web APIs\nreadxl for .xls and .xlsx sheets\ngooglesheets4 for Google Sheets\ngoogledrive for Google Drive files\nrvest for web scraping\njsonlite for JSON\nxml2 for XML.\nReading data files with read.table()\nThe read.table() function is one of the most commonly used functions for reading data. The help file for read.table() is worth reading in its entirety if only because the function gets used a lot (run ?read.table in R). I know, I know, everyone always says to read the help file, but this one is actually worth reading.\nThe read.table() function has a few important arguments:\nfile, the name of a file, or a connection\nheader, logical indicating if the file has a header line\nsep, a string indicating how the columns are separated\ncolClasses, a character vector indicating the class of each column in the dataset\nnrows, the number of rows in the dataset. By default read.table() reads an entire file.\ncomment.char, a character string indicating the comment character. This defalts to \"#\". If there are no commented lines in your file, it’s worth setting this to be the empty string \"\".\nskip, the number of lines to skip from the beginning\nstringsAsFactors, should character variables be coded as factors? This defaults to FALSE. However, back in the “old days”, it defaulted to TRUE. The reason for this was because, if you had data that were stored as strings, it was because those strings represented levels of a categorical variable. Now, we have lots of data that is text data and they do not always represent categorical variables. So you may want to set this to be FALSE in those cases. If you always want this to be FALSE, you can set a global option via options(stringsAsFactors = FALSE). I’ve never seen so much heat generated on discussion forums about an R function argument than the stringsAsFactors argument. Seriously.\nFor small to moderately sized datasets, you can usually call read.table() without specifying any other arguments\n\n\ndata <- read.table(\"foo.txt\")\n\n\n\nNote: foo.txt is not a real dataset here. It is only used as an example for how to use read.table().\nIn this case, R will automatically:\nskip lines that begin with a #\nfigure out how many rows there are (and how much memory needs to be allocated)\nfigure what type of variable is in each column of the table.\nTelling R all these things directly makes R run faster and more efficiently. The read.csv() function is identical to read.table() except that some of the defaults are set differently (like the sep argument).\nReading in larger datasets with read.table()\nWith much larger datasets, there are a few things that you can do that will make your life easier and will prevent R from choking.\nRead the help page for read.table(), which contains many hints\nMake a rough calculation of the memory required to store your dataset (see the next section for an example of how to do this). If the dataset is larger than the amount of RAM on your computer, you can probably stop right here.\nSet comment.char = \"\" if there are no commented lines in your file.\nUse the colClasses argument. Specifying this option instead of using the default can make read.table() run MUCH faster, often twice as fast. In order to use this option, you have to know the class of each column in your data frame. If all of the columns are “numeric”, for example, then you can just set colClasses = \"numeric\". A quick an dirty way to figure out the classes of each column is the following:\n\n\ninitial <- read.table(\"datatable.txt\", nrows = 100)\nclasses <- sapply(initial, class)\ntabAll <- read.table(\"datatable.txt\", colClasses = classes)\n\n\n\nNote: datatable.txt is not a real dataset here. It is only used as an example for how to use read.table().\nSet nrows. This does not make R run faster but it helps with memory usage. A mild overestimate is okay. You can use the Unix tool wc to calculate the number of lines in a file.\nIn general, when using R with larger datasets, it’s also useful to know a few things about your system.\nHow much memory is available on your system?\nWhat other applications are in use? Can you close any of them?\nAre there other users logged into the same system?\nWhat operating system ar you using? Some operating systems can limit the amount of memory a single process can access\nCalculating Memory Requirements for R Objects\nBecause R stores all of its objects physical memory, it is important to be cognizant of how much memory is being used up by all of the data objects residing in your workspace. One situation where it is particularly important to understand memory requirements is when you are reading in a new dataset into R. Fortunately, it is easy to make a back of the envelope calculation of how much memory will be required by a new dataset.\nFor example, suppose I have a data frame with 1,500,000 rows and 120 columns, all of which are numeric data. Roughly, how much memory is required to store this data frame?\nWell, on most modern computers double precision floating point numbers are stored using 64 bits of memory, or 8 bytes. Given that information, you can do the following calculation\n1,500,000 × 120 × 8 bytes/numeric = 1,440,000,000 bytes\n= 1,440,000,000 / 220 bytes/MB\n= 1,373.29 MB\n= 1.34 GB\nSo the dataset would require about 1.34 GB of RAM. Most computers these days have at least that much RAM. However, you need to be aware of\nwhat other programs might be running on your computer, using up RAM\nwhat other R objects might already be taking up RAM in your workspace\nReading in a large dataset for which you do not have enough RAM is one easy way to freeze up your computer (or at least your R session). This is usually an unpleasant experience that usually requires you to kill the R process, in the best case scenario, or reboot your computer, in the worst case. So make sure to do a rough calculation of memory requirements before reading in a large dataset. You’ll thank me later.\nUsing the readr package\nThe readr package is recently developed by RStudio to deal with reading in large flat files quickly. The package provides replacements for functions like read.table() and read.csv(). The analogous functions in readr are read_table() and read_csv(). These functions are often much faster than their base R analogues and provide a few other nice features such as progress meters.\nFor example, the package includes a variety of functions in the read_* family that allow you to read in data from different formats of flat files. The following table gives a guide to several functions in the read_* family.\n\nreadr function\nUse\nread_csv\nReads comma-separated file\nread_csv2\nReads semicolon-separated file\nread_tsv\nReads tab-separated file\nread_delim\nGeneral function for reading delimited files\nread_fwf\nReads fixed width files\nread_log\nReads log files\n\nNote: In this code, I have used the kable() function from the knitr package to create the summary table in a table format, rather than as basic R output. This function is very useful for formatting basic tables in R markdown documents. For more complex tables, check out the pander and xtable packages.\nFor the most part, you can read use read_table() and read_csv() pretty much anywhere you might use read.table() and read.csv(). In addition, if there are non-fatal problems that occur while reading in the data, you will get a warning and the returned data frame will have some information about which rows/observations triggered the warning. This can be very helpful for “debugging” problems with your data before you get neck deep in data analysis.\nThe importance of the read_csv() function is perhaps better understood from an historical perspective. R’s built in read.csv() function similarly reads CSV files, but the read_csv() function in readr builds on that by removing some of the quirks and “gotchas” of read.csv() as well as dramatically optimizing the speed with which it can read data into R. The read_csv() function also adds some nice user-oriented features like a progress meter and a compact method for specifying column types.\nA typical call to read_csv() will look as follows.\n\n\nlibrary(readr)\nteams <- read_csv(here(\"data\", \"team_standings.csv\"))\nteams\n\n\n# A tibble: 32 × 2\n   Standing Team       \n      <dbl> <chr>      \n 1        1 Spain      \n 2        2 Netherlands\n 3        3 Germany    \n 4        4 Uruguay    \n 5        5 Argentina  \n 6        6 Brazil     \n 7        7 Ghana      \n 8        8 Paraguay   \n 9        9 Japan      \n10       10 Chile      \n# … with 22 more rows\n\nBy default, read_csv() will open a CSV file and read it in line-by-line. Similar to read.table(), you can tell the function to skip lines or which lines are comments:\n\n\nread_csv(\"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\",\n  skip = 2)\n\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\n\n\nread_csv(\"# A comment I want to skip\n  x,y,z\n  1,2,3\",\n  comment = \"#\")\n\n\n# A tibble: 1 × 3\n      x     y     z\n  <dbl> <dbl> <dbl>\n1     1     2     3\n\nIt will also (by default), read in the first few rows of the table in order to figure out the type of each column (i.e. integer, character, etc.). From the read_csv() help page:\n\nIf ‘NULL’, all column types will be imputed from the first 1000 rows on the input. This is convenient (and fast), but not robust. If the imputation fails, you’ll need to supply the correct types yourself.\n\nYou can specify the type of each column with the col_types argument.\nIn general, it is a good idea to specify the column types explicitly. This rules out any possible guessing errors on the part of read_csv(). Also, specifying the column types explicitly provides a useful safety check in case anything about the dataset should change without you knowing about it.\n\n\nteams <- read_csv(here(\"data\", \"team_standings.csv\"), \n                  col_types = \"cc\")\n\n\n\nNote that the col_types argument accepts a compact representation. Here \"cc\" indicates that the first column is character and the second column is character (there are only two columns). Using the col_types argument is useful because often it is not easy to automatically figure out the type of a column by looking at a few rows (especially if a column has many missing values).\nThe read_csv() function will also read compressed files automatically. There is no need to decompress the file first or use the gzfile connection function. The following call reads a gzip-compressed CSV file containing download logs from the RStudio CRAN mirror.\n\n\nlogs <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                 n_max = 10)\n\n\n\nNote that the warnings indicate that read_csv() may have had some difficulty identifying the type of each column. This can be solved by using the col_types argument.\n\n\nlogs <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                 col_types = \"ccicccccci\", \n                 n_max = 10)\nlogs\n\n\n# A tibble: 10 × 10\n   date   time     size r_version r_arch r_os  package version country\n   <chr>  <chr>   <int> <chr>     <chr>  <chr> <chr>   <chr>   <chr>  \n 1 2016-… 22:00… 1.89e6 3.3.0     x86_64 ming… data.t… 1.9.6   US     \n 2 2016-… 22:00… 4.54e4 3.3.1     x86_64 ming… assert… 0.1     US     \n 3 2016-… 22:00… 1.43e7 3.3.1     x86_64 ming… stringi 1.1.1   DE     \n 4 2016-… 22:00… 1.89e6 3.3.1     x86_64 ming… data.t… 1.9.6   US     \n 5 2016-… 22:00… 3.90e5 3.3.1     x86_64 ming… foreach 1.4.3   US     \n 6 2016-… 22:00… 4.88e4 3.3.1     x86_64 linu… tree    1.0-37  CO     \n 7 2016-… 22:00… 5.25e2 3.3.1     x86_64 darw… surviv… 2.39-5  US     \n 8 2016-… 22:00… 3.23e6 3.3.1     x86_64 ming… Rcpp    0.12.5  US     \n 9 2016-… 22:00… 5.56e5 3.3.1     x86_64 ming… tibble  1.1     US     \n10 2016-… 22:00… 1.52e5 3.3.1     x86_64 ming… magrit… 1.5     US     \n# … with 1 more variable: ip_id <int>\n\nYou can specify the column type in a more detailed fashion by using the various col_* functions. For example, in the log data above, the first column is actually a date, so it might make more sense to read it in as a Date object. If we wanted to just read in that first column, we could do\n\n\nlogdates <- read_csv(here(\"data\", \"2016-07-19.csv.bz2\"), \n                     col_types = cols_only(date = col_date()),\n                     n_max = 10)\nlogdates\n\n\n# A tibble: 10 × 1\n   date      \n   <date>    \n 1 2016-07-19\n 2 2016-07-19\n 3 2016-07-19\n 4 2016-07-19\n 5 2016-07-19\n 6 2016-07-19\n 7 2016-07-19\n 8 2016-07-19\n 9 2016-07-19\n10 2016-07-19\n\nNow the date column is stored as a Date object which can be used for relevant date-related computations (for example, see the lubridate package).\nNote: The read_csv() function has a progress option that defaults to TRUE. This options provides a nice progress meter while the CSV file is being read. However, if you are using read_csv() in a function, or perhaps embedding it in a loop, it is probably best to set progress = FALSE.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is the point of reference for using relative paths with the here::here() function?\nWhy was the argument stringsAsFactors=TRUE historically used?\nWhat is the difference between .Rds and .Rda file formats?\nWhat function in readr would you use to read a file where fields were separated with “|”?\n\nAdditional Resources\n\nhttps://swcarpentry.github.io/r-novice-inflammation/11-supp-read-write-csv\nhttps://support.rstudio.com/hc/en-us/articles/218611977-Importing-Data-with-the-RStudio-IDE\nhttps://jhudatascience.org/tidyversecourse/get-data.html\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/cracked_setwd.png",
    "last_modified": "2021-09-06T22:05:09-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-02-literate-programming/",
    "title": "Literate Statistical Programming",
    "description": "Introduction to literate statistical programming tools including R Markdown.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-02",
    "categories": [
      "module 1",
      "week 1",
      "R Markdown",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nWeaving and Tangling\nSweave\nrmarkdown\nknitr\n\nCreate and Knit Your First R Markdown Document\nWebsites and Books in R Markdown\nblogdown\nbookdown\ndistill\n\nTips and tricks in R Markdown in RStudio\nRun code\nInsert a comment in R and R Markdown\nKnit a R Markdown document\nCode snippets\nOrdered list in R Markdown\nNew code chunk in R Markdown\nReformat code\nRStudio addins\nOthers\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nhttps://rafalab.github.io/dsbook/reproducible-projects-with-rstudio-and-r-markdown.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown/\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-literate-statistical-programming.html\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown/\nLearning objectives\n\nAt the end of this lesson you will:\nBe able to define literate programming\nRecognize differences between available tools to for literate programming\nKnow how to efficiently work within RStudio for efficient literate programming\nCreate a R Markdown document\n\nIntroduction\nOne basic idea to make writing reproducible reports easier is what’s known as literate statistical programming (or sometimes called literate statistical practice). This comes from the idea of literate programming in the area of writing computer programs.\nThe idea is to think of a report or a publication as a stream of text and code. The text is readable by people and the code is readable by computers. The analysis is described in a series of text and code chunks. Each kind of code chunk will do something like load some data or compute some results. Each text chunk will relay something in a human readable language. There might also be presentation code that formats tables and figures and there’s article text that explains what’s going on around all this code. This stream of text and code is a literate statistical program or a literate statistical analysis.\nWeaving and Tangling\nLiterate programs by themselves are a bit difficult to work with, but they can be processed in two important ways. Literate programs can be weaved to produce human readable documents like PDFs or HTML web pages, and they can tangled to produce machine-readable “documents,” or in other words, machine readable code. The basic idea behind literate programming in order to generate the different kinds of output you might need, you only need a single source document—you can weave and tangle to get the rest. In order to use a system like this you need a documentational language, that’s human readable, and you need a programming language that’s machine readable (or can be compiled/interpreted into something that’s machine readable).\nSweave\nOne of the original literate programming systems in R that was designed to do this was called Sweave. Sweave enables users to combine R code with a documentation program called LaTeX. Sweave files ends a .Rnw and have R code weaved through the document:\n<<plot1, height=4, width=5, eval=FALSE>>=\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n@\nOnce you have created your .Rnw file, Sweave will process the file, executing the R chunks and replacing them with output as appropriate before creating the PDF document.\nIt was originally developed by Fritz Leisch, who is a core member of R, and the code base is still maintained by R Core. The Sweave system comes with any installation of R.\nThere are many limitations to the original Sweave system. One of the limitations is that it is focused primarily on LaTeX, which is not a documentation language that many people are familiar with. Therefore, it can be difficult to learn this type of markup language if you’re not already in a field that uses it regularly. Sweave also lacks a lot of features that people find useful like caching, and multiple plots per page and mixing programming languages.\nInstead, folks have moved towards using something called knitr, which offers everything Sweave does, plus it extends it further. With Sweave, additional tools are required for advanced operations, whereas knitr supports more internally. We’ll discuss knitr below.\nrmarkdown\nAnother choice for literate programming is to build documents based on Markdown language. A markdown file is a plain text file that is typically given the extension .md.. The rmarkdown R package takes a R Markdown file (.Rmd) and weaves together R code chunks like this:\n```{r plot1, height=4, width=5, eval=FALSE, echo=TRUE}\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n```\n\nThe best resource for learning about R Markdown this by Yihui Xie, J. J. Allaire, and Garrett Grolemund:\nhttps://bookdown.org/yihui/rmarkdown/\nThe R Markdown Cookbook by Yihui Xie, Christophe Dervieux, and Emily Riederer is really good too:\nhttps://bookdown.org/yihui/rmarkdown-cookbook/\nThe authors of the 2nd book describe the motivation for the 2nd book as:\n\n“However, we have received comments from our readers and publisher that it would be beneficial to provide more practical and relatively short examples to show the interesting and useful usage of R Markdown, because it can be daunting to find out how to achieve a certain task from the aforementioned reference book (put another way, that book is too dry to read). As a result, this cookbook was born.”\n\n\nBecause this is lecture is built in a .Rmd file, let’s demonstrate how this work. I am going to change eval=FALSE to eval=TRUE.\n\n\ndata(airquality)\nplot(airquality$Ozone ~ airquality$Wind)\n\n\n\n\n\nWhy do we not see the back ticks ``` anymore in the code chunk above that made the plot?\nWhat do you think we should do if we want to have the code executed, but we want to hide the code that made it?\n\nBefore we leave this section, I find that there is quite a bit of terminology to understand the magic behind rmarkdown that can be confusing, so let’s break it down:\nPandoc. Pandoc is a command line tool with no GUI that converts documents (e.g. from number of different markup formats to many other formats, such as .doc, .pdf etc). It is completely independent from R (but does come bundled with RStudio).\nMarkdown (markup language). Markdown is a lightweight markup language with plain text formatting syntax designed so that it can be converted to HTML and many other formats. A markdown file is a plain text file that is typically given the extension .md. It is completely independent from R.\nmarkdown (R package). markdown is an R package which converts .md files into HTML. It is no longer recommended for use has been surpassed by rmarkdown (discussed below).\nR Markdown (markup language). R Markdown is an extension of the markdown syntax. R Markdown files are plain text files that typically have the file extension .Rmd.\nrmarkdown (R package). The R package rmarkdown is a library that uses pandoc to process and convert .Rmd files into a number of different formats. This core function is rmarkdown::render(). Note: this package only deals with the markdown language. If the input file is e.g. .Rhtml or .Rnw, then you need to use knitr prior to calling pandoc (see below).\n\nCheck out the R Markdown Quick Tour for more:\nhttps://rmarkdown.rstudio.com/authoring_quick_tour.html\n\nknitr\nOne of the alternative that has come up in recent times is something called knitr. The knitr package for R takes a lot of these ideas of literate programming and updates and improves upon them. knitr still uses R as its programming language, but it allows you to mix other programming languages in. You can also use a variety of documentation languages now, such as LaTeX, markdown and HTML. knitr was developed by Yihui Xie while he was a graduate student at Iowa State and it has become a very popular package for writing literate statistical programs.\nKnitr takes a plain text document with embedded code, executes the code and ‘knits’ the results back into the document.\nFor for example, it converts\nAn R Markdown (.Rmd) file into a standard markdown file (.md)\nAn .Rnw (Sweave) file into to .tex format.\nAn .Rhtml file into to .html.\nThe core function is knitr::knit() and by default this will look at the input document and try and guess what type it is e.g. Rnw, Rmd etc.\nThis core function performs three roles:\nA source parser, which looks at the input document and detects which parts are code that the user wants to be evaluated.\nA code evaluator, which evaluates this code\nAn output renderer, which writes the results of evaluation back to the document in a format which is interpretable by the raw output type. For instance, if the input file is an .Rmd, the output render marks up the output of code evaluation in .md format.\n\n\n\nFigure 1: Converting a Rmd file to many outputs using knitr and pandoc\n\n\n\n[Source]\nAs seen in the figure above, from there pandoc is used to convert e.g. a .md file into many other types of file formats into a .html, etc.\nSo in summary:\n\n“R Markdown stands on the shoulders of knitr and Pandoc. The former executes the computer code embedded in Markdown, and converts R Markdown to Markdown. The latter renders Markdown to the output format you want (such as PDF, HTML, Word, and so on).”\n\n[Source]\nCreate and Knit Your First R Markdown Document\n\n\nWhen creating your first R Markdown document, in RStudio you can\nGo to File > New File > R Markdown…\nFeel free to edit the Title\nMake sure to select “Default Output Format” to be HTML\nClick “OK.” RStudio creates the R Markdown document and places some boilerplate text in there just so you can see how things are setup.\nClick the “Knit” button (or go to File > Knit Document) to make sure you can create the HTML output\nIf you successfully knit your first R Markdown document, then congratulations!\n\n\n\nFigure 2: Mission accomplished!\n\n\n\nWebsites and Books in R Markdown\nNow that you are on the road to using R Markdown documents, it is important to know about other wonderful things you do with these documents. For example, let’s say you have multiple .Rmd documents that you want to put together into a website, blog, book, etc.\nThere are primarily two ways to build multiple .Rmd documents together:\nblogdown for building websites\nbookdown for authoring books\nIn this section, we briefly introduce both packages, but it’s worth mentioning that the rmarkdown package also has a built-in site generator to build websites.\nblogdown\n\n\n\nFigure 3: blogdown logo\n\n\n\n[Source]\nThe blogdown R package is built on top of R Markdown, supports multi-page HTML output to write a blog post or a general page in an Rmd document, or a plain Markdown document. These source documents (e.g. .Rmd or .md) are built into a static website (i.e. a bunch of static HTML files, images and CSS files). Using this folder of files, it is very easy to publish it to any web server as a website. Also, it is easy to maintain because it is only a single folder.\n\nFor example, my personal website was built in blogdown:\nhttps://www.stephaniehicks.com\nOther really great examples can be found here:\nhttps://awesome-blogdown.com\n\nOther advantages include the content likely being reproducible, easier to maintain, and easy to convert pages to e.g. PDF or other formats in the future if you do not want to convert to HTML files. Because it is based on the Markdown syntax, it is easy to write technical documents, including math equations, insert figures or tables with captions, cross-reference with figure or table numbers, add citations, and present theorems or proofs.\nHere’s a video you can watch of someone making a blogdown website.\n\n\n\n\n[Source on YouTube]\nbookdown\n\n\n\nFigure 4: blogdown logo\n\n\n\n[Source]\nSimilar to blogdown, the bookdown R package is built on top of R Markdown, but also offers features like multi-page HTML output, numbering and cross-referencing figures/tables/sections/equations, inserting parts/appendices, and imported the GitBook style (https://www.gitbook.com) to create elegant and appealing HTML book pages. Share\n\nFor example, the previous version of this course was built in bookdown:\nhttps://rdpeng.github.io/Biostat776/\nAnother example is the Tidyverse Skills for Data Science book that the JHU Data Science Lab wrote. The github repo that contains all the .Rmd files can be found here.\nhttps://jhudatascience.org/tidyversecourse/\nhttps://github.com/jhudsl/tidyversecourse\n\nNote: Even though the word “book” is in “bookdown,” this package is not only for books. It really can be anything that consists of multiple .Rmd documents meant to be read in a linear sequence such as course dissertation/thesis, handouts, study notes, a software manual, a thesis, or even a diary.\nhttps://bookdown.org/yihui/rmarkdown/basics-examples.html#examples-books\ndistill\nThere is another great way to build blogs or websites using the distill for R Markdown.\nhttps://rstudio.github.io/distill\nDistill for R Markdown combines the technical authoring features of the Distill web framework (optimized for scientific and technical communication) with R Markdown, enabling a fully reproducible workflow based on literate programming (Knuth 1984).\nDistill articles include:\nReader-friendly typography that adapts well to mobile devices.\nFeatures essential to technical writing like LaTeX math, citations, and footnotes.\nFlexible figure layout options (e.g. displaying figures at a larger width than the article text).\nAttractively rendered tables with optional support for pagination.\nSupport for a wide variety of diagramming tools for illustrating concepts. The ability to incorporate JavaScript and D3-based interactive visualizations.\nA variety of ways to publish articles, including support for publishing sets of articles as a Distill website or as a Distill blog.\nThis course website is built in Distill for R Markdown:\nWebsite: https://stephaniehicks.com/jhustatcomputing2021\nGithub: https://github.com/stephaniehicks/jhustatcomputing2021\nSome other cool things about distill is the use of footnotes and asides.\nFor example.1 The number of the footnote will be automatically generated.\nYou can also optionally include notes in the gutter of the article (immediately to the right of the article text). To do this use the aside tag.\n\nThis content will appear in the gutter of the article.\nYou can also include figures in the gutter. Just enclose the code chunk which generates the figure in an aside tag\nTips and tricks in R Markdown in RStudio\nHere are shortcuts and tips on efficiently using RStudio to improve how you write code.\nRun code\nIf you want to run a code chunk:\ncommand + Enter on Mac\nCtrl + Enter on Windows\nInsert a comment in R and R Markdown\nTo insert a comment:\ncommand + Shift + C on Mac\nCtrl + Shift + C on Windows\nThis shortcut can be used both for:\nR code when you want to comment your code. It will add a # at the beginning of the line\nfor text in R Markdown. It will add <!-- and --> around the text\nNote that if you want to comment more than one line, select all the lines you want to comment then use the shortcut. If you want to uncomment a comment, apply the same shortcut.\nKnit a R Markdown document\nYou can knit R Markdown documents by using this shortcut:\ncommand + Shift + K on Mac\nCtrl + Shift + K on Windows\nCode snippets\nCode snippets is usually a few characters long and is used as a shortcut to insert a common piece of code. You simply type a few characters then press Tab and it will complete your code with a larger code. Tab is then used again to navigate through the code where customization is required. For instance, if you type fun then press Tab, it will auto-complete the code with the required code to create a function:\nname <- function(variables) {\n  \n}\nPressing Tab again will jump through the placeholders for you to edit it. So you can first edit the name of the function, then the variables and finally the code inside the function (try by yourself!).\nThere are many code snippets by default in RStudio. Here are the code snippets I use most often:\nlib to call library()\n\n\nlibrary(package)\n\n\n\nmat to create a matrix\n\n\nmatrix(data, nrow = rows, ncol = cols)\n\n\n\nif, el, and ei to create conditional expressions such as if() {}, else {} and else if () {}\n\nif (condition) {\n  \n}\n\nelse {\n  \n}\n\nelse if (condition) {\n  \n}\n\nfun to create a function\n\n\nname <- function(variables) {\n  \n}\n\n\n\nfor to create for loops\n\n\nfor (variable in vector) {\n  \n}\n\n\n\nts to insert a comment with the current date and time (useful if you have very long code and share it with others so they see when it has been edited)\n\n\n# Tue Jan 21 20:20:14 2020 ------------------------------\n\n\n\nYou can see all default code snippets and add yours by clicking on Tools > Global Options… > Code (left sidebar) > Edit Snippets…\nOrdered list in R Markdown\nIn R Markdown, when creating an ordered list such as this one:\nItem 1\nItem 2\nItem 3\nInstead of bothering with the numbers and typing\n1. Item 1\n2. Item 2\n3. Item 3\nyou can simply type\n1. Item 1\n1. Item 2\n1. Item 3\nfor the exact same result (try it yourself or check the code of this article!). This way you do not need to bother which number is next when creating a new item.\nTo go even further, any numeric will actually render the same result as long as the first item is the number you want to start from. For example, you could type:\n1. Item 1\n7. Item 2\n3. Item 3\nwhich renders\nItem 1\nItem 2\nItem 3\nHowever, I suggest always using the number you want to start from for all items because if you move one item at the top, the list will start with this new number. For instance, if we move 7. Item 2 from the previous list at the top, the list becomes:\n7. Item 2\n1. Item 1\n3. Item 3\nwhich incorrectly renders\nItem 2\nItem 1\nItem 3\nNew code chunk in R Markdown\nWhen editing R Markdown documents, you will need to insert a new R code chunk many times. The following shortcuts will make your life easier:\ncommand + option + I on Mac (or command + alt + I depending on your keyboard)\nCtrl + ALT + I on Windows\nReformat code\nA clear and readable code is always easier and faster to read (and look more professional when sharing it to collaborators). To automatically apply the most common coding guidelines such as white spaces, indents, etc., use:\ncmd + Shift + A on Mac\nCtrl + Shift + A on Windows\nSo for example the following code which does not respect the guidelines (and which is not easy to read):\n1+1\n  for(i in 1:10){if(!i%%2){next}\nprint(i)\n }\nbecomes much more neat and readable:\n1 + 1\nfor (i in 1:10) {\n  if (!i %% 2) {\n    next\n  }\n  print(i)\n}\nRStudio addins\nRStudio addins are extensions which provide a simple mechanism for executing advanced R functions from within RStudio. In simpler words, when executing an addin (by clicking a button in the Addins menu), the corresponding code is executed without you having to write the code. RStudio addins have the advantage that they allow you to execute complex and advanced code much more easily than if you would have to write it yourself.\n\nFor more information about RStudio addins, check out:\nhttps://rstudio.github.io/rstudioaddins/\nhttps://statsandr.com/blog/tips-and-tricks-in-rstudio-and-r-markdown/\n\nOthers\nSimilar to many other programs, you can also use:\ncommand + Shift + N on Mac and Ctrl + Shift + N on Windows to open a new R Script\ncommand + S on Mac and Ctrl + S on Windows to save your current script or R Markdown document\nCheck out Tools –> Keyboard Shortcuts Help to see a long list of these shortcuts.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is literate programming?\nWhat was the first literate statistical programming tool to weave together a statistical language (R) with a markup language (LaTeX)?\nWhat is knitr and how is different than other literate statistical programming tools?\nWhere can you find a list of other commands that help make your code writing more efficient in RStudio?\n\nAdditional Resources\n\nRMarkdown Tips and Tricks by Indrajeet Patil\nhttps://bookdown.org/yihui/rmarkdown/\nhttps://bookdown.org/yihui/rmarkdown-cookbook/\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nThis will become a hover-able footnote↩︎\n",
    "preview": "https://d33wubrfki0l68.cloudfront.net/61d189fd9cdf955058415d3e1b28dd60e1bd7c9b/9791d/images/rmarkdownflow.png",
    "last_modified": "2021-09-04T23:05:59-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-02-reference-management/",
    "title": "Reference management",
    "description": "How to use citations and incorporate references from a bibliography in R Markdown.",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-02",
    "categories": [
      "module 1",
      "week 1",
      "R Markdown",
      "programming"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nCitation management software\nLinking .bib file with R Markdown\nInline citation\nCitation styles\nOther cool features\n\nOther useful tips\nPost-lecture materials\nPractice\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nAuthoring in R Markdown from RStudio\nCitations from Reproducible Research in R from the Monash Data Fluency initiative\nBibliography from R Markdown Cookbook\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://andreashandel.github.io/MADAcourse/\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html\nhttps://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html\nhttps://monashdatafluency.github.io/r-rep-res/citations.html\nLearning objectives\n\nAt the end of this lesson you will:\nKnow what types of bibliography file formats can be used in a R Markdown file\nLearn how to add citations to a R Markdown file\nKnow how to change the citation style (e.g. APA, Chicago, etc)\n\nIntroduction\nFor almost any data analysis, especially if it is meant for publication in the academic literature, you will have to cite other people’s work and include the references (bibliographies or citations) in your work. In this class, you are likely to need to include references and cite other people’s work like in a regular research paper.\nR provides nice function citation() that helps us generating citation blob for R packages that we have used. Let’s try generating citation text for rmarkdown package by using the following command\n\n\ncitation(\"rmarkdown\")\n\n\n\nTo cite the 'rmarkdown' package in publications, please use:\n\n  JJ Allaire and Yihui Xie and Jonathan McPherson and Javier\n  Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham\n  and Joe Cheng and Winston Chang and Richard Iannone (2021).\n  rmarkdown: Dynamic Documents for R. R package version 2.10.\n  URL https://rmarkdown.rstudio.com.\n\n  Yihui Xie and J.J. Allaire and Garrett Grolemund (2018). R\n  Markdown: The Definitive Guide. Chapman and Hall/CRC. ISBN\n  9781138359338. URL https://bookdown.org/yihui/rmarkdown.\n\n  Yihui Xie and Christophe Dervieux and Emily Riederer (2020).\n  R Markdown Cookbook. Chapman and Hall/CRC. ISBN\n  9780367563837. URL\n  https://bookdown.org/yihui/rmarkdown-cookbook.\n\nTo see these entries in BibTeX format, use 'print(<citation>,\nbibtex=TRUE)', 'toBibtex(.)', or set\n'options(citation.bibtex.max=999)'.\n\nI assume you are familiar with how citing references works, and hopefully, you are already using a reference manager. If not, let me know in the discussion boards.\nTo have something that plays well with R Markdown, you need file format that stores all the references. Click here to learn more other possible file formats available to you to use within a R Markdown file:\nhttps://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html\nCitation management software\nAs you can see, there are ton of file formats including .medline (MEDLINE), .bib (BibTeX), .ris (RIS), .enl (EndNote).\nI will not discuss underlying citational management software itself, but I will talk briefly how you might create one of these file formats.\nIf you recall the output from citation(\"rmarkdown\") above, we might consider manually copying and pasting the output into a citation management software, but instead we can use write_bib() function from knitr package to create a bibliography file ending in .bib.\nLet’s run the following code in order to generate a my-refs.bib file\n\n\nknitr::write_bib(\"rmarkdown\", file = \"my-refs.bib\")\n\n\n\nNow we can see we have the file saved locally.\n\n\nlist.files()\n\n\n[1] \"my-refs.bib\"                \"reference-management_files\"\n[3] \"reference-management.html\"  \"reference-management.Rmd\"  \n\nIf you open up the my-refs.bib file, you will see\n@Manual{R-rmarkdown,\n  title = {rmarkdown: Dynamic Documents for R},\n  author = {JJ Allaire and Yihui Xie and Jonathan McPherson and Javier Luraschi and Kevin Ushey and Aron Atkins and Hadley Wickham and Joe Cheng and Winston Chang and Richard Iannone},\n  year = {2021},\n  note = {R package version 2.8},\n  url = {https://CRAN.R-project.org/package=rmarkdown},\n}\n\n@Book{rmarkdown2018,\n  title = {R Markdown: The Definitive Guide},\n  author = {Yihui Xie and J.J. Allaire and Garrett Grolemund},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2018},\n  note = {ISBN 9781138359338},\n  url = {https://bookdown.org/yihui/rmarkdown},\n}\n\n@Book{rmarkdown2020,\n  title = {R Markdown Cookbook},\n  author = {Yihui Xie and Christophe Dervieux and Emily Riederer},\n  publisher = {Chapman and Hall/CRC},\n  address = {Boca Raton, Florida},\n  year = {2020},\n  note = {ISBN 9780367563837},\n  url = {https://bookdown.org/yihui/rmarkdown-cookbook},\n}\n\nNote there are three keys that we will use later on:\nR-rmarkdown\nrmarkdown2018\nrmarkdown2020\n\nLinking .bib file with R Markdown\nIn order to use references within a R Markdown file, you will need to specify the name and a location of a bibliography file using the bibliography metadata field in a YAML metadata section. For example:\n---\ntitle: \"My top ten favorite R packages\"\noutput: html_document\nbibliography: my-refs.bib\n---\nYou can include multiple reference files using the following syntax, alternatively you can concatenate two bib files into one.\n---\nbibliography: [\"my-refs1.bib\", \"my-refs2.bib\"]\n---\nInline citation\nNow we can start using those bib keys that we have learned just before, using the following syntax\n[@key] for single citation\n[@key1; @key2] multiple citation can be separated by semi-colon\n[-@key] in order to suppress author name, and just display the year\n[see @key1 p 12; also this ref @key2] is also a valid syntax\nLet’s start by citing the rmarkdown package using the following code and press Knit button:\nI have been using the amazing Rmarkdown package (Allaire et al. 2021)! I should also go and read (Xie, Allaire, and Grolemund 2018; and Xie, Dervieux, and Riederer 2020) books.\nPretty cool, eh??\nTo celebrate, I’ll show you another one of my favorite art pieces from Allison Horst.\n\n\n\nFigure 1: R Markdown magic\n\n\n\n[Source: Artwork by Allison Horst]\nCitation styles\nBy default, Pandoc will use a Chicago author-date format for citations and references.\nTo use another style, you will need to specify a CSL (Citation Style Language) file in the csl metadata field, e.g.,\n---\ntitle: \"My top ten favorite R packages\"\noutput: html_document\nbibliography: my-refs.bib\ncsl: biomed-central.csl\n---\n\nTo find your required formats, we recommend using the Zotero Style Repository, which makes it easy to search for and download your desired style.\n\nCSL files can be tweaked to meet custom formatting requirements. For example, we can change the number of authors required before “et al.” is used to abbreviate them. This can be simplified through the use of visual editors such as the one available at https://editor.citationstyles.org.\nOther cool features\nAdd an item to a bibliography without using it\nBy default, the bibliography will only display items that are directly referenced in the document. If you want to include items in the bibliography without actually citing them in the body text, you can define a dummy nocite metadata field and put the citations there.\n---\nnocite: |\n  @item1, @item2\n---\nAdd all items to the bibliography\nIf we do not wish to explicitly state all of the items within the bibliography but would still like to show them in our references, we can use the following syntax:\n---\nnocite: '@*'\n---\nThis will force all items to be displayed in the bibliography.\n\nYou can also have an appendix appear after bibliography. For more on this, see:\nhttps://bookdown.org/yihui/rmarkdown-cookbook/bibliography.html\n\nOther useful tips\nWe have learned that inside your file that contains all your references (e.g. my-refs.bib), typically each reference gets a key, which is a shorthand that is generated by the reference manager or you can create yourself.\nFor instance, I use a format of lower-case first author last name followed by 4 digit year for each reference followed by a keyword (e.g name of a software package). Alternatively, you can omit the keyword. But note that if I cite a paper by the same first author that was published in the same year, then a lower case letter is added to the end. For instance, for a paper that I wrote as 1st author in 2010, my bibtex key might be hicks2021 or hicks2021a. You can decide what scheme to use, just pick one and use it forever.\nIn your R Markdown document, you can then cite the reference by adding the key, such as ...in the paper by Hicks et al. [@hicks2021]....\nPost-lecture materials\nPractice\nHere are some post-lecture tasks to practice some the material discussed.\n\nTry out the following:\nWhat do you notice that’s different when you run citation(\"tidyverse\") (compared to citation(\"rmarkdown\"))?\nInstall the following packages:\n\ninstall.packages(c(\"bibtex\", \"RefManageR\")\n\nWhat do they do? How might they be helpful to you in terms of reference management?\nInstead of using a .bib file, try using a different bibliography file format in an R Markdown document.\nPractice using a different CSL file to change the citation style.\n\n\n\n\nAllaire, JJ, Yihui Xie, Jonathan McPherson, Javier Luraschi, Kevin Ushey, Aron Atkins, Hadley Wickham, Joe Cheng, Winston Chang, and Richard Iannone. 2021. Rmarkdown: Dynamic Documents for r. https://CRAN.R-project.org/package=rmarkdown.\n\n\nXie, Yihui, J. J. Allaire, and Garrett Grolemund. 2018. R Markdown: The Definitive Guide. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown.\n\n\nXie, Yihui, Christophe Dervieux, and Emily Riederer. 2020. R Markdown Cookbook. Boca Raton, Florida: Chapman; Hall/CRC. https://bookdown.org/yihui/rmarkdown-cookbook.\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/rmarkdown_wizards.png",
    "last_modified": "2021-09-04T23:04:51-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-02-reproducible-research/",
    "title": "Reproducible Research",
    "description": "Introduction to reproducible research covering some basic concepts and ideas that are related to reproducible reporting",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-09-02",
    "categories": [
      "module 1",
      "week 1",
      "R",
      "reproducibility"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction\nWhat is wrong with replication?\n\nReproducibility to the Rescue\nWhy does this matter?\nTypes of reproducibility\nElements of computational reproducibility\n\n“X” to “Computational X”\nExample: machine learning in the life sciences\n\nThe Data Science Pipeline\nAuthors and Readers\n\nPost-lecture materials\nFinal Questions\n\n\n\nAn article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code and data, that produced the result. —Claerbout and Karrenbach (1992)\n\n[Link to Claerbout and Karrenbach (1992) article]\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nStatistical programming, Small mistakes, big impacts by Simon Schwab and Leonhard Held\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://ropensci.github.io/reproducibility-guide/sections/introduction/\nhttps://rdpeng.github.io/Biostat776/\nReproducible Research: A Retrospective by Roger Peng and Stephanie Hicks\nLearning objectives\n\nAt the end of this lesson you will:\nKnow the difference between replication and reproducibility\nIdentify valid reasons why replication and/or reproducibility is not always possible\nIdentify the type of reproducibility\nIdentify key components to enable reproducible data analyses\n\nIntroduction\nThis lecture will be about reproducible reporting, and I want to take the opportunity to cover some basic concepts and ideas that are related to reproducible reporting, just in case you have not heard about it or don not know what it is.\nBefore we get to reproducibility, we need to cover a little background with respect to how science works (even if you are not a scientist, this is important). The basic idea is that in science, replication is the most important element of verifying and validating findings. So if you claim that X causes Y, or that Vitamin C improves disease, or that something causes a problem, what happens is that other scientists that are independent of you will try to investigate that same question and see if they come up with a similar result. If lots of different people come up with the same result and replicate the original finding, then we tend to think that the original finding was probably true and that this is a real relationship or real finding.\nThe ultimate standard in strengthening scientific evidence is replication. The goal is to have independent people to do independent things with different data, different methods, and different laboratories and see if you get the same result. There is a sense that if a relationship in nature is truly there, then it should be robust to having different people discover it in different ways. Replication is particularly important in areas where findings can have big policy impacts or can influence regulatory types of decisions.\nWhat is wrong with replication?\nThere is really nothing wrong with it. This is what science has been doing for a long time, through hundreds of years. And there is nothing wrong with it today. But the problem is that it is becoming more and more challenging to do replication or to replicate other studies. Here are some reasons:\nOften times studies are much larger and more costly than previously. If you want to do ten versions of the same study, you need ten times as much money and there is not as much money around as there used to be.\nSometimes it is difficult to replicate a study because if the original study took 20 years to do, it is difficult to wait around another 20 years for replication.\nSome studies are just plain unique, such as studying the impact of a massive earthquake in a very specific location and time. If you are looking at a unique situation in time or a unique population, you cannot readily replicate that situation.\nThere are a lot of good reasons why you cannot replicate a study. If you cannot replicate a study, is the alternative just to do nothing, just let that study stand by itself?\nThe idea behind a reproducible reporting is to create a kind of minimum standard (or a middle ground) where we will not be replicating a study, but maybe we can do something in between. The basic problem is that you have the gold standard, which is replication, and then you have the worst standard which is doing nothing. What can we do that’s in between the gold standard and doing nothing? That is where reproducibility comes in. That’s how we can kind of bridge the gap between replication and nothing.\nIn non-research settings, often full replication is not even the point. Often the goal is to preserve something to the point where anybody in an organization can repeat what you did (for example, after you leave the organization). In this case, reproducibility is key to maintaining the history of a project and making sure that every step along the way is clear.\nSummary\nReplication, whereby scientific questions are examined and verified independently by different scientists, is the gold standard for scientific validity.\nReplication can be difficult and often there are no resources to independently replicate a study.\nReproducibility, whereby data and code are re-analyzed by independent scientists to obtain the same results of the original investigator, is a reasonable minimum standard when replication is not possible.\nReproducibility to the Rescue\nWhy do we need this kind of middle ground? I have not clearly defined reproducibility yet, but the basic idea is that you need to make the data available for the original study and the computational methods available so that other people can look at your data and run the kind of analysis that you have run, and come to the same findings that you found.\nWhat reproducible reporting is about is a validation of the data analysis. Because you are not collecting independent data using independent methods, it is a little bit more difficult to validate the scientific question itself. But if you can take someone’s data and reproduce their findings, then you can, in some sense, validate the data analysis.\nThis involves having the data and the code because more likely than not, the analysis will have been done on the computer using some sort of programming language, like R. So you can take their code and their data and reproduce the findings that they come up with. Then you can at least have confidence that you can reproduce the analysis.\nRecently, there has been a lot of discussion of reproducibility in the media and in the scientific literature. The journal Science had a special issue on reproducibility and data replication.\nhttps://www.science.org/toc/science/334/6060\nOther journals have specific policies to promote reproducibility in manuscripts that are published in their journals. For example, the Journal of American Statistical Association (JASA) requires authors to submit their code and data to reproduce their analyses and a set of Associate Editors of Reproducibility review those materials as part of the review process:\nhttps://jasa-acs.github.io/repro-guide\nWhy does this matter?\nHere is an example. In 2012, a feature on the TV show 60 minutes looked at a major incident at Duke University where many results involving a promising cancer test were found to be not reproducible. This led to a number of studies and clinical trials having to be stopped, followed by an investigation which is still ongoing.\n\n\n\n\n[Source on YouTube]\nTypes of reproducibility\nWhat are the different kinds of reproducible research? Enabling reproducibility can be complicated, but by separating out some of the levels and degrees of reproducibility the problem can become more manageable because we can focus our efforts on what best suits our specific scientific domain. Victoria Stodden (2014), a prominent scholar on this topic, has identified some useful distinctions in reproducible research:\nComputational reproducibility: when detailed information is provided about code, software, hardware and implementation details.\nEmpirical reproducibility: when detailed information is provided about non-computational empirical scientific experiments and observations. In practice this is enabled by making data freely available, as well as details of how the data was collected.\nStatistical reproducibility: when detailed information is provided about the choice of statistical tests, model parameters, threshold values, etc. This mostly relates to pre-registration of study design to prevent p-value hacking and other manipulations.\n[Source]\nElements of computational reproducibility\nWhat do we need for computational reproducibility? There are a variety of ways to talk about this, but one basic definition that we hae come up with is that there are four things that are required to make results reproducible:\nAnalytic data. The data that were used for the analysis that was presented should be available for others to access. This is different from the raw data because very often in a data analysis the raw data are not all used for the analysis, but rather some subset is used. It may be interesting to see the raw data but impractical to actually have it. Analytic data is key to examining the data analysis.\nAnalytic code. The analytic code is the code that was applied to the analytic data to produce the key results. This may be preprocessing code, regression modeling code, or really any other code used to produce the results from the analytic data.\nDocumentation. Documentation of that code and the data is very important.\nDistribution. Finally, there needs to be some standard means of distribution, so all this data in the code is easily accessible.\nSummary\nReproducible reporting is about is a validation of the data analysis\nThere are multiple types of reproducibility\nThere are four elements to computational reproducibility\n“X” to “Computational X”\nWhat is driving this need for a “reproducibility middle ground” between replication and doing nothing? For starters, there are a lot of new technologies on the scene and in many different fields of study including, biology, chemistry and environmental science. These technologies allow us to collect data at a much higher throughput so we end up with these very complex and very high dimensional data sets. These datasets can be collected almost instantaneously compared to even just ten years ago—the technology has allowed us to create huge data sets at essentially the touch of a button. Furthermore, we the computing power to take existing (already huge) databases and merge them into even bigger and bigger databases. Finally, the massive increase in computing power has allowed us to implement more sophisticated and complex analysis routines.\nThe analyses themselves, the models that we fit and the algorithms that we run, are much much more complicated than they used to be. Having a basic understanding of these algorithms is difficult, even for a sophisticated person, and it is almost impossible to describe these algorithms with words alone. Understanding what someone did in a data analysis now requires looking at code and scrutinizing the computer programs that people used.\nThe bottom line with all these different trends is that for every field “X”, there is now “Computational X”. There’s computational biology, computational astronomy—whatever it is you want, there is a computational version of it.\nExample: machine learning in the life sciences\nOne example of an area where reproducibility is important comes from research that I have conducted in the area of machine learning in the life sciences.\n\n\n\nFigure 1: Article in Nature Methods on ‘Reproduciblity standards for machine learning in the life sciences’\n\n\n\n[Link to tweet and link to article]\nIn the above article, computational reproducibility is not throught of as a binary property, but rather it is on a sliding scale that reflects the time needed to reproduce. Published works fall somewhere on this scale, which is bookended by ‘forever’, for a completely irreproducible work, and ‘zero’, for a work where one can automatically repeat the entire analysis with a single keystroke.\nAs in many cases it is difficult to impose a single standard that divides work into ‘reproducible’ and ‘irreproducible’. Therefore, instead a menu is proposed of three standards with varying degrees of rigor for computational reproducibility:\nBronze standard. The authors make the data, models and code used in the analysis publicly available. The bronze standard is the minimal standard for reproducibility. Without data, models and code, it is not possible to reproduce a work.\nSilver standard. In addition to meeting the bronze standard: (1) the dependencies of the analysis can be downloaded and installed in a single command; (2) key details for reproducing the work are documented, including the order in which to run the analysis scripts, the operating system used and system resource requirements; and (3) all random components in the analysis are set to be deterministic. The silver standard is a midway point between minimal availability and full automation. Works that meet this standard will take much less time to reproduce than ones only meeting the bronze standard.\nGold standard. The work meets the silver standard, and the authors make the analysis reproducible with a single command. The gold standard for reproducibility is full automation. When a work meets this standard, it will take little to no effort for a scientist to reproduce it.\nThe Data Science Pipeline\nThe basic issue is when you read a description of a data analysis, such as in an article or a technical report, for the most part, what you get is the report and nothing else. Of course, everyone knows that behind the scenes there’s a lot that went into this report and that is what I call the data science pipeline.\nThe Data Science PipelineIn this pipeline, there are two “actors”: the author of the report/article and the reader. On the left side, the author is going from left to right along this pipeline. The reader is going from right to left. If you are the reader, you read the article, and you may want to know more about what happened e.g.\nWhere are the data?\nWhat methods were used here?\nThe basic idea behind computational reproducibility is to focus on the elements in the blue box: the analytic data and the computational results. With computational reproducibility the goal is to allow the author of a report and the reader of that report to “meet in the middle”.\nAuthors and Readers\nIt is important to realize that there are multiple players when you talk about reproducibility–there are different types of parties that have different types of interests. There are authors who produce research and they want to make their research reproducible. There are also readers of research and they want to reproduce that work. Everyone needs tools to make their lives easier.\nOne current challenge is that authors of research have to undergo considerable effort to make their results available to a wide audience. Publishing data and code today is not necessarily a trivial task. Although there are a number of resources available now, that were not available even five years ago, it is still a bit of a challenge to get things out on the web (or at least distributed widely). Resources like GitHub, kipoi, and RPubs and various data repositories have made a big difference, but there is still a ways to go with respect to building up the public reproducibility infrastructure.\nFurthermore, even when data and code are available, readers often have to download the data, download the code, and then they have to piece everything together, usually by hand. It’s not always an easy task to put the data and code together. Also, readers may not have the same computational resources that the original authors did. If the original authors used an enormous computing cluster, for example, to do their analysis, the readers may not have that same enormous computing cluster at their disposal. It may be difficult for readers to reproduce the same results.\nGenerally, the toolbox for doing reproducible research is small, although it’s definitely growing. In practice, authors often just throw things up on the web. There are journals and supplementary materials, but they are famously disorganized. There are only a few central databases that authors can take advantage of to post their data and make it available. So if you are working in a field that has a central database that everyone uses, that is great. If you are not, then you have to assemble your own resources.\nSummary\nThe process of conducting and disseminating research can be depicted as a “data science pipeline”\nReaders and consumers of data science research are typically not privy to the details of the data science pipeline\nOne view of reproducibility is that it gives research consumers partial access to the raw pipeline elements.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is the difference between replication and reproducible?\nWhy can replication be difficult to achieve? Why is reproducibility a reasonable minimum standard when replication is not possible?\nWhat is needed to reproduce the results of a data analysis?\n\n\n\n\n",
    "preview": "posts/2021-09-02-reproducible-research/reproducible-research_files/figure-html5/noah-1.png",
    "last_modified": "2021-09-01T15:42:59-04:00",
    "input_file": {},
    "preview_width": 1100,
    "preview_height": 712
  },
  {
    "path": "posts/2021-08-31-introduction-to-gitgithub/",
    "title": "Introduction to git/GitHub",
    "description": "Version control is a game changer; or how I learned to love git/GitHub",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-08-31",
    "categories": [
      "module 1",
      "week 1",
      "programming",
      "version control",
      "git",
      "GitHub"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nIntroduction to git/GitHub\ngit\nGitHub\nWhy use git/GitHub?\nWhat to (not) do\nHow to use Git/GitHub\n\nGetting Started\nUsing git/GitHub in our course\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nHappy Git with R from Jenny Bryan\nChapter on git and GitHub in dsbook from Rafael Irizarry\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://andreashandel.github.io/MADAcourse/\nLearning objectives\n\nAt the end of this lesson you will:\nKnow what Git and GitHub are.\nKnow why one might want to use them.\nHave created and set up a GitHub account.\n\nIntroduction to git/GitHub\nThis document gives a brief explanation of GitHub and how we will use it for this course.\ngit\nGit is what is called a version control system for file management. The main idea is that as you (and your collaborators) work on a project, the software tracks, and records any changes made by anyone.\nSimilar to the “track changes” features in Microsoft Word, but more rigorous, powerful, and scaled up to multiple files\nGreat for solo or collaborative work\nGitHub\nGitHub is a hosting service on internet for git-aware folders and projects\nSimilar to the DropBox or Google, but more structured, powerful, and programmatic\nGreat for solo or collaborative work!\nTechnically GitHub is distinct from Git. However, GitHub is in some sense the interface and Git the underlying engine (a bit like RStudio and R).\nSince we will only be using Git through GitHub, I tend to not distinguish between the two. In the following, I refer to all of it as just GitHub. Note that other interfaces to Git exist, e.g., Bitbucket, but GitHub is the most widely used one.\nWhy use git/GitHub?\nYou want to use GitHub to avoid this:\n\n\n\nFigure 1: How not to use GitHub [image from PhD Comics]\n\n\n\n[Source: PhD Comics]\nGitHub gives you a clean way to track your projects. It is also very well suited to collaborative work. Historically, version control was used for software development. However, it has become broader and is now used for many types of projects, including data science projects.\nTo learn a bit more about Git/GitHub and why you might want to use it, read this article by Jenny Bryan.\nNote her explanation of what’s special with the README.md file on GitHub.\nWhat to (not) do\nGitHub is ideal if you have a project with a fair number of files, most of those files are text files (such as code, LaTeX, (R)markdown, etc.) and different people work on different parts of the project.\nGitHub is less useful if you have a lot of non-text files (e.g. Word or Powerpoint) and different team members might want to edit the same document at the same time. In that instance, a solution like Google Docs, Word+Dropbox, Word+Onedrive, etc. might be better.\nHow to use Git/GitHub\nGit and GitHub is fundamentally based on commands you type into the command line. Lots of online resources show you how to use the command line. This is the most powerful, and the way I almost always interact with git/GitHub. However, many folks find this the most confusing way to use git/GitHub. Alternatively, there are graphical interfaces.\nGitHub itself provides a grapical interface with basic functionality.\nRStudio also has Git/GitHub integration. Of course this only works for R project GitHub integration.\nThere are also third party GitHub clients with many advanced features, most of which you won’t need initially, but might eventually.\nNote: As student, you can (and should) upgrade to the Pro version of GitHub for free (i.e. access to unlimited private repositories is one benefit), see the GitHub student developer pack on how to do this.\nGetting Started\nOne of my favorite resources for getting started with git/GitHub is the Happy Git with R from Jenny Bryan:\nhttps://happygitwithr.com\n\n\n\nFigure 2: A screenshot of the Happy Git with R online book from Jenny Bryan .\n\n\n\nIt truly is one of the best resources out there for getting started with git/GitHub, especially with the integration to RStudio. Therefore, at this point, I will encourage all of you to go read through the online book.\nSome of you may only need to skim it, others will need to spend some time reading through it. Either way, I will bet that you won’t regret the time investment.\nUsing git/GitHub in our course\nIn this course, you will use git/GitHub in the following ways:\nProject 0 (optional) - You will create a website introducing yourself to folks in the course and deploy it on GitHub.\nProjects 1-3 - You will be asked to practice using git locally (on your compute environment) to track your changes over time and, if you wish (but highly suggested), you can practice pushing your project solutions to a private GitHub repository on your GitHub account (i.e. git add, git commit, git push, git pull, etc) .\nLearning these skills will be useful down the road if you ever work collaboratively on a project (i.e. writing code as a group). In this scenario, you will use the skills you have been practicing in your projects to work together as a team in a single GitHub repository.\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nWhat is version control?\nWhat is the difference between git and GitHub?\nWhat are other version controls software/tools that are available besides git?\n\nAdditional Resources\n\ngit and GitHub in the dsbook by Rafael Irizarry.\n\n\n\n\n",
    "preview": "posts/2021-08-31-introduction-to-gitgithub/../../images/phdversioncontrol.gif",
    "last_modified": "2021-08-30T23:15:53-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-31-introduction-to-r-and-rstudio/",
    "title": "Introduction to R and RStudio",
    "description": "Let's dig into the R programming language and the RStudio integrated developer environment",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-08-31",
    "categories": [
      "module 1",
      "week 1",
      "R",
      "programming",
      "RStudio"
    ],
    "contents": "\n\nContents\nPre-lecture materials\nRead ahead\nAcknowledgements\n\nLearning objectives\nOverview and history of R\ntl;dr (R in a nutshell)\nBasic Features of R\nFree Software\nDesign of the R System\nLimitations of R\n\nUsing R and RStudio\nInstalling R and RStudio\nRStudio default options\nInstalling and loading R packages\nGetting started in RStudio\n\nPost-lecture materials\nFinal Questions\nAdditional Resources\n\n\n\nThere are only two kinds of languages: the ones people complain about and the ones nobody uses. —Bjarne Stroustrup\n\nPre-lecture materials\nRead ahead\n\nBefore class, you can prepare by reading the following materials:\nAn overview and history of R from Roger Peng\nInstalling R and RStudio from Rafael Irizarry\nGetting Started in R and RStudio from Rafael Irizarry\n\nAcknowledgements\nMaterial for this lecture was borrowed and adopted from\nhttps://rdpeng.github.io/Biostat776/lecture-introduction-and-overview.html\nhttps://rafalab.github.io/dsbook\nhttps://rmd4sci.njtierney.com\nhttps://andreashandel.github.io/MADAcourse\nLearning objectives\n\nAt the end of this lesson you will:\nLearn about (some of) the history of R.\nIdentify some of the strengths and weaknesses of R.\nInstall R and Rstudio on your computer.\nKnow how to install and load R packages.\n\nOverview and history of R\nBelow is a very quick introduction to R, to get you set up and running. We’ll go deeper into R and coding later.\ntl;dr (R in a nutshell)\nLike every programming language, R has its advantages and disadvantages. If you search the internet, you will quickly discover lots of folks with opinions about R. Some of the features that are useful to know are:\nR is open-source, freely accessible, and cross-platform (multiple OS).\nR is a “high-level” programming language, relatively easy to learn.\nWhile “Low-level” programming languages (e.g. Fortran, C, etc) often have more efficient code, they can also be harder to learn because it is designed to be close to a machine language.\nIn contrast, high-level languages deal more with variables, objects, functions, loops, and other abstract CS concepts with a focus on usability over optimal program efficiency.\n\nR is great for statistics, data analysis, websites, web apps, data visualizations, and so much more!\nR integrates easily with document preparation systems like \\(\\LaTeX\\), but R files can also be used to create .docx, .pdf, .html, .ppt files with integrated R code output and graphics.\nThe R Community is very dynamic, helpful and welcoming.\nCheck out the #rstats on Twitter, TidyTuesday podcast and community activity in the R4DS Online Learning Community, and r/rstats subreddit.\nIf you are looking for more local resources, check out R-Ladies Baltimore.\n\nThrough R packages, it is easy to get lots of state-of-the-art algorithms.\nDocumentation and help files for R are generally good.\nWhile we use R in this course, it is not the only option to analyze data. Maybe the most similar to R, and widely used, is Python, which is also free. There is also commercial software that can be used to analyze data (e.g., Matlab, Mathematica, Tableau, SAS, SPSS). Other more general programming languages are suitable for certain types of analyses as well (e.g., C, Fortran, Perl, Java, Julia).\nDepending on your future needs or jobs, you might have to learn one or several of those additional languages. The good news is that even though those languages are all different, they all share general ways of thinking and structuring code. So once you understand a specific concept (e.g., variables, loops, branching statements or functions), it applies to all those languages. Thus, learning a new programming language is much easier once you already know one. And R is a good one to get started with.\nWith the skills gained in this course, hopefully you will find R a fun and useful programming langauge for your future projects.\n\n\n\nFigure 1: Artwork by Allison Horst on learning R\n\n\n\n[Source: Artwork by Allison Horst]\nBasic Features of R\nToday R runs on almost any standard computing platform and operating system. Its open source nature means that anyone is free to adapt the software to whatever platform they choose. Indeed, R has been reported to be running on modern tablets, phones, PDAs, and game consoles.\nOne nice feature that R shares with many popular open source projects is frequent releases. These days there is a major annual release, typically in October, where major new features are incorporated and released to the public. Throughout the year, smaller-scale bugfix releases will be made as needed. The frequent releases and regular release cycle indicates active development of the software and ensures that bugs will be addressed in a timely manner. Of course, while the core developers control the primary source tree for R, many people around the world make contributions in the form of new feature, bug fixes, or both.\nAnother key advantage that R has over many other statistical packages (even today) is its sophisticated graphics capabilities. R’s ability to create “publication quality” graphics has existed since the very beginning and has generally been better than competing packages. Today, with many more visualization packages available than before, that trend continues. R’s base graphics system allows for very fine control over essentially every aspect of a plot or graph. Other newer graphics systems, like lattice and ggplot2 allow for complex and sophisticated visualizations of high-dimensional data.\nR has maintained the original S philosophy (see box below), which is that it provides a language that is both useful for interactive work, but contains a powerful programming language for developing new tools. This allows the user, who takes existing tools and applies them to data, to slowly but surely become a developer who is creating new tools.\n\nFor a great discussion on an overview and history of R and the S programming language, read through this chapter from Roger D. Peng.\n\nFinally, one of the joys of using R has nothing to do with the language itself, but rather with the active and vibrant user community. In many ways, a language is successful inasmuch as it creates a platform with which many people can create new things. R is that platform and thousands of people around the world have come together to make contributions to R, to develop packages, and help each other use R for all kinds of applications. The R-help and R-devel mailing lists have been highly active for over a decade now and there is considerable activity on web sites like Stack Overflow, Twitter #rstats and Reddit.\nFree Software\nA major advantage that R has over many other statistical packages and is that it’s free in the sense of free software (it’s also free in the sense of free beer). The copyright for the primary source code for R is held by the R Foundation and is published under the GNU General Public License version 2.0.\nAccording to the Free Software Foundation, with free software, you are granted the following four freedoms\nThe freedom to run the program, for any purpose (freedom 0).\nThe freedom to study how the program works, and adapt it to your needs (freedom 1). Access to the source code is a precondition for this.\nThe freedom to redistribute copies so you can help your neighbor (freedom 2).\nThe freedom to improve the program, and release your improvements to the public, so that the whole community benefits (freedom 3). Access to the source code is a precondition for this.\n\nYou can visit the Free Software Foundation’s web site to learn a lot more about free software. The Free Software Foundation was founded by Richard Stallman in 1985 and Stallman’s personal web site is an interesting read if you happen to have some spare time.\n\nDesign of the R System\nThe primary R system is available from the Comprehensive R Archive Network, also known as CRAN. CRAN also hosts many add-on packages that can be used to extend the functionality of R.\nThe R system is divided into 2 conceptual parts:\nThe “base” R system that you download from CRAN:\nLinux\nWindows\nMac\nEverything else.\nR functionality is divided into a number of packages.\nThe “base” R system contains, among other things, the base package which is required to run R and contains the most fundamental functions.\nThe other packages contained in the “base” system include utils, stats, datasets, graphics, grDevices, grid, methods, tools, parallel, compiler, splines, tcltk, stats4.\nThere are also “Recommended” packages: boot, class, cluster, codetools, foreign, KernSmooth, lattice, mgcv, nlme, rpart, survival, MASS, spatial, nnet, Matrix.\nWhen you download a fresh installation of R from CRAN, you get all of the above, which represents a substantial amount of functionality. However, there are many other packages available:\nThere are over 10,000 packages on CRAN that have been developed by users and programmers around the world.\nThere are also many packages associated with the Bioconductor project.\nPeople often make packages available on their personal websites; there is no reliable way to keep track of how many packages are available in this fashion.\n\nQuestions:\nHow many R packages are on CRAN today?\nHow many R packages are on Bioconductor today?\nHow many R packages are on GitHub today?\n\nLimitations of R\nNo programming language or statistical analysis system is perfect. R certainly has a number of drawbacks. For starters, R is essentially based on almost 50 year old technology, going back to the original S system developed at Bell Labs. There was originally little built in support for dynamic or 3-D graphics (but things have improved greatly since the “old days”).\nAnother commonly cited limitation of R is that objects must generally be stored in physical memory (though this is increasingly not true anymore). This is in part due to the scoping rules of the language, but R generally is more of a memory hog than other statistical packages. However, there have been a number of advancements to deal with this, both in the R core and also in a number of packages developed by contributors. Also, computing power and capacity has continued to grow over time and amount of physical memory that can be installed on even a consumer-level laptop is substantial. While we will likely never have enough physical memory on a computer to handle the increasingly large datasets that are being generated, the situation has gotten quite a bit easier over time.\nAt a higher level one “limitation” of R is that its functionality is based on consumer demand and (voluntary) user contributions. If no one feels like implementing your favorite method, then it’s your job to implement it (or you need to pay someone to do it). The capabilities of the R system generally reflect the interests of the R user community. As the community has ballooned in size over the past 10 years, the capabilities have similarly increased. When I first started using R, there was very little in the way of functionality for the physical sciences (physics, astronomy, etc.). However, now some of those communities have adopted R and we are seeing more code being written for those kinds of applications.\nUsing R and RStudio\n\nIf R is the engine and bare bones of your car, then RStudio is like the rest of the car. The engine is super critical part of your car. But in order to make things properly functional, you need to have a steering wheel, comfy seats, a radio, rear and side view mirrors, storage, and seatbelts. — Nicholas Tierney\n\n[Source]\nThe RStudio layout has the following features:\nOn the upper left, something called a Rmarkdown script\nOn the lower left, the R console\nOn the lower right, the view for files, plots, packages, help, and viewer.\nOn the upper right, the environment / history pane\n\n\n\nFigure 2: A screenshot of the RStudio integrated developer environment (IDE) – aka the working environment.\n\n\n\nThe R console is the bit where you can run your code. This is where the R code in your Rmarkdown document gets sent to run (we’ll learn about these files later).\nThe file/plot/pkg viewer is a handy browser for your current files, like Finder, or File Explorer, plots are where your plots appear, you can view packages, see the help files. And the environment / history pane contains the list of things you have created, and the past commands that you have run.\nInstalling R and RStudio\nIf you have not already, install R first. If you already have R installed, make sure it is a fairly recent version, version 4.0 or newer. If yours is older, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it’s a recent version, it should be of the 1.4.X series.\n\nInstalling R and RStudio should be fairly straightforward. However, a great set of detailed instructions is in Rafael Irizarry’s dsbook\nhttps://rafalab.github.io/dsbook/installing-r-rstudio.html\n\nIf things don’t work, ask for help in the courseplus discussion board.\nI personally only have experience with Mac, but everything should work on all the standard operating systems (Windows, Mac, and even Linux).\nRStudio default options\nTo first get set up, I highly recommend changing the following setting\nTools > Global Options (or Cmd + , on macOS)\nUnder the General tab:\nFor workspace\nUncheck restore .RData into workspace at startup\nSave workspace to .RData on exit : “Never”\n\nFor History\nUncheck \"Always save history (even when not saving .RData)\nUncheck “Remove duplicate entries in history”\n\nThis means that you won’t save the objects and other things that you create in your R session and reload them. This is important for two reasons\nReproducibility: you don’t want to have objects from last week cluttering your session\nPrivacy: you don’t want to save private data or other things to your session. You only want to read these in.\nYour “history” is the commands that you have entered into R.\nAdditionally, not saving your history means that you won’t be relying on things that you typed in the last session, which is a good habit to get into!\nInstalling and loading R packages\nAs we discussed, most of the functionality and features in R come in the form of add-on packages. There are tens of thousands of packages available, some big, some small, some well documented, some not. We will be using many different packages in this course. Of course, you are free to install and use any package you come across for any of the assignments.\nThe “official” place for packages is the CRAN website. If you are interested in packages on a specific topic, the CRAN task views provide curated descriptions of packages sorted by topic.\nTo install an R package from CRAN, one can simply call the install.packages() function and pass the name of the package as an argument. For example, to install the ggplot2 package from CRAN: open RStudio,go to the R prompt (the > symbol) in the lower-left corner and type\n\n\ninstall.packages(\"ggplot2\")\n\n\n\nand the appropriate version of the package will be installed.\nOften, a package needs other packages to work (called dependencies), and they are installed automatically. It usually does not matter if you use a single or double quotation mark around the name of the package.\n\nQuestions:\nAs you installed the ggplot2 package, what other packages were installed?\nWhat happens if you tried to install GGplot2?\n\nIt could be that you already have all packages required by ggplot2 installed. In that case, you will not see any other packages installed. To see which of the packages above ggplot2 needs (and thus installs if it is not present), type into the R console:\n\n\ntools::package_dependencies(\"ggplot2\")\n\n\n\nIn RStudio, you can also install (and update/remove) packages by clicking on the ‘Packages’ tab in the bottom right window.\nIt is very common these days for packages to be developed on GitHub. It is possible to install packages from GitHub directly. Those usually contain the latest version of the package, with features that might not be available yet on the CRAN website. Sometimes, in early development stages, a package is only on GitHub until the developer(s) feel it is good enough for CRAN submission. So installing from GitHub gives you the latest. The downside is that packages under development can often be buggy and not working right. To install packages from GitHub, you need to install the remotes package and then use the following function\n\n\nremotes::install_github()\n\n\n\nWe will not do that now, but it is quite likely that at one point later in this course we will.\nYou only need to install a package once, unless you upgrade/re-install R. Once installed, you still need to load the package before you can use it. That has to happen every time you start a new R session. You do that using the library() command. For instance to load the ggplot2 package, type\n\n\nlibrary('ggplot2')\n\n\n\nYou may or may not see a short message on the screen. Some packages show messages when you load them, and others do not.\nThis was a quick overview of R packages. We will use a lot of them, so you will get used to them rather quickly.\nGetting started in RStudio\nWhile one can use R and do pretty much every task, including all the ones we cover in this class, without using RStudio, RStudio is very useful, has lots of features that make your R coding life easier and has become pretty much the default integrated development environment (IDE) for R. Since RStudio has lots of features, it takes time to learn them. A good resource to learn more about RStudio are the R Studio Essentials collection of videos.\n\nFor more information on setting up and getting started with R, RStudio, and R packages, read the Getting Started chapter in the dsbook:\nhttps://rafalab.github.io/dsbook/getting-started.html\nThis chapter gives some tips, shortcuts, and ideas that might be of interest even to those of you who already have R and/or RStudio experience.\n\nPost-lecture materials\nFinal Questions\nHere are some post-lecture questions to help you think about the material discussed.\n\nQuestions:\nIf a software company asks you, as a requirement for using their software, to sign a license that restricts you from using their software to commit illegal activities, is this consistent with the “Four Freedoms” of Free Software?\nWhat is an R package and what is it used for?\nWhat function in R can be used to install packages from CRAN?\nWhat is a limitation of the current R system?\n\nAdditional Resources\n\nR for Data Science by Wickham & Grolemund (2017). Covers most of the basics of using R for data analysis.\nAdvanced R by Wickham (2014). Covers a number of areas including object-oriented, programming, functional programming, profiling and other advanced topics.\nRStudio IDE cheatsheet\n\n\n\n\n",
    "preview": "https://github.com/allisonhorst/stats-illustrations/raw/master/rstats-artwork/r_first_then.png",
    "last_modified": "2021-08-30T23:12:24-04:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "Welcome!",
    "description": "Overview course information for students enrolled in JHSPH Biostatistics 140.776 in Fall 2021",
    "author": [
      {
        "name": "Stephanie Hicks",
        "url": "https://stephaniehicks.com/"
      }
    ],
    "date": "2021-08-31",
    "categories": [
      "course-admin",
      "module 1",
      "week 1"
    ],
    "contents": "\nWelcome! I am very excited to have you in our one-term (i.e. half a semester) course on Statistical Computing course number (140.776) offered by the Department of Biostatistics at the Johns Hopkins Bloomberg School of Public Health.\nThis course is designed for ScM and PhD students at Johns Hopkins Bloomberg School of Public Health. I am pretty flexible about permitting outside students, but I want everyone to be aware of the goals and assumptions so no one feels like they are surprised by how the class works.\n\nThe primary goal of the course is to teach you practical programming and computational skills required for the research and application of statistical methods.\n\nThis class is not designed to teach the theoretical aspects of statistical or computational methods, but rather the goal is to help with the practical issues related to setting up a statistical computing environment for data analyses, developing high-quality R packages, conducting reproducible data analyses, best practices for data visualization and writing code, and creating websites for personal or project use.\nAssumptions and pre-requisites\nThe course is designed for students in the Johns Hopkins Biostatistics Masters and PhD programs. However, we do not assume a significant background in statistics. Specifically we assume:\n1. You know the basics of at least one programming language (e.g. R or Python)\nIf it’s not R, we assume that you are willing to spend the time to learn R\nYou have heard of things such as control structures, functions, loops, etc\nKnow the difference between different data types (e.g. character, numeric, etc)\nKnow the basics of plotting (e.g. what is a scatterplot, histogram, etc)\n2. You know the basics of computing environments\nYou have access to a computing environment (i.e. locally on a laptop or working in the cloud)\nYou generally feel comfortable with installing and working with software\n3. You know the basics of statistics\nThe central dogma (estimates, standard errors, basic distributions, etc.)\nKey statistical terms and methods\nDifferences between estimation vs testing vs prediction\nKnow how to fit and interpret basic statistical models (e.g. linear models)\n4. You know the basics of reproducible research\nDifference between replication and reproducible\nKnow how to cite references (e.g. like in a publication)\nSomewhat familiar with tools that enable reproducible research (In complete transparency, we will briefly cover these topics in the first week, but depending on your comfort level with them, this may impact whether you choose to continue with the course).\nSince the target audience for this course is advanced students in statistics we will not be able to spend significant time covering these concepts and technologies. To give you some idea about how these prerequisites will impact your experience in the course, we will be turning in all assignments via R Markdown documents and you will be encouraged (not required) to use git/GitHub to track changes to your code over time. The majority of the assignments will involve learning the practical issues around performing data analyses, building software packages, building websites, etc all using the R programming language. Data analyses you will perform will also often involve significant data extraction, cleaning, and transformation. We will learn about tools to do all of this, but hopefully most of this sounds familiar to you so you can focus on the concepts we will be teaching around best practices for statistical computing.\n\nSome resources that may be useful if you feel you may be missing pieces of this background:\nStatistics - Mathematical Biostatistics Bootcamp I (Coursera); Mathematical Biostatistics Bootcamp II (Coursera)\nBasic Data Science - Cloud Data Science (Leanpub); Data Science Specialization (Coursera)\nVersion Control - Github Learning Lab; Happy Git and Github for the useR\nRmarkdown - Rmarkdown introduction\n\nGetting set up\nYou must install R and RStudio on your computing environment in order to complete this course. These are two different applications that must be installed separately before they can be used together:\nR is the core underlying programming language and computing engine that we will be learning in this course\nRStudio is an interface into R that makes many aspects of using and programming R simpler\nBoth R and RStudio are available for Windows, macOS, and most flavors of Unix and Linux. Please download the version that is suitable for your computing setup.\nThroughout the course, we will make use of numerous R add-on packages that must be installed over the Internet. Packages can be installed using the install.packages() function in R. For example, to install the tidyverse package, you can run\ninstall.packages(\"tidyverse\")\nin the R console.\nHow to Download R for Windows\nGo to https://cran.r-project.org and\nClick the link to “Download R for Windows”\nClick on “base”\nClick on “Download R 4.1.1 for Windows”\nVideo Demo for Downloading R for WindowsHow to Download R for the Mac\nGoto https://cran.r-project.org and\nClick the link to “Download R for (Mac) OS X”.\nClick on “R-4.1.1.pkg”\nVideo Demo for Downloading R for the MacHow to Download RStudio\nGoto https://rstudio.com and\nClick on “Products” in the top menu\nThen click on “RStudio” in the drop down menu\nClick on “RStudio Desktop”\nClick the button that says “DOWNLOAD RSTUDIO DESKTOP”\nClick the button under “RStudio Desktop” Free\nUnder the section “All Installers” choose the file that is appropriate for your operating system.\n\n\n\nVideo Demo for Downloading RStudioLearning Objectives\nThe goal is by the end of the class, students will be able to:\nInstall and configure software necessary for a statistical programming environment and with version control\nDiscuss generic programming language concepts as they are implemented in a high-level statistical language\nWrite and debug code in base R and the tidyverse (and integrate code from Python modules)\nBuild basic data visualizations using R and the tidyverse\nBuild and organize a software package with documentation for publishing on the internet\nDiscuss and implement basic statistical computing algorithms for optimization, linear regression, and Monte Carlo\nCourse Staff\nThe course instructor this year is Stephanie Hicks, but this course has been previously taught for a number of years by Roger Peng. We are both faculty in the Biostatistics Department at Johns Hopkins and Directors of the Johns Hopkins Data Science Lab.\nMy research focuses on developing fast, scalable, statistical methodology and open-source software for genomics and biomedical data analysis for human health and disease. My research is problem-forward: I develop statistical methods and software that are motivated by concrete problems, often with real-world, noisy, messy data. I’m also interested in developing theory for how to incorporate design thinking (alongside statistical thinking) in practice of data analysis.\nIf you want, you can find me on Twitter. I’m also a co-host of the The Corresponding Author podcast, member of the Editorial Board for Genome Biology, an Associate Editor for Reproducibility at the Journal of the American Statistical Association, and co-founder of R-Ladies Baltimore.\nRoger’s research focuses on air pollution, spatial statistics, and reproducibility. We have been colleagues and friends for over 3 years and I am really excited to have the opportunity to teach this course.\nWe also have a couple of amazing TAs this year:\nAthena Chen (achen70@jhu.edu). She is a fifth year PhD Candidate in the Department of Biostatistics working on characterizing antibody responses to various antigens including the human microbiome and viruses such as coronaviruses and HIV. She is broadly interested in immunology and statistical proteomics. In her free time, she enjoys weight lifting, yoga, cooking, and a nice cup of coffee.\nRuzhang Zhao (rzhao@jhu.edu). He is a third year PhD student from Department of Biostatistics. His research interests are single cell genomics and statistical genetics. For more information, please visit http://ruzhangzhao.com.\nCourse logistics\nAs with all things in a pandemic, this year we are continuing to teach this course virtually (similar to last year) to be able to have a large group of students benefit from it. The course webpage will be here at:\nhttps://www.stephaniehicks.com/jhustatcomputing2021\nAll communication for the course is going to take place on one of three platforms:\nCourseplus: for discussion, sharing resources, collaborating, and announcements\nGithub: for getting access to course materials (e.g. lectures, project assignments)\nCourse Github: https://github.com/stephaniehicks/jhustatcomputing2021\n\nZoom: for live class lectures\nCourse Zoom: Link available on Courseplus\nThe plan is for recorded lectures will be posted online after class ends\n\nThe primary communication for the class will go through Courseplus That is where we will post course announcements, host most of our asynchronous course discussion, and as the primary means of communication between course participants and course instructors.\n\nIf you are registered for the course, you should have access to Courseplus now. Once you have access you will also be able to find the course Zoom links. Zoom links for office hours will also be posted on Courseplus.\n\nAssignment Due Dates\nAll course assignment due dates appear on the Schedule and Syllabus.\nThe Pandemic\nThis is how 2020 felt:\n\n\n\nFigure 1: How 2020 felt\n\n\n\nWhile there are many positive things that have happened in 2021, for many folks, 2021 has not been much of an improvement\n\n\n\nFigure 2: How 2021 feels\n\n\n\nIt is super tough to be dealing with the pandemic, an economic crisis, challenges with visas and travel and coordinating school online. As your instructor, I understand that this is not an ordinary year. I am ultra sympathetic to family challenges and life challenges. I have three small children (who may make cameos in lectures frome time to time).\nMy goal is to make as much of the class asynchronous as possible so you can work whenever you have time. My plan is to be as understanding as possible when it comes to grading, and any issues that come up with the course. Please don’t hesitate to reach out to me (or the TAs) if you are having issues and we will do our best to direct you to whatever resources we have/accommodate you however we can.\nI think the material in this course is important, fun, and this is an opportunity to learn a lot. But life is more important than a course and if there was ever a time that life might get in the way of learning, it’s likely now.\nGrading\nPhilosophy\nWe believe the purpose of graduate education is to train you to be able to think for yourself and initiate and complete your own projects. We are super excited to talk to you about ideas, work out solutions with you, and help you to figure out how to produce professional data analyses. We do not think that graduate school grades are important for this purpose. This means that we do not care very much about graduate student grades.\nThat being said, we have to give you a grade so they will be:\nA - Excellent - 90%+\nB - Passing - 80%+\nC - Needs improvement - 70%+\nWe rarely give out grades below a C and if you consistently submit work, and do your best you are very likely to get an A or a B in the course.\nRelative weights\nThe grades are based on three projects (plus one entirely optional project to help you get set up). The breakdown of grading will be\n33% for Project 1\n33% for Project 2\n34% for Project 3\nIf you submit an project solution, it is your own work, and it meets a basic level of completeness and effort you will get 100% for that project. If you submit a project solution, but it doesn’t meet basic completeness and effort you will receive 50%. If you do not submit an solution you will receive 0%.\nSubmitting assignments\nPlease write up your project solutions using R Markdown. In some cases, you will compile a R Markdown file into an HTML file and submit your HTML file to the dropbox on Courseplus. In other cases, you may create an R package or website. In all of the above, when applicable, show all your code and provide as much explanation / documentation as you can.\nFor each project, we will provide a time when we download the materials. We will assume whatever version we download at that time is what you are turning in.\nReproducibility\nWe will talk about reproducibility a bit during class, and it will be a part of the homework assignments as well. Reproducibility of scientific code is very challenging, so the faculty and TAs completely understand difficulties that arise. But we think that it is important that you practice reproducible research. In particular, your project assignments should perform the tasks that you are asked to do and create the figures and tables you are asked to make as a part of the compilation of your document. We will have some pointers for some issues that have come up as we announce the projects.\nCode of Conduct\nWe are committed to providing a welcoming, inclusive, and harassment-free experience for everyone, regardless of gender, gender identity and expression, age, sexual orientation, disability, physical appearance, body size, race, ethnicity, religion (or lack thereof), political beliefs/leanings, or technology choices. We do not tolerate harassment of course participants in any form. Sexual language and imagery is not appropriate for any work event, including group meetings, conferences, talks, parties, Twitter and other online media. This code of conduct applies to all course participants, including instructors and TAs, and applies to all modes of interaction, both in-person and online, including GitHub project repos, Slack channels, and Twitter.\nCourse participants violating these rules will be referred to leadership of the Department of Biostatistics and the Title IX coordinator at JHU and may face expulsion from the class.\nAll class participants agree to:\nBe considerate in speech and actions, and actively seek to acknowledge and respect the boundaries of other members.\nBe respectful. Disagreements happen, but do not require poor behavior or poor manners. Frustration is inevitable, but it should never turn into a personal attack. A community where people feel uncomfortable or threatened is not a productive one. Course participants should be respectful both of the other course participants and those outside the course.\nRefrain from demeaning, discriminatory, or harassing behavior and speech. Harassment includes, but is not limited to: deliberate intimidation; stalking; unwanted photography or recording; sustained or willful disruption of talks or other events; inappropriate physical contact; use of sexual or discriminatory imagery, comments, or jokes; and unwelcome sexual attention. If you feel that someone has harassed you or otherwise treated you inappropriately, please alert Stephanie Hicks.\nTake care of each other. Refrain from advocating for, or encouraging, any of the above behavior. And, if someone asks you to stop, then stop. Alert Stephanie Hicks if you notice a dangerous situation, someone in distress, or violations of this code of conduct, even if they seem inconsequential.\nNeed Help?\nPlease speak with Stephanie Hicks or one of the TAs. You can also reach out to Karen Bandeen-Roche, chair of the department of Biostatistics or Margaret Taub, Ombudsman for the Department of Biostatistics.\nYou may also reach out to any Hopkins resource for sexual harassment, discrimination, or misconduct:\nJHU Sexual Assault Helpline, 410-516-7333 (confidential)\nUniversity Sexual Assault Response and Prevention website\nJohns Hopkins Compliance Hotline, 844-SPEAK2US (844-733-2528)\nHopkins Policies Online\nJHU Office of Institutional Equity 410-516-8075 (nonconfidential)\nJohns Hopkins Student Assistance Program (JHSAP), 443-287-7000\nUniversity Health Services, 410-955-1892\nThe Faculty and Staff Assistance Program (FASAP), 443-997-7000\nFeedback\nWe welcome feedback on this Code of Conduct.\nLicense and attribution\nThis Code of Conduct is distributed under a Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0) license. Portions of above text comprised of language from the Codes of Conduct adopted by rOpenSci and Django, which are licensed by CC BY-SA 4.0 and CC BY 3.0. This work was further inspired by Ada Initiative’s ‘’how to design a code of conduct for your community’’ and Geek Feminism’s Code of conduct evaluations and expanded by Ashley Johnson and Shannon Ellis in the Jeff Leek group.\nAcademic Ethics\nStudents enrolled in the Bloomberg School of Public Health of The Johns Hopkins University assume an obligation to conduct themselves in a manner appropriate to the University’s mission as an institution of higher education. A student is obligated to refrain from acts which he or she knows, or under the circumstances has reason to know, impair the academic integrity of the University. Violations of academic integrity include, but are not limited to: cheating; plagiarism; knowingly furnishing false information to any agent of the University for inclusion in the academic record; violation of the rights and welfare of animal or human subjects in research; and misconduct as a member of either School or University committees or recognized groups or organizations.\nStudents should be familiar with the policies and procedures specified under Policy and Procedure Manual Student-01 (Academic Ethics), available on the school’s portal.\nThe faculty, staff and students of the Bloomberg School of Public Health and the Johns Hopkins University have the shared responsibility to conduct themselves in a manner that upholds the law and respects the rights of others. Students enrolled in the School are subject to the Student Conduct Code (detailed in Policy and Procedure Manual Student-06) and assume an obligation to conduct themselves in a manner which upholds the law and respects the rights of others. They are responsible for maintaining the academic integrity of the institution and for preserving an environment conducive to the safe pursuit of the School’s educational, research, and professional practice missions.\nDisability support services\nIf you are a student with a documented disability who requires an academic accommodation, please contact the Office of Disability Support Services at 410-502-6602 or via email at JHSPH.dss@jhu.edu. Accommodations take effect upon approval and apply to the remainder of the time for which a student is registered and enrolled at the Bloomberg School of Public Health.\nPrevious versions of the class\nhttps://rdpeng.github.io/Biostat776\nTypos and corrections\nFeel free to submit typos/errors/etc via the github repository associated with the class: https://github.com/stephaniehicks/jhustatcomputing2021. You will have the thanks of your grateful instructor!\n\n\n\n",
    "preview": "https://media.giphy.com/media/XdIOEZTt6dL7zTYWIo/giphy.gif",
    "last_modified": "2021-08-31T12:31:02-04:00",
    "input_file": {}
  }
]
